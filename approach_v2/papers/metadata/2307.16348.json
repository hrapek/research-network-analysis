{
    "abstract": "This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.",
    "arxivId": "2307.16348",
    "authors": [
        {
            "authorId": "1663736991",
            "name": "Devin White",
            "url": "https://www.semanticscholar.org/author/1663736991"
        },
        {
            "authorId": "123804962",
            "name": "Mingkang Wu",
            "url": "https://www.semanticscholar.org/author/123804962"
        },
        {
            "authorId": "151479011",
            "name": "Ellen R. Novoseller",
            "url": "https://www.semanticscholar.org/author/151479011"
        },
        {
            "authorId": "2194602",
            "name": "Vernon J. Lawhern",
            "url": "https://www.semanticscholar.org/author/2194602"
        },
        {
            "authorId": "3436871",
            "name": "Nicholas R. Waytowich",
            "url": "https://www.semanticscholar.org/author/3436871"
        },
        {
            "authorId": "33403228",
            "name": "Yongcan Cao",
            "url": "https://www.semanticscholar.org/author/33403228"
        }
    ],
    "citationVelocity": 0,
    "citations": [
        {
            "arxivId": "2409.07268",
            "authors": [
                {
                    "authorId": "2320763891",
                    "name": "Ziang Liu"
                },
                {
                    "authorId": "2244291786",
                    "name": "Junjie Xu"
                },
                {
                    "authorId": "153028502",
                    "name": "Xingjiao Wu"
                },
                {
                    "authorId": "2259176383",
                    "name": "Jing Yang"
                },
                {
                    "authorId": "2262513159",
                    "name": "Liang He"
                }
            ],
            "doi": "10.48550/arXiv.2409.07268",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "781d3c7fe990303f56f5da39eba3253f152e3505",
            "title": "Multi-Type Preference Learning: Empowering Preference-Based Reinforcement Learning with Equal Preferences",
            "url": "https://www.semanticscholar.org/paper/781d3c7fe990303f56f5da39eba3253f152e3505",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2408.04190",
            "authors": [
                {
                    "authorId": "2295954954",
                    "name": "Heewoong Choi"
                },
                {
                    "authorId": "47165273",
                    "name": "Sangwon Jung"
                },
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "2315133096",
                    "name": "Taesup Moon"
                }
            ],
            "doi": "10.48550/arXiv.2408.04190",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "8525434adbf25984e55c78063c71bcb958d364e4",
            "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/8525434adbf25984e55c78063c71bcb958d364e4",
            "venue": "ICML",
            "year": 2024
        },
        {
            "arxivId": "2406.06495",
            "authors": [
                {
                    "authorId": "2135227820",
                    "name": "Calarina Muslimani"
                },
                {
                    "authorId": "2122740553",
                    "name": "Bram Grooten"
                },
                {
                    "authorId": "2297771697",
                    "name": "Deepak Ranganatha Sastry Mamillapalli"
                },
                {
                    "authorId": "1691997",
                    "name": "Mykola Pechenizkiy"
                },
                {
                    "authorId": "2571038",
                    "name": "D. Mocanu"
                },
                {
                    "authorId": "2276510220",
                    "name": "M. E. Taylor"
                }
            ],
            "doi": "10.48550/arXiv.2406.06495",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "b224881223d805a76493a4d44c37d815422e2ea4",
            "title": "Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity",
            "url": "https://www.semanticscholar.org/paper/b224881223d805a76493a4d44c37d815422e2ea4",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2405.00746",
            "authors": [
                {
                    "authorId": "2135227820",
                    "name": "Calarina Muslimani"
                },
                {
                    "authorId": "2276510220",
                    "name": "M. E. Taylor"
                }
            ],
            "doi": "10.5555/3635637.3663173",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "3e65e47eb6361213715b3098659ca75fbb172e4f",
            "title": "Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/3e65e47eb6361213715b3098659ca75fbb172e4f",
            "venue": "AAMAS",
            "year": 2024
        },
        {
            "arxivId": "2402.02423",
            "authors": [
                {
                    "authorId": "2112499487",
                    "name": "Yifu Yuan"
                },
                {
                    "authorId": "2198399020",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2146275908",
                    "name": "Yi Ma"
                },
                {
                    "authorId": "2256465607",
                    "name": "Zibin Dong"
                },
                {
                    "authorId": "2261023910",
                    "name": "Hebin Liang"
                },
                {
                    "authorId": "2124810107",
                    "name": "Jinyi Liu"
                },
                {
                    "authorId": "2283518365",
                    "name": "Zhixin Feng"
                },
                {
                    "authorId": "2163994",
                    "name": "Kai-Wen Zhao"
                },
                {
                    "authorId": "2256046812",
                    "name": "Yan Zheng"
                }
            ],
            "doi": "10.48550/arXiv.2402.02423",
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "5f3c4145842792e5bc7de14f918fc381be00b06f",
            "title": "Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback",
            "url": "https://www.semanticscholar.org/paper/5f3c4145842792e5bc7de14f918fc381be00b06f",
            "venue": "ICLR",
            "year": 2024
        }
    ],
    "corpusId": 260334244,
    "doi": "10.48550/arXiv.2307.16348",
    "fieldsOfStudy": [
        "Computer Science"
    ],
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "isPublisherLicensed": true,
    "is_open_access": true,
    "is_publisher_licensed": true,
    "numCitedBy": 5,
    "numCiting": 36,
    "paperId": "8e0e795463a2007497c9c257f9de337c53f1c4b9",
    "references": [
        {
            "arxivId": "2303.16194",
            "authors": [
                {
                    "authorId": "1580188581",
                    "name": "Andrew Szot"
                },
                {
                    "authorId": "2111672235",
                    "name": "Amy Zhang"
                },
                {
                    "authorId": "1746610",
                    "name": "Dhruv Batra"
                },
                {
                    "authorId": "145276578",
                    "name": "Z. Kira"
                },
                {
                    "authorId": "153145615",
                    "name": "Franziska Meier"
                }
            ],
            "doi": "10.48550/arXiv.2303.16194",
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "d8c0b2fbacdade8bde676feff9da9aaeda244839",
            "title": "BC-IRL: Learning Generalizable Reward Functions from Demonstrations",
            "url": "https://www.semanticscholar.org/paper/d8c0b2fbacdade8bde676feff9da9aaeda244839",
            "venue": "ICLR",
            "year": 2023
        },
        {
            "arxivId": "2205.12401",
            "authors": [
                {
                    "authorId": "103617120",
                    "name": "Xi-Xi Liang"
                },
                {
                    "authorId": "2166310899",
                    "name": "Katherine Shu"
                },
                {
                    "authorId": "3436470",
                    "name": "Kimin Lee"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "doi": "10.48550/arXiv.2205.12401",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "cc9f2fd320a279741403c4bfbeb91179803c428c",
            "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/cc9f2fd320a279741403c4bfbeb91179803c428c",
            "venue": "ICLR",
            "year": 2022
        },
        {
            "arxivId": "2111.03026",
            "authors": [
                {
                    "authorId": "3436470",
                    "name": "Kimin Lee"
                },
                {
                    "authorId": "152447364",
                    "name": "Laura M. Smith"
                },
                {
                    "authorId": "2745001",
                    "name": "A. Dragan"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "51965de80f86432d42749427db1e5bb0fa1e204c",
            "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/51965de80f86432d42749427db1e5bb0fa1e204c",
            "venue": "NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "arxivId": "2108.07259",
            "authors": [
                {
                    "authorId": "8307674",
                    "name": "Erdem Biyik"
                },
                {
                    "authorId": "1482546251",
                    "name": "Aditi Talati"
                },
                {
                    "authorId": "1779671",
                    "name": "Dorsa Sadigh"
                }
            ],
            "doi": "10.1109/HRI53351.2022.9889650",
            "intent": [],
            "isInfluential": false,
            "paperId": "ca9a66f802d83706c353d11ea5d18b276092c9f9",
            "title": "APReL: A Library for Active Preference-based Reward Learning Algorithms",
            "url": "https://www.semanticscholar.org/paper/ca9a66f802d83706c353d11ea5d18b276092c9f9",
            "venue": "2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
            "year": 2021
        },
        {
            "arxivId": "2106.05091",
            "authors": [
                {
                    "authorId": "3436470",
                    "name": "Kimin Lee"
                },
                {
                    "authorId": "152447364",
                    "name": "Laura M. Smith"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
            "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
            "url": "https://www.semanticscholar.org/paper/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8",
            "venue": "ICML",
            "year": 2021
        },
        {
            "arxivId": "2010.07467",
            "authors": [
                {
                    "authorId": "1380909956",
                    "name": "Huixin Zhan"
                },
                {
                    "authorId": "1755581",
                    "name": "Feng Tao"
                },
                {
                    "authorId": "33403228",
                    "name": "Yongcan Cao"
                }
            ],
            "doi": "10.1109/LRA.2021.3063927",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "9f17313e0d7b173765800e6f471407544e0f2202",
            "title": "Human-Guided Robot Behavior Learning: A GAN-Assisted Preference-Based Reinforcement Learning Approach",
            "url": "https://www.semanticscholar.org/paper/9f17313e0d7b173765800e6f471407544e0f2202",
            "venue": "IEEE Robotics and Automation Letters",
            "year": 2020
        },
        {
            "arxivId": "2006.14091",
            "authors": [
                {
                    "authorId": "8307674",
                    "name": "Erdem Biyik"
                },
                {
                    "authorId": "2426559",
                    "name": "Dylan P. Losey"
                },
                {
                    "authorId": "147561324",
                    "name": "Malayandi Palan"
                },
                {
                    "authorId": "51217629",
                    "name": "Nicholas C. Landolfi"
                },
                {
                    "authorId": "71979783",
                    "name": "Gleb Shevchuk"
                },
                {
                    "authorId": "1779671",
                    "name": "Dorsa Sadigh"
                }
            ],
            "doi": "10.1177/02783649211041652",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "e5fc53230b7abced54d83058286dfff7f631289d",
            "title": "Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences",
            "url": "https://www.semanticscholar.org/paper/e5fc53230b7abced54d83058286dfff7f631289d",
            "venue": "Int. J. Robotics Res.",
            "year": 2020
        },
        {
            "arxivId": "2006.08910",
            "authors": [
                {
                    "authorId": "2636690",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "2108693711",
                    "name": "Ruosong Wang"
                },
                {
                    "authorId": "40577530",
                    "name": "Lin F. Yang"
                },
                {
                    "authorId": "2109423866",
                    "name": "Aarti Singh"
                },
                {
                    "authorId": "144292541",
                    "name": "A. Dubrawski"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "c4f946b43c372c674f632076163ece7e5d54481b",
            "title": "Preference-based Reinforcement Learning with Finite-Time Guarantees",
            "url": "https://www.semanticscholar.org/paper/c4f946b43c372c674f632076163ece7e5d54481b",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2002.09089",
            "authors": [
                {
                    "authorId": "47627548",
                    "name": "Daniel S. Brown"
                },
                {
                    "authorId": "2056201523",
                    "name": "Russell Coleman"
                },
                {
                    "authorId": "143889804",
                    "name": "R. Srinivasan"
                },
                {
                    "authorId": "2791038",
                    "name": "S. Niekum"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "658018e556484e3d9c6bcc00c726bf5eb503ef86",
            "title": "Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences",
            "url": "https://www.semanticscholar.org/paper/658018e556484e3d9c6bcc00c726bf5eb503ef86",
            "venue": "ICML",
            "year": 2020
        },
        {
            "arxivId": "1910.04365",
            "authors": [
                {
                    "authorId": "8307674",
                    "name": "Erdem Biyik"
                },
                {
                    "authorId": "147561324",
                    "name": "Malayandi Palan"
                },
                {
                    "authorId": "51217629",
                    "name": "Nicholas C. Landolfi"
                },
                {
                    "authorId": "2426559",
                    "name": "Dylan P. Losey"
                },
                {
                    "authorId": "1779671",
                    "name": "Dorsa Sadigh"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "8cc77d98ea62a4ad0515e74dcd2635a0d7b338d3",
            "title": "Asking Easy Questions: A User-Friendly Approach to Active Reward Learning",
            "url": "https://www.semanticscholar.org/paper/8cc77d98ea62a4ad0515e74dcd2635a0d7b338d3",
            "venue": "CoRL",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "47627548",
                    "name": "Daniel S. Brown"
                },
                {
                    "authorId": "3461969",
                    "name": "Wonjoon Goo"
                },
                {
                    "authorId": "2791038",
                    "name": "S. Niekum"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "9e13129b14166291892ce0b3a0ee90c02dc7625f",
            "title": "Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations",
            "url": "https://www.semanticscholar.org/paper/9e13129b14166291892ce0b3a0ee90c02dc7625f",
            "venue": "CoRL",
            "year": 2019
        },
        {
            "arxivId": "1904.06387",
            "authors": [
                {
                    "authorId": "47627548",
                    "name": "Daniel S. Brown"
                },
                {
                    "authorId": "3461969",
                    "name": "Wonjoon Goo"
                },
                {
                    "authorId": "40608791",
                    "name": "P. Nagarajan"
                },
                {
                    "authorId": "2791038",
                    "name": "S. Niekum"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d",
            "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
            "url": "https://www.semanticscholar.org/paper/2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d",
            "venue": "ICML",
            "year": 2019
        },
        {
            "arxivId": "1901.10995",
            "authors": [
                {
                    "authorId": "66821245",
                    "name": "Adrien Ecoffet"
                },
                {
                    "authorId": "39378983",
                    "name": "Joost Huizinga"
                },
                {
                    "authorId": "39799304",
                    "name": "J. Lehman"
                },
                {
                    "authorId": "1846883",
                    "name": "Kenneth O. Stanley"
                },
                {
                    "authorId": "2552141",
                    "name": "J. Clune"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "c520bf47db3360ae3a52219771390a354ed8a91f",
            "title": "Go-Explore: a New Approach for Hard-Exploration Problems",
            "url": "https://www.semanticscholar.org/paper/c520bf47db3360ae3a52219771390a354ed8a91f",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": "1811.06521",
            "authors": [
                {
                    "authorId": "6675568",
                    "name": "Borja Ibarz"
                },
                {
                    "authorId": "2990741",
                    "name": "J. Leike"
                },
                {
                    "authorId": "3408089",
                    "name": "Tobias Pohlen"
                },
                {
                    "authorId": "2060655766",
                    "name": "G. Irving"
                },
                {
                    "authorId": "34313265",
                    "name": "S. Legg"
                },
                {
                    "authorId": "2330246606",
                    "name": "Dario Amodei"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
            "title": "Reward learning from human preferences and demonstrations in Atari",
            "url": "https://www.semanticscholar.org/paper/3e6cde685fdf321d7edf9319f7b07c01ff79c11a",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "arxivId": "1801.01290",
            "authors": [
                {
                    "authorId": "2587648",
                    "name": "Tuomas Haarnoja"
                },
                {
                    "authorId": "35499972",
                    "name": "Aurick Zhou"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "811df72e210e20de99719539505da54762a11c6d",
            "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
            "url": "https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d",
            "venue": "ICML",
            "year": 2018
        },
        {
            "arxivId": "1709.10163",
            "authors": [
                {
                    "authorId": "1938253",
                    "name": "Garrett Warnell"
                },
                {
                    "authorId": "3436871",
                    "name": "Nicholas R. Waytowich"
                },
                {
                    "authorId": "2194602",
                    "name": "Vernon J. Lawhern"
                },
                {
                    "authorId": "144848112",
                    "name": "P. Stone"
                }
            ],
            "doi": "10.1609/aaai.v32i1.11485",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "abcf11a9af3d83f85c5fbfffc5901d416ca7a73f",
            "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces",
            "url": "https://www.semanticscholar.org/paper/abcf11a9af3d83f85c5fbfffc5901d416ca7a73f",
            "venue": "AAAI",
            "year": 2017
        },
        {
            "arxivId": "1707.06347",
            "authors": [
                {
                    "authorId": "47971768",
                    "name": "John Schulman"
                },
                {
                    "authorId": "143909660",
                    "name": "Filip Wolski"
                },
                {
                    "authorId": "6515819",
                    "name": "Prafulla Dhariwal"
                },
                {
                    "authorId": "38909097",
                    "name": "Alec Radford"
                },
                {
                    "authorId": "2067138712",
                    "name": "Oleg Klimov"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
            "title": "Proximal Policy Optimization Algorithms",
            "url": "https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
            "venue": "ArXiv",
            "year": 2017
        },
        {
            "arxivId": "1706.03741",
            "authors": [
                {
                    "authorId": "145791315",
                    "name": "P. Christiano"
                },
                {
                    "authorId": "2990741",
                    "name": "J. Leike"
                },
                {
                    "authorId": "31035595",
                    "name": "Tom B. Brown"
                },
                {
                    "authorId": "26890260",
                    "name": "Miljan Martic"
                },
                {
                    "authorId": "34313265",
                    "name": "S. Legg"
                },
                {
                    "authorId": "2330246606",
                    "name": "Dario Amodei"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
            "title": "Deep Reinforcement Learning from Human Preferences",
            "url": "https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
            "venue": "NIPS",
            "year": 2017
        },
        {
            "arxivId": "1606.06565",
            "authors": [
                {
                    "authorId": "2698777",
                    "name": "Dario Amodei"
                },
                {
                    "authorId": "37232298",
                    "name": "C. Olah"
                },
                {
                    "authorId": "5164568",
                    "name": "J. Steinhardt"
                },
                {
                    "authorId": "145791315",
                    "name": "P. Christiano"
                },
                {
                    "authorId": "47971768",
                    "name": "John Schulman"
                },
                {
                    "authorId": "30415265",
                    "name": "Dandelion Man\u00e9"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7",
            "title": "Concrete Problems in AI Safety",
            "url": "https://www.semanticscholar.org/paper/e86f71ca2948d17b003a5f068db1ecb2b77827f7",
            "venue": "ArXiv",
            "year": 2016
        },
        {
            "arxivId": "1603.00448",
            "authors": [
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
            "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
            "venue": "ICML",
            "year": 2016
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "1885349",
                    "name": "Aja Huang"
                },
                {
                    "authorId": "2772217",
                    "name": "Chris J. Maddison"
                },
                {
                    "authorId": "35099444",
                    "name": "A. Guez"
                },
                {
                    "authorId": "2175946",
                    "name": "L. Sifre"
                },
                {
                    "authorId": "47568983",
                    "name": "George van den Driessche"
                },
                {
                    "authorId": "4337102",
                    "name": "Julian Schrittwieser"
                },
                {
                    "authorId": "2460849",
                    "name": "Ioannis Antonoglou"
                },
                {
                    "authorId": "2749418",
                    "name": "Vedavyas Panneershelvam"
                },
                {
                    "authorId": "1975889",
                    "name": "Marc Lanctot"
                },
                {
                    "authorId": "48373216",
                    "name": "S. Dieleman"
                },
                {
                    "authorId": "2401609",
                    "name": "Dominik Grewe"
                },
                {
                    "authorId": "4111313",
                    "name": "John Nham"
                },
                {
                    "authorId": "2583391",
                    "name": "Nal Kalchbrenner"
                },
                {
                    "authorId": "1701686",
                    "name": "I. Sutskever"
                },
                {
                    "authorId": "2542999",
                    "name": "T. Lillicrap"
                },
                {
                    "authorId": "40662181",
                    "name": "M. Leach"
                },
                {
                    "authorId": "2645384",
                    "name": "K. Kavukcuoglu"
                },
                {
                    "authorId": "1686971",
                    "name": "T. Graepel"
                },
                {
                    "authorId": "48987704",
                    "name": "D. Hassabis"
                }
            ],
            "doi": "10.1038/nature16961",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
            "title": "Mastering the game of Go with deep neural networks and tree search",
            "url": "https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490",
            "venue": "Nature",
            "year": 2016
        },
        {
            "arxivId": "1509.02971",
            "authors": [
                {
                    "authorId": "2542999",
                    "name": "T. Lillicrap"
                },
                {
                    "authorId": "2323922",
                    "name": "Jonathan J. Hunt"
                },
                {
                    "authorId": "1863250",
                    "name": "A. Pritzel"
                },
                {
                    "authorId": "2801204",
                    "name": "N. Heess"
                },
                {
                    "authorId": "1968210",
                    "name": "Tom Erez"
                },
                {
                    "authorId": "2109481",
                    "name": "Yuval Tassa"
                },
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "1688276",
                    "name": "Daan Wierstra"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
            "title": "Continuous control with deep reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f",
            "venue": "ICLR",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1973021",
                    "name": "C. Celemin"
                },
                {
                    "authorId": "1399026476",
                    "name": "J. Ruiz-del-Solar"
                }
            ],
            "doi": "10.1109/ICAR.2015.7251514",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "bbab8ab3976d6887e133c33e33ff77e3da1608c7",
            "title": "COACH: Learning continuous actions from COrrective Advice Communicated by Humans",
            "url": "https://www.semanticscholar.org/paper/bbab8ab3976d6887e133c33e33ff77e3da1608c7",
            "venue": "2015 International Conference on Advanced Robotics (ICAR)",
            "year": 2015
        },
        {
            "arxivId": "1507.04888",
            "authors": [
                {
                    "authorId": "3331786",
                    "name": "Markus Wulfmeier"
                },
                {
                    "authorId": "3214791",
                    "name": "Peter Ondruska"
                },
                {
                    "authorId": "1834086",
                    "name": "I. Posner"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "9ba266a4a4644e877fc37a64be3beddce8904cf7",
            "title": "Maximum Entropy Deep Inverse Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/9ba266a4a4644e877fc37a64be3beddce8904cf7",
            "venue": "",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3255983",
                    "name": "Volodymyr Mnih"
                },
                {
                    "authorId": "2645384",
                    "name": "K. Kavukcuoglu"
                },
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "2228824",
                    "name": "Andrei A. Rusu"
                },
                {
                    "authorId": "144056327",
                    "name": "J. Veness"
                },
                {
                    "authorId": "1792298",
                    "name": "Marc G. Bellemare"
                },
                {
                    "authorId": "1753223",
                    "name": "Alex Graves"
                },
                {
                    "authorId": "3137672",
                    "name": "Martin A. Riedmiller"
                },
                {
                    "authorId": "145600108",
                    "name": "A. Fidjeland"
                },
                {
                    "authorId": "2273072",
                    "name": "Georg Ostrovski"
                },
                {
                    "authorId": "48348688",
                    "name": "Stig Petersen"
                },
                {
                    "authorId": "50388928",
                    "name": "Charlie Beattie"
                },
                {
                    "authorId": "49813280",
                    "name": "Amir Sadik"
                },
                {
                    "authorId": "2460849",
                    "name": "Ioannis Antonoglou"
                },
                {
                    "authorId": "143776287",
                    "name": "Helen King"
                },
                {
                    "authorId": "2106164",
                    "name": "D. Kumaran"
                },
                {
                    "authorId": "1688276",
                    "name": "Daan Wierstra"
                },
                {
                    "authorId": "34313265",
                    "name": "S. Legg"
                },
                {
                    "authorId": "48987704",
                    "name": "D. Hassabis"
                }
            ],
            "doi": "10.1038/nature14236",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
            "title": "Human-level control through deep reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/340f48901f72278f6bf78a04ee5b01df208cc508",
            "venue": "Nature",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2056578",
                    "name": "Jaedeug Choi"
                },
                {
                    "authorId": "1741330",
                    "name": "Kee-Eung Kim"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "91eefd86f01f9979986a529a1ee30b94a1addef5",
            "title": "Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions",
            "url": "https://www.semanticscholar.org/paper/91eefd86f01f9979986a529a1ee30b94a1addef5",
            "venue": "NIPS",
            "year": 2012
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "1986848",
                    "name": "Zoran Popovic"
                },
                {
                    "authorId": "145231047",
                    "name": "V. Koltun"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "1e045f3447f69d9a7cac18ef23062ea8dd661285",
            "title": "Nonlinear Inverse Reinforcement Learning with Gaussian Processes",
            "url": "https://www.semanticscholar.org/paper/1e045f3447f69d9a7cac18ef23062ea8dd661285",
            "venue": "NIPS",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2056578",
                    "name": "Jaedeug Choi"
                },
                {
                    "authorId": "1741330",
                    "name": "Kee-Eung Kim"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "8c802ca0f26177d2dda7664778ab7bb3337edfb7",
            "title": "MAP Inference for Bayesian Inverse Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/8c802ca0f26177d2dda7664778ab7bb3337edfb7",
            "venue": "NIPS",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144288136",
                    "name": "W. B. Knox"
                },
                {
                    "authorId": "144848112",
                    "name": "P. Stone"
                }
            ],
            "doi": "10.1145/1597735.1597738",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "256c3bd45ab7452bb51721eb25d3367bb654225e",
            "title": "Interactively shaping agents via human reinforcement: the TAMER framework",
            "url": "https://www.semanticscholar.org/paper/256c3bd45ab7452bb51721eb25d3367bb654225e",
            "venue": "K-CAP '09",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1753269",
                    "name": "Brian D. Ziebart"
                },
                {
                    "authorId": "2306081367",
                    "name": "Andrew L. Maas"
                },
                {
                    "authorId": "1756566",
                    "name": "J. Bagnell"
                },
                {
                    "authorId": "144021446",
                    "name": "A. Dey"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
            "title": "Maximum Entropy Inverse Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/c8221c054459e37edbf313668523d667fe5c1536",
            "venue": "AAAI",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2274296969",
                    "name": "Andrew Y. Ng"
                },
                {
                    "authorId": "2274384301",
                    "name": "Stuart Russell"
                }
            ],
            "doi": "10.2460/AJVR.67.2.323",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "dd6960d576da0d769ca30de6eb6607a66806211f",
            "title": "Algorithms for Inverse Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/dd6960d576da0d769ca30de6eb6607a66806211f",
            "venue": "ICML 2000",
            "year": 2000
        },
        {
            "arxivId": "2203.10050",
            "authors": [
                {
                    "authorId": "2109073979",
                    "name": "Jongjin Park"
                },
                {
                    "authorId": "2067714176",
                    "name": "Younggyo Seo"
                },
                {
                    "authorId": "143720148",
                    "name": "Jinwoo Shin"
                },
                {
                    "authorId": "2118338545",
                    "name": "Honglak Lee"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "3436470",
                    "name": "Kimin Lee"
                }
            ],
            "doi": "10.48550/arXiv.2203.10050",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "f2a2401a35b6b892d43642b31700e83e88b2ebb8",
            "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/f2a2401a35b6b892d43642b31700e83e88b2ebb8",
            "venue": "ICLR",
            "year": 2022
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2550764",
                    "name": "Justin Fu"
                },
                {
                    "authorId": "2257221234",
                    "name": "Katie Luo"
                },
                {
                    "authorId": "2257194327",
                    "name": "Sergey Levine"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "941ba185f01b1a0a27453fd178aa5f010510ee8b",
            "title": "Learning Robust Rewards with Adverserial Inverse Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/941ba185f01b1a0a27453fd178aa5f010510ee8b",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144389295",
                    "name": "Christian Wirth"
                },
                {
                    "authorId": "1688702",
                    "name": "R. Akrour"
                },
                {
                    "authorId": "26599977",
                    "name": "G. Neumann"
                },
                {
                    "authorId": "1747752",
                    "name": "Johannes F\u00fcrnkranz"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0",
            "title": "A Survey of Preference-Based Reinforcement Learning Methods",
            "url": "https://www.semanticscholar.org/paper/84082634110fcedaaa32632f6cc16a034eedb2a0",
            "venue": "J. Mach. Learn. Res.",
            "year": 2017
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "title": "Rating-based Reinforcement Learning",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/8e0e795463a2007497c9c257f9de337c53f1c4b9",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023
}