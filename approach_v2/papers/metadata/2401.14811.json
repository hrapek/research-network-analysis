{
    "abstract": "In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms.",
    "arxivId": "2401.14811",
    "authors": [
        {
            "authorId": "147156275",
            "name": "Joar Skalse",
            "url": "https://www.semanticscholar.org/author/147156275"
        },
        {
            "authorId": "144938187",
            "name": "A. Abate",
            "url": "https://www.semanticscholar.org/author/144938187"
        }
    ],
    "citationVelocity": 0,
    "citations": [
        {
            "arxivId": "2411.17749",
            "authors": [
                {
                    "authorId": "2332538978",
                    "name": "Andrew Garber"
                },
                {
                    "authorId": "2259929484",
                    "name": "Rohan Subramani"
                },
                {
                    "authorId": "2332538543",
                    "name": "Linus Luu"
                },
                {
                    "authorId": "2248780238",
                    "name": "Mark Bedaywi"
                },
                {
                    "authorId": "2332539466",
                    "name": "Stuart Russell"
                },
                {
                    "authorId": "2237427074",
                    "name": "Scott Emmons"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "1ef44575869ecc361f473754cc2e4f7b736055ac",
            "title": "Will an AI with Private Information Allow Itself to Be Switched Off?",
            "url": "https://www.semanticscholar.org/paper/1ef44575869ecc361f473754cc2e4f7b736055ac",
            "venue": "",
            "year": 2024
        },
        {
            "arxivId": "2406.07295",
            "authors": [
                {
                    "authorId": "2305727002",
                    "name": "Marcus Williams"
                }
            ],
            "doi": "10.48550/arXiv.2406.07295",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "57d32f83c01fe7d84005b226950f369dcbfb7483",
            "title": "Multi-objective Reinforcement learning from AI Feedback",
            "url": "https://www.semanticscholar.org/paper/57d32f83c01fe7d84005b226950f369dcbfb7483",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2405.06624",
            "authors": [
                {
                    "authorId": "2307266086",
                    "name": "David Dalrymple"
                },
                {
                    "authorId": "147156275",
                    "name": "Joar Skalse"
                },
                {
                    "authorId": "1865800402",
                    "name": "Y. Bengio"
                },
                {
                    "authorId": "2294870297",
                    "name": "Stuart Russell"
                },
                {
                    "authorId": "2301017257",
                    "name": "Max Tegmark"
                },
                {
                    "authorId": "1775517",
                    "name": "S. Seshia"
                },
                {
                    "authorId": "2237802754",
                    "name": "Steve Omohundro"
                },
                {
                    "authorId": "2301018099",
                    "name": "Christian Szegedy"
                },
                {
                    "authorId": "2301017668",
                    "name": "Ben Goldhaber"
                },
                {
                    "authorId": "2301018296",
                    "name": "Nora Ammann"
                },
                {
                    "authorId": "2247589240",
                    "name": "Alessandro Abate"
                },
                {
                    "authorId": "2301018352",
                    "name": "Joe Halpern"
                },
                {
                    "authorId": "2301018627",
                    "name": "Clark Barrett"
                },
                {
                    "authorId": "2301110889",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "120636597",
                    "name": "Tan Zhi-Xuan"
                },
                {
                    "authorId": "2301018066",
                    "name": "Jeannette Wing"
                },
                {
                    "authorId": "2284592680",
                    "name": "Joshua B. Tenenbaum"
                }
            ],
            "doi": "10.48550/arXiv.2405.06624",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "d81e8d1d3c8acbcf9755ef470098b9c02adf5963",
            "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems",
            "url": "https://www.semanticscholar.org/paper/d81e8d1d3c8acbcf9755ef470098b9c02adf5963",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2404.19370",
            "authors": [
                {
                    "authorId": "2298968368",
                    "name": "Kristina Levina"
                },
                {
                    "authorId": "2298968876",
                    "name": "Nikolaos Pappas"
                },
                {
                    "authorId": "2860340",
                    "name": "Athanasios Karapantelakis"
                },
                {
                    "authorId": "2287690603",
                    "name": "Aneta Vulgarakis Feljan"
                },
                {
                    "authorId": "2357436",
                    "name": "Jendrik Seipp"
                }
            ],
            "doi": "10.48550/arXiv.2404.19370",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "e350304f3f9ed48e14fc205f67bc44191eac6883",
            "title": "Numeric Reward Machines",
            "url": "https://www.semanticscholar.org/paper/e350304f3f9ed48e14fc205f67bc44191eac6883",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2311.01990",
            "authors": [
                {
                    "authorId": "2265387552",
                    "name": "Jonathan Colaco Carr"
                },
                {
                    "authorId": "2249830094",
                    "name": "Prakash Panangaden"
                },
                {
                    "authorId": "2249762747",
                    "name": "D. Precup"
                }
            ],
            "doi": "10.48550/arXiv.2311.01990",
            "intent": [
                "result"
            ],
            "isInfluential": false,
            "paperId": "e6a207308c3f898d2c97ddf771c0060c23b93fe5",
            "title": "Conditions on Preference Relations that Guarantee the Existence of Optimal Policies",
            "url": "https://www.semanticscholar.org/paper/e6a207308c3f898d2c97ddf771c0060c23b93fe5",
            "venue": "AISTATS",
            "year": 2023
        },
        {
            "arxivId": "2310.00435",
            "authors": [
                {
                    "authorId": "32305445",
                    "name": "Silviu Pitis"
                }
            ],
            "doi": "10.48550/arXiv.2310.00435",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "d6f452265914d8f5ecf185f999170f6a106602e8",
            "title": "Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards",
            "url": "https://www.semanticscholar.org/paper/d6f452265914d8f5ecf185f999170f6a106602e8",
            "venue": "NeurIPS",
            "year": 2023
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2136937270",
                    "name": "G. D. Giacomo"
                },
                {
                    "authorId": "2259946773",
                    "name": "Marco Favorito"
                },
                {
                    "authorId": "2130048573",
                    "name": "Luciana Silo"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "60abe0dc677021fbe67c5e0ba9b1bd96fb04c07c",
            "title": "for LTL f Goal Speci\ufb01cations",
            "url": "https://www.semanticscholar.org/paper/60abe0dc677021fbe67c5e0ba9b1bd96fb04c07c",
            "venue": "",
            "year": null
        }
    ],
    "corpusId": 260814902,
    "doi": "10.48550/arXiv.2401.14811",
    "fieldsOfStudy": [
        "Computer Science"
    ],
    "influentialCitationCount": 0,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 7,
    "numCiting": 43,
    "paperId": "99d61aa0894413aac41892eb98d9a4c66eb72bf6",
    "references": [
        {
            "arxivId": "2307.12184",
            "authors": [
                {
                    "authorId": "9584679",
                    "name": "Shuwa Miura"
                }
            ],
            "doi": "10.48550/arXiv.2307.12184",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "6bdf93b437d32ca866849074b1c163b2efc0a07e",
            "title": "On the Expressivity of Multidimensional Markov Reward",
            "url": "https://www.semanticscholar.org/paper/6bdf93b437d32ca866849074b1c163b2efc0a07e",
            "venue": "ArXiv",
            "year": 2023
        },
        {
            "arxivId": "2212.03201",
            "authors": [
                {
                    "authorId": "147156275",
                    "name": "Joar Skalse"
                },
                {
                    "authorId": "144938187",
                    "name": "A. Abate"
                }
            ],
            "doi": "10.48550/arXiv.2212.03201",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "aae382fe95b061018f214b1f101c55b6f4ae176b",
            "title": "Misspecification in Inverse Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/aae382fe95b061018f214b1f101c55b6f4ae176b",
            "venue": "AAAI",
            "year": 2022
        },
        {
            "arxivId": "2209.13085",
            "authors": [
                {
                    "authorId": "147156275",
                    "name": "Joar Skalse"
                },
                {
                    "authorId": "2155647347",
                    "name": "Nikolaus H. R. Howe"
                },
                {
                    "authorId": "52510051",
                    "name": "Dmitrii Krasheninnikov"
                },
                {
                    "authorId": "145055042",
                    "name": "David Krueger"
                }
            ],
            "doi": "10.48550/arXiv.2209.13085",
            "intent": [],
            "isInfluential": false,
            "paperId": "004357dd9bbf3012c8fe0ccada4da401bf85dfff",
            "title": "Defining and Characterizing Reward Hacking",
            "url": "https://www.semanticscholar.org/paper/004357dd9bbf3012c8fe0ccada4da401bf85dfff",
            "venue": "ArXiv",
            "year": 2022
        },
        {
            "arxivId": "2208.11838",
            "authors": [
                {
                    "authorId": "144938187",
                    "name": "A. Abate"
                },
                {
                    "authorId": "104283116",
                    "name": "Y. Almulla"
                },
                {
                    "authorId": "2105894098",
                    "name": "James Fox"
                },
                {
                    "authorId": "2182691758",
                    "name": "David Hyland"
                },
                {
                    "authorId": "2059979020",
                    "name": "M. Wooldridge"
                }
            ],
            "doi": "10.48550/arXiv.2208.11838",
            "intent": [
                "result"
            ],
            "isInfluential": false,
            "paperId": "1046565b3eeae47298bd751162f2c13902af299f",
            "title": "Learning Task Automata for Reinforcement Learning using Hidden Markov Models",
            "url": "https://www.semanticscholar.org/paper/1046565b3eeae47298bd751162f2c13902af299f",
            "venue": "ECAI",
            "year": 2022
        },
        {
            "arxivId": "2212.13769",
            "authors": [
                {
                    "authorId": "147156275",
                    "name": "Joar Skalse"
                },
                {
                    "authorId": "84379741",
                    "name": "Lewis Hammond"
                },
                {
                    "authorId": "2052541890",
                    "name": "Charlie Griffin"
                },
                {
                    "authorId": "144938187",
                    "name": "A. Abate"
                }
            ],
            "doi": "10.24963/ijcai.2022/476",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "b570223a5c69dae4f74c9b2e7d3122494f26e8ae",
            "title": "Lexicographic Multi-Objective Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/b570223a5c69dae4f74c9b2e7d3122494f26e8ae",
            "venue": "IJCAI",
            "year": 2022
        },
        {
            "arxivId": "2206.13637",
            "authors": [
                {
                    "authorId": "1411003408",
                    "name": "Mehran Shakerinava"
                },
                {
                    "authorId": "2111187",
                    "name": "Siamak Ravanbakhsh"
                }
            ],
            "doi": "10.48550/arXiv.2206.13637",
            "intent": [
                "background",
                "result",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "dd2d785622f2e3ce9a47c8a11788cff312b23529",
            "title": "Utility Theory for Sequential Decision Making",
            "url": "https://www.semanticscholar.org/paper/dd2d785622f2e3ce9a47c8a11788cff312b23529",
            "venue": "ICML",
            "year": 2022
        },
        {
            "arxivId": "2202.01511",
            "authors": [
                {
                    "authorId": "51004138",
                    "name": "Mirco Mutti"
                },
                {
                    "authorId": "120592807",
                    "name": "Ric De Santi"
                },
                {
                    "authorId": "2152233139",
                    "name": "Piersilvio De  Bartolomeis"
                },
                {
                    "authorId": "1792167",
                    "name": "Marcello Restelli"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "fa950a27da0281095f7be7d8a2224391dcbd247b",
            "title": "Challenging Common Assumptions in Convex Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/fa950a27da0281095f7be7d8a2224391dcbd247b",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "arxivId": "2112.15422",
            "authors": [
                {
                    "authorId": "1990124",
                    "name": "P. Vamplew"
                },
                {
                    "authorId": "2157116927",
                    "name": "Benjamin J. Smith"
                },
                {
                    "authorId": "2054580965",
                    "name": "Johan Kallstrom"
                },
                {
                    "authorId": "123143505",
                    "name": "G. Ramos"
                },
                {
                    "authorId": "40464454",
                    "name": "Roxana R\u0103dulescu"
                },
                {
                    "authorId": "1917202",
                    "name": "D. Roijers"
                },
                {
                    "authorId": "2082425055",
                    "name": "Conor F. Hayes"
                },
                {
                    "authorId": "1711918",
                    "name": "F. Heintz"
                },
                {
                    "authorId": "46964021",
                    "name": "P. Mannion"
                },
                {
                    "authorId": "2367164",
                    "name": "Pieter J. K. Libin"
                },
                {
                    "authorId": "3327913",
                    "name": "Richard Dazeley"
                },
                {
                    "authorId": "2763108",
                    "name": "Cameron Foale"
                }
            ],
            "doi": "10.1007/s10458-022-09575-5",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "443d21148f28d56b8d52a1fbb49971956efed999",
            "title": "Scalar reward is not enough: a response to Silver, Singh, Precup and Sutton (2021)",
            "url": "https://www.semanticscholar.org/paper/443d21148f28d56b8d52a1fbb49971956efed999",
            "venue": "Autonomous Agents and Multi-Agent Systems",
            "year": 2021
        },
        {
            "arxivId": "2111.00876",
            "authors": [
                {
                    "authorId": "152422014",
                    "name": "David Abel"
                },
                {
                    "authorId": "2605877",
                    "name": "Will Dabney"
                },
                {
                    "authorId": "3134710",
                    "name": "A. Harutyunyan"
                },
                {
                    "authorId": "2543534",
                    "name": "Mark K. Ho"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "144368601",
                    "name": "Doina Precup"
                },
                {
                    "authorId": "2108384183",
                    "name": "Satinder Singh"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "567abe32df6dfe7d6ba19cbcb8e6426262902821",
            "title": "On the Expressivity of Markov Reward",
            "url": "https://www.semanticscholar.org/paper/567abe32df6dfe7d6ba19cbcb8e6426262902821",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "arxivId": "2106.03787",
            "authors": [
                {
                    "authorId": "1737555",
                    "name": "M. Geist"
                },
                {
                    "authorId": "2064800890",
                    "name": "Julien P'erolat"
                },
                {
                    "authorId": "2902651",
                    "name": "M. Lauri\u00e8re"
                },
                {
                    "authorId": "47431108",
                    "name": "R. \u00c9lie"
                },
                {
                    "authorId": "21534787",
                    "name": "Sarah Perrin"
                },
                {
                    "authorId": "1936951",
                    "name": "Olivier Bachem"
                },
                {
                    "authorId": "1708654",
                    "name": "R. Munos"
                },
                {
                    "authorId": "1721354",
                    "name": "O. Pietquin"
                }
            ],
            "doi": "10.5555/3535850.3535906",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "046bc091cbe8354d965cb157a44a9934621198ad",
            "title": "Concave Utility Reinforcement Learning: the Mean-field Game viewpoint",
            "url": "https://www.semanticscholar.org/paper/046bc091cbe8354d965cb157a44a9934621198ad",
            "venue": "AAMAS",
            "year": 2021
        },
        {
            "arxivId": "2106.00661",
            "authors": [
                {
                    "authorId": "3331540",
                    "name": "Tom Zahavy"
                },
                {
                    "authorId": "1389654226",
                    "name": "Brendan O'Donoghue"
                },
                {
                    "authorId": "2755582",
                    "name": "Guillaume Desjardins"
                },
                {
                    "authorId": "2108384183",
                    "name": "Satinder Singh"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "5c37023c35fc1c95565d56b4fc4821fcf768651a",
            "title": "Reward is enough for convex MDPs",
            "url": "https://www.semanticscholar.org/paper/5c37023c35fc1c95565d56b4fc4821fcf768651a",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "arxivId": "2010.07877",
            "authors": [
                {
                    "authorId": "2578985",
                    "name": "Victoria Krakovna"
                },
                {
                    "authorId": "1749270",
                    "name": "Laurent Orseau"
                },
                {
                    "authorId": "2047148926",
                    "name": "Richard Ngo"
                },
                {
                    "authorId": "26890260",
                    "name": "Miljan Martic"
                },
                {
                    "authorId": "34313265",
                    "name": "S. Legg"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "532d734c9d622cc6dd6aa6cb6b4dce032ad591aa",
            "title": "Avoiding Side Effects By Considering Future Tasks",
            "url": "https://www.semanticscholar.org/paper/532d734c9d622cc6dd6aa6cb6b4dce032ad591aa",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2010.03950",
            "authors": [
                {
                    "authorId": "15316342",
                    "name": "Rodrigo Toro Icarte"
                },
                {
                    "authorId": "1758085",
                    "name": "Toryn Q. Klassen"
                },
                {
                    "authorId": "2682734",
                    "name": "R. Valenzano"
                },
                {
                    "authorId": "1683896",
                    "name": "Sheila A. McIlraith"
                }
            ],
            "doi": "10.1613/jair.1.12440",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "6778d6a0f959cdcc42718ee9fc279fd1f00f3d88",
            "title": "Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/6778d6a0f959cdcc42718ee9fc279fd1f00f3d88",
            "venue": "J. Artif. Intell. Res.",
            "year": 2020
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "35302287",
                    "name": "Mohammadhosein Hasanbeig"
                },
                {
                    "authorId": "145763733",
                    "name": "D. Kroening"
                },
                {
                    "authorId": "144938187",
                    "name": "A. Abate"
                }
            ],
            "doi": "10.1007/978-3-030-57628-8_1",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "e3450150135b97ad379cc35fe44afde021c37d44",
            "title": "Deep Reinforcement Learning with Temporal Logics",
            "url": "https://www.semanticscholar.org/paper/e3450150135b97ad379cc35fe44afde021c37d44",
            "venue": "FORMATS",
            "year": 2020
        },
        {
            "arxivId": "2007.02151",
            "authors": [
                {
                    "authorId": "1753620840",
                    "name": "Junyu Zhang"
                },
                {
                    "authorId": "2063871",
                    "name": "Alec Koppel"
                },
                {
                    "authorId": "3387859",
                    "name": "A. S. Bedi"
                },
                {
                    "authorId": "40868287",
                    "name": "Csaba Szepesvari"
                },
                {
                    "authorId": "145731462",
                    "name": "Mengdi Wang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
            "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities",
            "url": "https://www.semanticscholar.org/paper/6bf9b58b8f418fb2922762550fb78bb22c83c0f8",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2006.06547",
            "authors": [
                {
                    "authorId": "49277224",
                    "name": "A. Turner"
                },
                {
                    "authorId": "13002147",
                    "name": "Neale Ratzlaff"
                },
                {
                    "authorId": "1729906",
                    "name": "Prasad Tadepalli"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "b6f028e8611417d813ed7939f2434e3fb8c1d641",
            "title": "Avoiding Side Effects in Complex Environments",
            "url": "https://www.semanticscholar.org/paper/b6f028e8611417d813ed7939f2434e3fb8c1d641",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2004.00273",
            "authors": [
                {
                    "authorId": "2153603617",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2372592",
                    "name": "Nima Roohi"
                },
                {
                    "authorId": "144739836",
                    "name": "Matthew West"
                },
                {
                    "authorId": "37782675",
                    "name": "Mahesh Viswanathan"
                },
                {
                    "authorId": "48834542",
                    "name": "G. Dullerud"
                }
            ],
            "doi": "10.1109/CDC42340.2020.9303982",
            "intent": [],
            "isInfluential": false,
            "paperId": "4b1db9a95c6aad9cab9eb4d54a52c1356fe291da",
            "title": "Statistically Model Checking PCTL Specifications on Markov Decision Processes via Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/4b1db9a95c6aad9cab9eb4d54a52c1356fe291da",
            "venue": "2020 59th IEEE Conference on Decision and Control (CDC)",
            "year": 2020
        },
        {
            "arxivId": "1906.09323",
            "authors": [
                {
                    "authorId": "138875199",
                    "name": "Sobhan Miryoosefi"
                },
                {
                    "authorId": "11963742",
                    "name": "Kiant\u00e9 Brantley"
                },
                {
                    "authorId": "1722360",
                    "name": "Hal Daum\u00e9"
                },
                {
                    "authorId": "144652072",
                    "name": "Miroslav Dud\u00edk"
                },
                {
                    "authorId": "1716301",
                    "name": "R. Schapire"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "9c5ab1e836acdba0c1f91360909e9e81c56d2f15",
            "title": "Reinforcement Learning with Convex Constraints",
            "url": "https://www.semanticscholar.org/paper/9c5ab1e836acdba0c1f91360909e9e81c56d2f15",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": "1905.06466",
            "authors": [
                {
                    "authorId": "145234267",
                    "name": "Wang Chi Cheung"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "9d546ee1a00469e17f236201dda95665f4348e4b",
            "title": "Exploration-Exploitation Trade-off in Reinforcement Learning on Online Markov Decision Processes with Global Concave Rewards",
            "url": "https://www.semanticscholar.org/paper/9d546ee1a00469e17f236201dda95665f4348e4b",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": "1902.02893",
            "authors": [
                {
                    "authorId": "32305445",
                    "name": "Silviu Pitis"
                }
            ],
            "doi": "10.1609/aaai.v33i01.33017949",
            "intent": [
                "background",
                "result",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "0b8c3226fa9da6807aa966172199a9d8d82a4185",
            "title": "Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach",
            "url": "https://www.semanticscholar.org/paper/0b8c3226fa9da6807aa966172199a9d8d82a4185",
            "venue": "AAAI",
            "year": 2019
        },
        {
            "arxivId": "1812.02690",
            "authors": [
                {
                    "authorId": "34840427",
                    "name": "Elad Hazan"
                },
                {
                    "authorId": "144695232",
                    "name": "S. Kakade"
                },
                {
                    "authorId": "2109143637",
                    "name": "Karan Singh"
                },
                {
                    "authorId": "52138406",
                    "name": "A. V. Soest"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "adab275554eb745cdc21fd6c526ec55a5b2ef362",
            "title": "Provably Efficient Maximum Entropy Exploration",
            "url": "https://www.semanticscholar.org/paper/adab275554eb745cdc21fd6c526ec55a5b2ef362",
            "venue": "ICML",
            "year": 2018
        },
        {
            "arxivId": "1806.01186",
            "authors": [
                {
                    "authorId": "2578985",
                    "name": "Victoria Krakovna"
                },
                {
                    "authorId": "1749270",
                    "name": "Laurent Orseau"
                },
                {
                    "authorId": "26890260",
                    "name": "Miljan Martic"
                },
                {
                    "authorId": "34313265",
                    "name": "S. Legg"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "0039dc54599ef609b3e7044f3b7cdfac354f79c2",
            "title": "Penalizing Side Effects using Stepwise Relative Reachability",
            "url": "https://www.semanticscholar.org/paper/0039dc54599ef609b3e7044f3b7cdfac354f79c2",
            "venue": "AISafety@IJCAI",
            "year": 2018
        },
        {
            "arxivId": "1805.11074",
            "authors": [
                {
                    "authorId": "3393407",
                    "name": "Chen Tessler"
                },
                {
                    "authorId": "3187297",
                    "name": "D. Mankowitz"
                },
                {
                    "authorId": "1712535",
                    "name": "Shie Mannor"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "cb7c479a36520da1caeeec67db10772351a390c6",
            "title": "Reward Constrained Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/cb7c479a36520da1caeeec67db10772351a390c6",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "arxivId": "1705.10528",
            "authors": [
                {
                    "authorId": "3381809",
                    "name": "Joshua Achiam"
                },
                {
                    "authorId": "145641013",
                    "name": "David Held"
                },
                {
                    "authorId": "3025260",
                    "name": "Aviv Tamar"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
            "title": "Constrained Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
            "venue": "ICML",
            "year": 2017
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2248497149",
                    "name": "Christopher M. Schulte"
                }
            ],
            "doi": "10.1177/1532708616636615",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "d6f97e84275238836009e549dc5df16e1a6f4c25",
            "title": "Possible Worlds",
            "url": "https://www.semanticscholar.org/paper/d6f97e84275238836009e549dc5df16e1a6f4c25",
            "venue": "",
            "year": 2016
        },
        {
            "arxivId": "1512.01629",
            "authors": [
                {
                    "authorId": "1819830",
                    "name": "Yinlam Chow"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "2542753",
                    "name": "Lucas Janson"
                },
                {
                    "authorId": "1696085",
                    "name": "M. Pavone"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
            "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
            "url": "https://www.semanticscholar.org/paper/759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
            "venue": "J. Mach. Learn. Res.",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2107919222",
                    "name": "Chunming Liu"
                },
                {
                    "authorId": "145880436",
                    "name": "Xin Xu"
                },
                {
                    "authorId": "46570618",
                    "name": "D. Hu"
                }
            ],
            "doi": "10.1109/TSMC.2014.2358639",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "df2da95303bedf417c76fa8439844d671fb056da",
            "title": "Multiobjective Reinforcement Learning: A Comprehensive Overview",
            "url": "https://www.semanticscholar.org/paper/df2da95303bedf417c76fa8439844d671fb056da",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
            "year": 2015
        },
        {
            "arxivId": "1402.0590",
            "authors": [
                {
                    "authorId": "1917202",
                    "name": "D. Roijers"
                },
                {
                    "authorId": "1990124",
                    "name": "P. Vamplew"
                },
                {
                    "authorId": "1766767",
                    "name": "Shimon Whiteson"
                },
                {
                    "authorId": "3327913",
                    "name": "Richard Dazeley"
                }
            ],
            "doi": "10.1613/jair.3987",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "a82795a372cec5266e7247054e9aace8f3b6a4f3",
            "title": "A Survey of Multi-Objective Sequential Decision-Making",
            "url": "https://www.semanticscholar.org/paper/a82795a372cec5266e7247054e9aace8f3b6a4f3",
            "venue": "J. Artif. Intell. Res.",
            "year": 2013
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "5403194",
                    "name": "C. Baier"
                },
                {
                    "authorId": "1727089",
                    "name": "J. Katoen"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "f1269591359fddc20f95da10c7bd4c054080b447",
            "title": "Principles of model checking",
            "url": "https://www.semanticscholar.org/paper/f1269591359fddc20f95da10c7bd4c054080b447",
            "venue": "",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2300122018",
                    "name": "Robert Leonard"
                }
            ],
            "doi": "10.1215/00182702-38-1-189",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "a4d99468a565310c7396869b3b8db814d28f3576",
            "title": "Theory of Games and Economic Behavior",
            "url": "https://www.semanticscholar.org/paper/a4d99468a565310c7396869b3b8db814d28f3576",
            "venue": "",
            "year": 2006
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1699868",
                    "name": "Satinder Singh"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "40868287",
                    "name": "Csaba Szepesvari"
                }
            ],
            "doi": "10.1023/A:1007678930559",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "712ec1bd9287ac210a7630ce03ca2b0930ebd351",
            "title": "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms",
            "url": "https://www.semanticscholar.org/paper/712ec1bd9287ac210a7630ce03ca2b0930ebd351",
            "venue": "Machine Learning",
            "year": 2000
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "34699434",
                    "name": "A. Ng"
                },
                {
                    "authorId": "1868677",
                    "name": "Daishi Harada"
                },
                {
                    "authorId": "145107462",
                    "name": "Stuart J. Russell"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "94066dc12fe31e96af7557838159bde598cb4f10",
            "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping",
            "url": "https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10",
            "venue": "ICML",
            "year": 1999
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "48948439",
                    "name": "J. Harsanyi"
                }
            ],
            "doi": "10.1086/257678",
            "intent": [
                "result"
            ],
            "isInfluential": true,
            "paperId": "03bece2ebaee1b46305c5684705036bf483f4af3",
            "title": "Cardinal Welfare, Individualistic Ethics, and Interpersonal Comparisons of Utility",
            "url": "https://www.semanticscholar.org/paper/03bece2ebaee1b46305c5684705036bf483f4af3",
            "venue": "Journal of Political Economy",
            "year": 1955
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "32305445",
                    "name": "Silviu Pitis"
                },
                {
                    "authorId": "2167321384",
                    "name": "D. Bailey"
                },
                {
                    "authorId": "2503659",
                    "name": "Jimmy Ba"
                }
            ],
            "doi": null,
            "intent": [
                "result"
            ],
            "isInfluential": false,
            "paperId": "0fc6d2ac846831b8a392f2a2436bf380c0147f4b",
            "title": "Rational Multi-Objective Agents Must Admit Non-Markov Reward Representations",
            "url": "https://www.semanticscholar.org/paper/0fc6d2ac846831b8a392f2a2436bf380c0147f4b",
            "venue": "",
            "year": 2022
        },
        {
            "arxivId": null,
            "authors": [],
            "doi": "10.5555/3535850",
            "intent": [],
            "isInfluential": false,
            "paperId": "deb146fe6e4ff871353c0775f8e066f14d441764",
            "title": "Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems",
            "url": "https://www.semanticscholar.org/paper/deb146fe6e4ff871353c0775f8e066f14d441764",
            "venue": "AAMAS",
            "year": 2022
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145234267",
                    "name": "Wang Chi Cheung"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "fab2b7d4c16160b91e097ae527668c590422a7fe",
            "title": "Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives",
            "url": "https://www.semanticscholar.org/paper/fab2b7d4c16160b91e097ae527668c590422a7fe",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2238176724",
                    "name": "R. S. Sutton"
                },
                {
                    "authorId": "1730590",
                    "name": "A. Barto"
                }
            ],
            "doi": "10.1109/TNN.1998.712192",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "title": "Reinforcement Learning: An Introduction",
            "url": "https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054",
            "venue": "IEEE Trans. Neural Networks",
            "year": 1998
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "title": "On the limitations of Markovian rewards to express multi-objective, risk-sensitive, and modal tasks",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/99d61aa0894413aac41892eb98d9a4c66eb72bf6",
    "venue": "Conference on Uncertainty in Artificial Intelligence",
    "year": 2024
}