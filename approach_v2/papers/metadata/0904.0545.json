{
    "abstract": "A technique called Time Hopping is proposed for speeding up reinforcement learning algorithms. It is applicable to continuous optimization problems running in computer simulations. Making shortcuts in time by hopping between distant states combined with off-policy reinforcement learning allows the technique to maintain higher learning rate. Experiments on a simulated biped crawling robot confirm that Time Hopping can accelerate the learning process more than seven times.",
    "arxivId": "0904.0545",
    "authors": [
        {
            "authorId": "1686032",
            "name": "Petar Kormushev",
            "url": "https://www.semanticscholar.org/author/1686032"
        },
        {
            "authorId": "2968867",
            "name": "K. Nomoto",
            "url": "https://www.semanticscholar.org/author/2968867"
        },
        {
            "authorId": "1762032",
            "name": "F. Dong",
            "url": "https://www.semanticscholar.org/author/1762032"
        },
        {
            "authorId": "48155991",
            "name": "K. Hirota",
            "url": "https://www.semanticscholar.org/author/48155991"
        }
    ],
    "citationVelocity": 0,
    "citations": [
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2563929",
                    "name": "Suat G\u00f6n\u00fcl"
                },
                {
                    "authorId": "2750082",
                    "name": "Tuncay Namli"
                },
                {
                    "authorId": "1707150",
                    "name": "A. Cosar"
                },
                {
                    "authorId": "2820623",
                    "name": "I. H. Toroslu"
                }
            ],
            "doi": "10.1016/J.ARTMED.2021.102062",
            "intent": [],
            "isInfluential": false,
            "paperId": "15a58d7ed4d5b3729fd7ac081ec32d652ec6981b",
            "title": "A reinforcement learning based algorithm for personalization of digital, just-in-time, adaptive interventions",
            "url": "https://www.semanticscholar.org/paper/15a58d7ed4d5b3729fd7ac081ec32d652ec6981b",
            "venue": "Artif. Intell. Medicine",
            "year": 2021
        },
        {
            "arxivId": "1811.11298",
            "authors": [
                {
                    "authorId": "80675449",
                    "name": "Arash Tavakoli"
                },
                {
                    "authorId": "6495683",
                    "name": "Vitaly Levdik"
                },
                {
                    "authorId": "18014232",
                    "name": "Riashat Islam"
                },
                {
                    "authorId": "1686032",
                    "name": "Petar Kormushev"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "ca537cf467c898dee473ff14ea18a47af05d0c9f",
            "title": "Prioritizing Starting States for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/ca537cf467c898dee473ff14ea18a47af05d0c9f",
            "venue": "ArXiv",
            "year": 2018
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "38968784",
                    "name": "Yunsick Sung"
                },
                {
                    "authorId": "50666390",
                    "name": "E. Ahn"
                },
                {
                    "authorId": "2818294",
                    "name": "Kyungeun Cho"
                }
            ],
            "doi": "10.1007/s11277-013-1235-4",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "8ead010e50ea32cfdaaeae9bec2d4c464f84bfda",
            "title": "Q-learning Reward Propagation Method for Reducing the Transmission Power of Sensor Nodes in Wireless Sensor Networks",
            "url": "https://www.semanticscholar.org/paper/8ead010e50ea32cfdaaeae9bec2d4c464f84bfda",
            "venue": "Wireless Personal Communications",
            "year": 2013
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "102910372",
                    "name": "Andr\u00e9s El-Fakdi Sencianes"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "92194faf984e62a86f9e013d7fd36b513a3a11c6",
            "title": "Gradient-based reinforcement learning techniques for underwater robotics behavior learning",
            "url": "https://www.semanticscholar.org/paper/92194faf984e62a86f9e013d7fd36b513a3a11c6",
            "venue": "",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1686032",
                    "name": "Petar Kormushev"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "35b39068497cfbd0136016ce12c29721b5b54243",
            "title": "Time Hopping Technique for Reinforcement Learning and its Application to Robot Control",
            "url": "https://www.semanticscholar.org/paper/35b39068497cfbd0136016ce12c29721b5b54243",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "38968784",
                    "name": "Yunsick Sung"
                },
                {
                    "authorId": "2818294",
                    "name": "Kyungeun Cho"
                },
                {
                    "authorId": "1687837",
                    "name": "Kyhyun Um"
                }
            ],
            "doi": "10.1145/1655925.1656047",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "3fe1c607e8b881f994c5101d911d2919ad11f480",
            "title": "A reward field model generation in Q-learning by dynamic programming",
            "url": "https://www.semanticscholar.org/paper/3fe1c607e8b881f994c5101d911d2919ad11f480",
            "venue": "ICIS",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1686032",
                    "name": "Petar Kormushev"
                },
                {
                    "authorId": "1762032",
                    "name": "F. Dong"
                },
                {
                    "authorId": "48155991",
                    "name": "K. Hirota"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "5a7e4938436fcbacd1b8c774d9ab233e151516c6",
            "title": "Probability Redistribution using Time Hopping for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/5a7e4938436fcbacd1b8c774d9ab233e151516c6",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": "0904.0546",
            "authors": [
                {
                    "authorId": "1686032",
                    "name": "Petar Kormushev"
                },
                {
                    "authorId": "2968867",
                    "name": "K. Nomoto"
                },
                {
                    "authorId": "1762032",
                    "name": "F. Dong"
                },
                {
                    "authorId": "48155991",
                    "name": "K. Hirota"
                }
            ],
            "doi": "10.20965/jaciii.2009.p0600",
            "intent": [],
            "isInfluential": false,
            "paperId": "2f4aa0eb816d5a1f88cf3f998f3f0f753ec5c329",
            "title": "Eligibility Propagation to Speed up Time Hopping for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/2f4aa0eb816d5a1f88cf3f998f3f0f753ec5c329",
            "venue": "J. Adv. Comput. Intell. Intell. Informatics",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "51054238",
                    "name": "Ioannis E. Polykretis"
                },
                {
                    "authorId": "2297496",
                    "name": "Mridul Aanjaneya"
                },
                {
                    "authorId": "2235492",
                    "name": "K. Michmizos"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "bc331ab44412b20b40b4f34ef96a8e8ae93f4c8d",
            "title": "Bioinspired Dynamic Control of Amphibious Articulated Creatures with Spiking Neural Networks",
            "url": "https://www.semanticscholar.org/paper/bc331ab44412b20b40b4f34ef96a8e8ae93f4c8d",
            "venue": "",
            "year": 2023
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145278148",
                    "name": "M. Carreras"
                },
                {
                    "authorId": "1404738231",
                    "name": "A. El-Fakdi"
                },
                {
                    "authorId": "1782279",
                    "name": "P. Ridao"
                }
            ],
            "doi": "10.1007/978-1-4614-5659-9_7",
            "intent": [],
            "isInfluential": false,
            "paperId": "ba61f007dd296368900e20247a29cb409fa619a6",
            "title": "Behavior Adaptation by Means of Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/ba61f007dd296368900e20247a29cb409fa619a6",
            "venue": "",
            "year": 2013
        }
    ],
    "corpusId": 17427740,
    "doi": null,
    "fieldsOfStudy": [
        "Computer Science"
    ],
    "influentialCitationCount": 0,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 10,
    "numCiting": 26,
    "paperId": "585f13d8a1c7dd9957f36ed644c6d312ce80adcf",
    "references": [
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1686032",
                    "name": "Petar Kormushev"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "35b39068497cfbd0136016ce12c29721b5b54243",
            "title": "Time Hopping Technique for Reinforcement Learning and its Application to Robot Control",
            "url": "https://www.semanticscholar.org/paper/35b39068497cfbd0136016ce12c29721b5b54243",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1686032",
                    "name": "Petar Kormushev"
                },
                {
                    "authorId": "1762032",
                    "name": "F. Dong"
                },
                {
                    "authorId": "48155991",
                    "name": "K. Hirota"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "5a7e4938436fcbacd1b8c774d9ab233e151516c6",
            "title": "Probability Redistribution using Time Hopping for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/5a7e4938436fcbacd1b8c774d9ab233e151516c6",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": "0904.0546",
            "authors": [
                {
                    "authorId": "1686032",
                    "name": "Petar Kormushev"
                },
                {
                    "authorId": "2968867",
                    "name": "K. Nomoto"
                },
                {
                    "authorId": "1762032",
                    "name": "F. Dong"
                },
                {
                    "authorId": "48155991",
                    "name": "K. Hirota"
                }
            ],
            "doi": "10.20965/jaciii.2009.p0600",
            "intent": [],
            "isInfluential": false,
            "paperId": "2f4aa0eb816d5a1f88cf3f998f3f0f753ec5c329",
            "title": "Eligibility Propagation to Speed up Time Hopping for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/2f4aa0eb816d5a1f88cf3f998f3f0f753ec5c329",
            "venue": "J. Adv. Comput. Intell. Intell. Informatics",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2536655",
                    "name": "A. Gosavi"
                }
            ],
            "doi": "10.1287/ijoc.1080.0305",
            "intent": [],
            "isInfluential": false,
            "paperId": "4a5f0c5c7d1b404cbb3a717e5581d86ff29025de",
            "title": "Reinforcement Learning: A Tutorial Survey and Recent Advances",
            "url": "https://www.semanticscholar.org/paper/4a5f0c5c7d1b404cbb3a717e5581d86ff29025de",
            "venue": "INFORMS J. Comput.",
            "year": 2009
        },
        {
            "arxivId": "0903.4930",
            "authors": [
                {
                    "authorId": "1686032",
                    "name": "Petar Kormushev"
                },
                {
                    "authorId": "2968867",
                    "name": "K. Nomoto"
                },
                {
                    "authorId": "1762032",
                    "name": "F. Dong"
                },
                {
                    "authorId": "48155991",
                    "name": "K. Hirota"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "c34f2d70e3722e1918a5343dddb889b9e3b26240",
            "title": "Time manipulation technique for speeding up reinforcement learning in simulations",
            "url": "https://www.semanticscholar.org/paper/c34f2d70e3722e1918a5343dddb889b9e3b26240",
            "venue": "ArXiv",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144638694",
                    "name": "Adam Coates"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "34699434",
                    "name": "A. Ng"
                }
            ],
            "doi": "10.1145/1390156.1390175",
            "intent": [],
            "isInfluential": false,
            "paperId": "f31592bf0aa8f8d24ea52576db878a3d557aa8e1",
            "title": "Learning for control from multiple demonstrations",
            "url": "https://www.semanticscholar.org/paper/f31592bf0aa8f8d24ea52576db878a3d557aa8e1",
            "venue": "ICML '08",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145116464",
                    "name": "J. Z. Kolter"
                },
                {
                    "authorId": "144079839",
                    "name": "M. Rodgers"
                },
                {
                    "authorId": "34699434",
                    "name": "A. Ng"
                }
            ],
            "doi": "10.1109/ROBOT.2008.4543305",
            "intent": [],
            "isInfluential": false,
            "paperId": "814eb43be709dd60e1478e7fc97c0c12ae10edf1",
            "title": "A control architecture for quadruped locomotion over rough terrain",
            "url": "https://www.semanticscholar.org/paper/814eb43be709dd60e1478e7fc97c0c12ae10edf1",
            "venue": "2008 IEEE International Conference on Robotics and Automation",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145116464",
                    "name": "J. Z. Kolter"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "34699434",
                    "name": "A. Ng"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "98eb84463c61b22b16d2f3300b106c0e5d99662b",
            "title": "Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion",
            "url": "https://www.semanticscholar.org/paper/98eb84463c61b22b16d2f3300b106c0e5d99662b",
            "venue": "NIPS",
            "year": 2007
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "144638694",
                    "name": "Adam Coates"
                },
                {
                    "authorId": "39100828",
                    "name": "M. Quigley"
                },
                {
                    "authorId": "34699434",
                    "name": "A. Ng"
                }
            ],
            "doi": "10.7551/mitpress/7503.003.0006",
            "intent": [],
            "isInfluential": false,
            "paperId": "0bfbdafdfbcc268860fe54ae4d8f08d487bcc762",
            "title": "An Application of Reinforcement Learning to Aerobatic Helicopter Flight",
            "url": "https://www.semanticscholar.org/paper/0bfbdafdfbcc268860fe54ae4d8f08d487bcc762",
            "venue": "NIPS",
            "year": 2006
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "34699434",
                    "name": "A. Ng"
                }
            ],
            "doi": "10.1007/11893318_3",
            "intent": [],
            "isInfluential": false,
            "paperId": "52e6437100b13a742271a3c8eed8dfe1f9b7ca14",
            "title": "Reinforcement Learning and Apprenticeship Learning for Robotic Control",
            "url": "https://www.semanticscholar.org/paper/52e6437100b13a742271a3c8eed8dfe1f9b7ca14",
            "venue": "Discovery Science",
            "year": 2006
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "34699434",
                    "name": "A. Ng"
                }
            ],
            "doi": "10.1145/1102351.1102352",
            "intent": [],
            "isInfluential": false,
            "paperId": "3b76c51beceb5057b1285bd7d709817cda17adc0",
            "title": "Exploration and apprenticeship learning in reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/3b76c51beceb5057b1285bd7d709817cda17adc0",
            "venue": "ICML",
            "year": 2005
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2686687",
                    "name": "Nate Kohl"
                },
                {
                    "authorId": "144848112",
                    "name": "P. Stone"
                }
            ],
            "doi": "10.1109/ROBOT.2004.1307456",
            "intent": [],
            "isInfluential": false,
            "paperId": "413b32dc8855f9db4c95657a3de5ca6c1d793da0",
            "title": "Policy gradient reinforcement learning for fast quadrupedal locomotion",
            "url": "https://www.semanticscholar.org/paper/413b32dc8855f9db4c95657a3de5ca6c1d793da0",
            "venue": "IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",
            "year": 2004
        },
        {
            "arxivId": "1106.0681",
            "authors": [
                {
                    "authorId": "145646162",
                    "name": "Craig Boutilier"
                },
                {
                    "authorId": "2064278710",
                    "name": "Bob Price"
                }
            ],
            "doi": "10.1613/jair.898",
            "intent": [],
            "isInfluential": false,
            "paperId": "3cce92bc77e86fcf6f31d8b0b9d57502eb92934a",
            "title": "Accelerating Reinforcement Learning through Implicit Imitation",
            "url": "https://www.semanticscholar.org/paper/3cce92bc77e86fcf6f31d8b0b9d57502eb92934a",
            "venue": "J. Artif. Intell. Res.",
            "year": 2003
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "81338045",
                    "name": "Michael Kearns"
                },
                {
                    "authorId": "1699868",
                    "name": "Satinder Singh"
                }
            ],
            "doi": "10.1023/A:1017984413808",
            "intent": [],
            "isInfluential": false,
            "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
            "title": "Near-Optimal Reinforcement Learning in Polynomial Time",
            "url": "https://www.semanticscholar.org/paper/dc649486b881e672eea6546da48c46e1f98daf32",
            "venue": "Machine Learning",
            "year": 2002
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144368601",
                    "name": "Doina Precup"
                },
                {
                    "authorId": "1699645",
                    "name": "R. Sutton"
                },
                {
                    "authorId": "1880237",
                    "name": "S. Dasgupta"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "e54152403da98a0403afef8477d42383d606e1f9",
            "title": "Off-Policy Temporal Difference Learning with Function Approximation",
            "url": "https://www.semanticscholar.org/paper/e54152403da98a0403afef8477d42383d606e1f9",
            "venue": "ICML",
            "year": 2001
        },
        {
            "arxivId": "cs/9905014",
            "authors": [
                {
                    "authorId": "144299726",
                    "name": "Thomas G. Dietterich"
                }
            ],
            "doi": "10.1613/jair.639",
            "intent": [],
            "isInfluential": false,
            "paperId": "4c96ca25d889251e20e33d01f24eec175301ab94",
            "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition",
            "url": "https://www.semanticscholar.org/paper/4c96ca25d889251e20e33d01f24eec175301ab94",
            "venue": "J. Artif. Intell. Res.",
            "year": 1999
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1707376",
                    "name": "S. Geva"
                },
                {
                    "authorId": "1731763",
                    "name": "J. Sitte"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "8acb19ab20036a29c13adf3672a7ac691781efe4",
            "title": "The Cart-Pole Experiment as a Benchmark for Trainable Controllers",
            "url": "https://www.semanticscholar.org/paper/8acb19ab20036a29c13adf3672a7ac691781efe4",
            "venue": "",
            "year": 1992
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1699645",
                    "name": "R. Sutton"
                }
            ],
            "doi": "10.1023/A:1022633531479",
            "intent": [],
            "isInfluential": false,
            "paperId": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
            "title": "Learning to predict by the methods of temporal differences",
            "url": "https://www.semanticscholar.org/paper/a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
            "venue": "Machine Learning",
            "year": 1988
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1730590",
                    "name": "A. Barto"
                },
                {
                    "authorId": "1850503",
                    "name": "S. Mahadevan"
                }
            ],
            "doi": "10.1023/A:1022140919877",
            "intent": [],
            "isInfluential": false,
            "paperId": "0a8149fb5aa8a5684e7d530c264451a5cb9250f5",
            "title": "Recent Advances in Hierarchical Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/0a8149fb5aa8a5684e7d530c264451a5cb9250f5",
            "venue": "Discret. Event Dyn. Syst.",
            "year": 2003
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144867807",
                    "name": "S. Thrun"
                },
                {
                    "authorId": "2149607686",
                    "name": "Anton Schwartz"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "26b8747eb4d7fb4d4fc45707606d5e969b9afb0c",
            "title": "Issues in Using Function Approximation for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/26b8747eb4d7fb4d4fc45707606d5e969b9afb0c",
            "venue": "",
            "year": 1999
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145231138",
                    "name": "M. Humphreys"
                }
            ],
            "doi": "10.7551/mitpress/3118.003.0018",
            "intent": [],
            "isInfluential": false,
            "paperId": "3adb0b3f0fd15096b578487143bb4db90c6ce563",
            "title": "Action selection methods using reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/3adb0b3f0fd15096b578487143bb4db90c6ce563",
            "venue": "",
            "year": 1997
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144867807",
                    "name": "S. Thrun"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "dcbef60688ca014a63e72e844fb997e738f6fde7",
            "title": "Efficient Exploration In Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/dcbef60688ca014a63e72e844fb997e738f6fde7",
            "venue": "",
            "year": 1992
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "title": "Time Hopping technique for faster reinforcement learning in simulations",
    "topics": [
        {
            "topic": "Reinforcement learning",
            "topicId": "2557",
            "url": "https://www.semanticscholar.org/topic/2557"
        },
        {
            "topic": "Frequency-hopping spread spectrum",
            "topicId": "8880",
            "url": "https://www.semanticscholar.org/topic/8880"
        },
        {
            "topic": "Continuous optimization",
            "topicId": "3581",
            "url": "https://www.semanticscholar.org/topic/3581"
        },
        {
            "topic": "Computer simulation",
            "topicId": "7425",
            "url": "https://www.semanticscholar.org/topic/7425"
        },
        {
            "topic": "Machine learning",
            "topicId": "168",
            "url": "https://www.semanticscholar.org/topic/168"
        },
        {
            "topic": "Mathematical optimization",
            "topicId": "89",
            "url": "https://www.semanticscholar.org/topic/89"
        },
        {
            "topic": "Algorithm",
            "topicId": "305",
            "url": "https://www.semanticscholar.org/topic/305"
        }
    ],
    "url": "https://www.semanticscholar.org/paper/585f13d8a1c7dd9957f36ed644c6d312ce80adcf",
    "venue": "arXiv.org",
    "year": 2009
}