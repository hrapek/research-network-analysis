{
    "abstract": "Maximum entropy deep reinforcement learning (RL) methods have been demonstrated on a range of challenging continuous tasks. However, existing methods either suffer from severe instability when training on large off-policy data or cannot scale to tasks with very high state and action dimensionality such as 3D humanoid locomotion. Besides, the optimality of desired Boltzmann policy set for non-optimal soft value function is not persuasive enough. In this paper, we first derive soft policy gradient based on entropy regularized expected reward objective for RL with continuous actions. Then, we present an off-policy actor-critic, model-free maximum entropy deep RL algorithm called deep soft policy gradient (DSPG) by combining soft policy gradient with soft Bellman equation. To ensure stable learning while eliminating the need of two separate critics for soft value functions, we leverage double sampling approach to making the soft Bellman equation tractable. The experimental results demonstrate that our method outperforms in performance over off-policy prior methods.",
    "arxivId": "1909.03198",
    "authors": [
        {
            "authorId": "145033252",
            "name": "Wenjie Shi",
            "url": "https://www.semanticscholar.org/author/145033252"
        },
        {
            "authorId": "1760750",
            "name": "Shiji Song",
            "url": "https://www.semanticscholar.org/author/1760750"
        },
        {
            "authorId": "145253556",
            "name": "Cheng Wu",
            "url": "https://www.semanticscholar.org/author/145253556"
        }
    ],
    "citationVelocity": 7,
    "citations": [
        {
            "arxivId": "2409.16862",
            "authors": [
                {
                    "authorId": "2153605373",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2553553",
                    "name": "Wenchuan Jia"
                },
                {
                    "authorId": "2268387739",
                    "name": "Yi Sun"
                },
                {
                    "authorId": "2323435789",
                    "name": "Dong He"
                }
            ],
            "doi": "10.48550/arXiv.2409.16862",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "76de1f9bf46367b5c82fc5927230aaedc93ffebf",
            "title": "Behavior evolution-inspired approach to walking gait reinforcement training for quadruped robots",
            "url": "https://www.semanticscholar.org/paper/76de1f9bf46367b5c82fc5927230aaedc93ffebf",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2407.20917",
            "authors": [
                {
                    "authorId": "2313730351",
                    "name": "Fabian Bongratz"
                },
                {
                    "authorId": "2943639",
                    "name": "Vladimir Golkov"
                },
                {
                    "authorId": "2313726929",
                    "name": "Lukas Mautner"
                },
                {
                    "authorId": "122566053",
                    "name": "Luca Della Libera"
                },
                {
                    "authorId": "2313730346",
                    "name": "Frederik Heetmeyer"
                },
                {
                    "authorId": "2313727756",
                    "name": "Felix Czaja"
                },
                {
                    "authorId": "2313728104",
                    "name": "Julian Rodemann"
                },
                {
                    "authorId": "2249123096",
                    "name": "Daniel Cremers"
                }
            ],
            "doi": "10.48550/arXiv.2407.20917",
            "intent": [],
            "isInfluential": false,
            "paperId": "3af32fccb0d3f3647272ba8513e9d3bf6a3b6384",
            "title": "How to Choose a Reinforcement-Learning Algorithm",
            "url": "https://www.semanticscholar.org/paper/3af32fccb0d3f3647272ba8513e9d3bf6a3b6384",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2407.18143",
            "authors": [
                {
                    "authorId": "1380705734",
                    "name": "Jean Seong Bjorn Choe"
                },
                {
                    "authorId": "2155892054",
                    "name": "Jong-Kook Kim"
                }
            ],
            "doi": "10.48550/arXiv.2407.18143",
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "d92a1dc339a928e0c0cde1d804e65dd85d1b3d4b",
            "title": "Maximum Entropy On-Policy Actor-Critic via Entropy Advantage Estimation",
            "url": "https://www.semanticscholar.org/paper/d92a1dc339a928e0c0cde1d804e65dd85d1b3d4b",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2405.13629",
            "authors": [
                {
                    "authorId": "2086829087",
                    "name": "Chen-Hao Chao"
                },
                {
                    "authorId": "2282474565",
                    "name": "Chien Feng"
                },
                {
                    "authorId": "2282635296",
                    "name": "Wei-Fang Sun"
                },
                {
                    "authorId": "2127191094",
                    "name": "Cheng-Kuang Lee"
                },
                {
                    "authorId": "2302795159",
                    "name": "Simon See"
                },
                {
                    "authorId": "2149791622",
                    "name": "Chun-Yi Lee"
                }
            ],
            "doi": "10.48550/arXiv.2405.13629",
            "intent": [],
            "isInfluential": false,
            "paperId": "865acaf2f40a14819ab44a6fb98b0937cc475555",
            "title": "Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow",
            "url": "https://www.semanticscholar.org/paper/865acaf2f40a14819ab44a6fb98b0937cc475555",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2405.11457",
            "authors": [
                {
                    "authorId": "104254725",
                    "name": "Yusheng Jiao"
                },
                {
                    "authorId": "2156929",
                    "name": "Feng Ling"
                },
                {
                    "authorId": "104424292",
                    "name": "Sina Heydari"
                },
                {
                    "authorId": "2801204",
                    "name": "N. Heess"
                },
                {
                    "authorId": "1879232",
                    "name": "J. Merel"
                },
                {
                    "authorId": "2294722114",
                    "name": "Eva Kanso"
                }
            ],
            "doi": "10.48550/arXiv.2405.11457",
            "intent": [],
            "isInfluential": false,
            "paperId": "1306f78b6bb3bdc23cd4cf0982ba787d48f0adcc",
            "title": "Deep Dive into Model-free Reinforcement Learning for Biological and Robotic Systems: Theory and Practice",
            "url": "https://www.semanticscholar.org/paper/1306f78b6bb3bdc23cd4cf0982ba787d48f0adcc",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1755581",
                    "name": "Feng Tao"
                },
                {
                    "authorId": "123804962",
                    "name": "Mingkang Wu"
                },
                {
                    "authorId": "33403228",
                    "name": "Yongcan Cao"
                }
            ],
            "doi": "10.1109/TAI.2023.3297988",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "034381afe227ca02ac4b7120c8927777792e68da",
            "title": "Generalized Maximum Entropy Reinforcement Learning via Reward Shaping",
            "url": "https://www.semanticscholar.org/paper/034381afe227ca02ac4b7120c8927777792e68da",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2024
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2190041644",
                    "name": "Yuntao Xue"
                },
                {
                    "authorId": "2267691289",
                    "name": "Weisheng Chen"
                }
            ],
            "doi": "10.1109/TVT.2023.3330703",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "2a251414e6937eae9429f7d349ce6c1542632669",
            "title": "RLoPlanner: Combining Learning and Motion Planner for UAV Safe Navigation in Cluttered Unknown Environments",
            "url": "https://www.semanticscholar.org/paper/2a251414e6937eae9429f7d349ce6c1542632669",
            "venue": "IEEE Transactions on Vehicular Technology",
            "year": 2024
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2179487098",
                    "name": "Caibo Dong"
                },
                {
                    "authorId": "2287812803",
                    "name": "Dazi Li"
                }
            ],
            "doi": "10.1007/s11063-024-11548-6",
            "intent": [],
            "isInfluential": false,
            "paperId": "c4cda9f9ec1e441fa76b9028e5ce7cc4a82a48f0",
            "title": "Adaptive Evolutionary Reinforcement Learning with Policy Direction",
            "url": "https://www.semanticscholar.org/paper/c4cda9f9ec1e441fa76b9028e5ce7cc4a82a48f0",
            "venue": "Neural Process. Lett.",
            "year": 2024
        },
        {
            "arxivId": "2311.01388",
            "authors": [
                {
                    "authorId": "123723354",
                    "name": "Daniel Jarrett"
                },
                {
                    "authorId": "39965049",
                    "name": "Ioana Bica"
                },
                {
                    "authorId": "1729969",
                    "name": "M. Schaar"
                }
            ],
            "doi": "10.48550/arXiv.2311.01388",
            "intent": [],
            "isInfluential": false,
            "paperId": "f90cf10944142040b6a0e49dd79f24274ceec59a",
            "title": "Time-series Generation by Contrastive Imitation",
            "url": "https://www.semanticscholar.org/paper/f90cf10944142040b6a0e49dd79f24274ceec59a",
            "venue": "NeurIPS",
            "year": 2023
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2202130576",
                    "name": "Gurpreet Singh Panesar"
                },
                {
                    "authorId": "2277665786",
                    "name": "Raman Chadha"
                }
            ],
            "doi": "10.1109/ICTACS59847.2023.10390413",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "19eb6e5b743e29628edd91dbad186f16f8ea515e",
            "title": "DDPG: Cloud Data Centre Hybrid Optimisation for Virtual Machine Migration and Job Scheduling",
            "url": "https://www.semanticscholar.org/paper/19eb6e5b743e29628edd91dbad186f16f8ea515e",
            "venue": "2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS)",
            "year": 2023
        },
        {
            "arxivId": "2310.18591",
            "authors": [
                {
                    "authorId": "123723354",
                    "name": "Daniel Jarrett"
                },
                {
                    "authorId": "83246796",
                    "name": "Alihan H\u00fcy\u00fck"
                },
                {
                    "authorId": "1729969",
                    "name": "M. Schaar"
                }
            ],
            "doi": "10.48550/arXiv.2310.18591",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "4d7032df86929584bb7a82e7bdb3a138f12f0719",
            "title": "Inverse Decision Modeling: Learning Interpretable Representations of Behavior",
            "url": "https://www.semanticscholar.org/paper/4d7032df86929584bb7a82e7bdb3a138f12f0719",
            "venue": "ICML",
            "year": 2023
        },
        {
            "arxivId": "2310.17173",
            "authors": [
                {
                    "authorId": "2051891068",
                    "name": "Dexter Neo"
                },
                {
                    "authorId": "2261794988",
                    "name": "Tsuhan Chen"
                }
            ],
            "doi": "10.48550/arXiv.2310.17173",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "64e0ac4e7a912143db797aa29bc3c9df9299f9fc",
            "title": "DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic",
            "url": "https://www.semanticscholar.org/paper/64e0ac4e7a912143db797aa29bc3c9df9299f9fc",
            "venue": "ArXiv",
            "year": 2023
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2189221688",
                    "name": "Fan Li"
                },
                {
                    "authorId": "51066540",
                    "name": "Mingsheng Fu"
                },
                {
                    "authorId": "50504907",
                    "name": "Wenyu Chen"
                },
                {
                    "authorId": "2153306356",
                    "name": "Fan Zhang"
                },
                {
                    "authorId": "2235162",
                    "name": "Haixian Zhang"
                },
                {
                    "authorId": "2064924431",
                    "name": "Hong Qu"
                },
                {
                    "authorId": "2146683466",
                    "name": "Zhang Yi"
                }
            ],
            "doi": "10.1109/TNNLS.2022.3215596",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "ee5b4967b8e8ba65fb40e74d596cb8d9627e301e",
            "title": "Improving Exploration in Actor\u2013Critic With Weakly Pessimistic Value Estimation and Optimistic Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/ee5b4967b8e8ba65fb40e74d596cb8d9627e301e",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2022
        },
        {
            "arxivId": "2208.04517",
            "authors": [
                {
                    "authorId": "2149170197",
                    "name": "Xin Jin"
                },
                {
                    "authorId": "2175279136",
                    "name": "Shu Zhao"
                },
                {
                    "authorId": "2118470047",
                    "name": "Le Zhang"
                },
                {
                    "authorId": "48551093",
                    "name": "Xin Zhao"
                },
                {
                    "authorId": "2056197580",
                    "name": "Qiang Deng"
                },
                {
                    "authorId": "1942317",
                    "name": "Chaoen Xiao"
                }
            ],
            "doi": "10.1145/3503161.3547737",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "1d37b290f5f7f2756c760874df81eee0992cd2b7",
            "title": "Attribute Controllable Beautiful Caucasian Face Generation by Aesthetics Driven Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/1d37b290f5f7f2756c760874df81eee0992cd2b7",
            "venue": "ACM Multimedia",
            "year": 2022
        },
        {
            "arxivId": "2203.09809",
            "authors": [
                {
                    "authorId": "5216888",
                    "name": "Taisuke Kobayashi"
                }
            ],
            "doi": "10.1016/j.rico.2022.100192",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "1043ea4b7e186c632b29e40f648db2b03df28cc9",
            "title": "Proximal Policy Optimization with Adaptive Threshold for Symmetric Relative Density Ratio",
            "url": "https://www.semanticscholar.org/paper/1043ea4b7e186c632b29e40f648db2b03df28cc9",
            "venue": "Results in Control and Optimization",
            "year": 2022
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "33718755",
                    "name": "Yuhu Cheng"
                },
                {
                    "authorId": "2111142270",
                    "name": "Longyang Huang"
                },
                {
                    "authorId": "2152857279",
                    "name": "C. L. P. Chen"
                },
                {
                    "authorId": "153316233",
                    "name": "Xuesong Wang"
                }
            ],
            "doi": "10.1109/TNNLS.2022.3155483",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "cefa05e24e4e10597be91bb163b73a6b185e7742",
            "title": "Robust Actor-Critic With Relative Entropy Regulating Actor",
            "url": "https://www.semanticscholar.org/paper/cefa05e24e4e10597be91bb163b73a6b185e7742",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2022
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2061282572",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "1760750",
                    "name": "Shiji Song"
                },
                {
                    "authorId": "2115218570",
                    "name": "Gao Huang"
                }
            ],
            "doi": "10.1109/TNNLS.2021.3121432",
            "intent": [],
            "isInfluential": false,
            "paperId": "84a32b3a9cfb78969c21afd69df9788142a27031",
            "title": "Exploration With Task Information for Meta Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/84a32b3a9cfb78969c21afd69df9788142a27031",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2021
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "34970419",
                    "name": "E. Morales"
                },
                {
                    "authorId": "1399074465",
                    "name": "Rafael Murrieta-Cid"
                },
                {
                    "authorId": "145830385",
                    "name": "Israel Becerra"
                },
                {
                    "authorId": "2141098817",
                    "name": "Marco A. Esquivel-Basaldua"
                }
            ],
            "doi": "10.1007/s11370-021-00398-z",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "7d541b023e6feb8aee059744b87e78c1d23cd160",
            "title": "A survey on deep learning and deep reinforcement learning in robotics with a tutorial on deep reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/7d541b023e6feb8aee059744b87e78c1d23cd160",
            "venue": "Intelligent Service Robotics",
            "year": 2021
        },
        {
            "arxivId": "2108.10533",
            "authors": [
                {
                    "authorId": "11839706",
                    "name": "Sooyoung Jang"
                },
                {
                    "authorId": "2117990266",
                    "name": "Hyungil Kim"
                }
            ],
            "doi": "10.3390/s22155845",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "39f4a08e11709ebec657a671a58ca36ba7e8e86f",
            "title": "Entropy-Aware Model Initialization for Effective Exploration in Deep Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/39f4a08e11709ebec657a671a58ca36ba7e8e86f",
            "venue": "Sensors",
            "year": 2021
        },
        {
            "arxivId": "2105.12991",
            "authors": [
                {
                    "authorId": "5216888",
                    "name": "Taisuke Kobayashi"
                }
            ],
            "doi": "10.1016/j.neunet.2022.04.021",
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "a12f2b73f097e78421166ec4bf4e16d4629673f1",
            "title": "Optimistic Reinforcement Learning by Forward Kullback-Leibler Divergence Optimization",
            "url": "https://www.semanticscholar.org/paper/a12f2b73f097e78421166ec4bf4e16d4629673f1",
            "venue": "Neural Networks",
            "year": 2021
        },
        {
            "arxivId": "2102.04881",
            "authors": [
                {
                    "authorId": "2064453117",
                    "name": "Florian E. Dorner"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "ec05bd6725ac6a5217021881cac8553581b3e313",
            "title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency",
            "url": "https://www.semanticscholar.org/paper/ec05bd6725ac6a5217021881cac8553581b3e313",
            "venue": "ArXiv",
            "year": 2021
        },
        {
            "arxivId": "2101.11331",
            "authors": [
                {
                    "authorId": "2053179501",
                    "name": "Philip J. Ball"
                },
                {
                    "authorId": "145029236",
                    "name": "S. Roberts"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "639b555c90d00c920de9119bd504e0ffd14cab6b",
            "title": "OffCon3: What is state of the art anyway?",
            "url": "https://www.semanticscholar.org/paper/639b555c90d00c920de9119bd504e0ffd14cab6b",
            "venue": "ArXiv",
            "year": 2021
        },
        {
            "arxivId": "2104.03807",
            "authors": [
                {
                    "authorId": "2925544",
                    "name": "Zahra Gharaee"
                },
                {
                    "authorId": "65994620",
                    "name": "Karl Holmquist"
                },
                {
                    "authorId": "2112501141",
                    "name": "Linbo He"
                },
                {
                    "authorId": "2228323",
                    "name": "M. Felsberg"
                }
            ],
            "doi": "10.1109/ICPR48806.2021.9412200",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "be0b985ed8df668f4a7177d8aedd348c8d62b686",
            "title": "A Bayesian Approach to Reinforcement Learning of Vision-Based Vehicular Control",
            "url": "https://www.semanticscholar.org/paper/be0b985ed8df668f4a7177d8aedd348c8d62b686",
            "venue": "2020 25th International Conference on Pattern Recognition (ICPR)",
            "year": 2021
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145033252",
                    "name": "Wenjie Shi"
                },
                {
                    "authorId": "143983679",
                    "name": "Gao Huang"
                },
                {
                    "authorId": "1760750",
                    "name": "Shiji Song"
                },
                {
                    "authorId": "2108369355",
                    "name": "Zhuoyuan Wang"
                },
                {
                    "authorId": "2115348865",
                    "name": "Tingyu Lin"
                },
                {
                    "authorId": "145253556",
                    "name": "Cheng Wu"
                }
            ],
            "doi": "10.1109/TPAMI.2020.3037898",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "856db6be9523250b07ce3e44369515f6435dac2c",
            "title": "Self-Supervised Discovering of Interpretable Features for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/856db6be9523250b07ce3e44369515f6435dac2c",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2020
        },
        {
            "arxivId": "2011.04118",
            "authors": [
                {
                    "authorId": "1403851982",
                    "name": "Pamela Carreno-Medrano"
                },
                {
                    "authorId": "2157788328",
                    "name": "Stephen L. Smith"
                },
                {
                    "authorId": "1768765",
                    "name": "D. Kuli\u0107"
                }
            ],
            "doi": "10.1109/TRO.2022.3192969",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "a6705f95c7fb9a2ccff7f9af70d5f1f96bf65ec8",
            "title": "Joint Estimation of Expertise and Reward Preferences From Human Demonstrations",
            "url": "https://www.semanticscholar.org/paper/a6705f95c7fb9a2ccff7f9af70d5f1f96bf65ec8",
            "venue": "IEEE Transactions on Robotics",
            "year": 2020
        },
        {
            "arxivId": "2011.00583",
            "authors": [
                {
                    "authorId": "47796324",
                    "name": "Yaodong Yang"
                },
                {
                    "authorId": "48094081",
                    "name": "Jun Wang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "c3662e9176a7ad90020bdd025c179c5925d0b5b0",
            "title": "An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective",
            "url": "https://www.semanticscholar.org/paper/c3662e9176a7ad90020bdd025c179c5925d0b5b0",
            "venue": "ArXiv",
            "year": 2020
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1811414728",
                    "name": "Haoqiang Chen"
                },
                {
                    "authorId": "2108077021",
                    "name": "Yadong Liu"
                },
                {
                    "authorId": "8526311",
                    "name": "Zongtan Zhou"
                },
                {
                    "authorId": "46570618",
                    "name": "D. Hu"
                },
                {
                    "authorId": "2118541438",
                    "name": "Ming Zhang"
                }
            ],
            "doi": "10.1007/s10489-020-01755-8",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "a52dea8e998c8f1ba4efa72490ac5f6af374c11b",
            "title": "GAMA: Graph Attention Multi-agent reinforcement learning algorithm for cooperation",
            "url": "https://www.semanticscholar.org/paper/a52dea8e998c8f1ba4efa72490ac5f6af374c11b",
            "venue": "Applied Intelligence",
            "year": 2020
        },
        {
            "arxivId": "2006.09646",
            "authors": [
                {
                    "authorId": "2192265736",
                    "name": "Amber Srivastava"
                },
                {
                    "authorId": "1770735",
                    "name": "S. Salapaka"
                }
            ],
            "doi": "10.1109/TCYB.2021.3102510",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "3e97a3571b194db69a186bb1eb1d65b591463fb4",
            "title": "Parameterized MDPs and Reinforcement Learning Problems\u2014A Maximum Entropy Principle-Based Framework",
            "url": "https://www.semanticscholar.org/paper/3e97a3571b194db69a186bb1eb1d65b591463fb4",
            "venue": "IEEE Transactions on Cybernetics",
            "year": 2020
        },
        {
            "arxivId": "2005.08844",
            "authors": [
                {
                    "authorId": "2115475688",
                    "name": "Donghoon Lee"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "450694857fc644e674dc05d3f3b207b3ff590ef7",
            "title": "Entropy-Augmented Entropy-Regularized Reinforcement Learning and a Continuous Path from Policy Gradient to Q-Learning",
            "url": "https://www.semanticscholar.org/paper/450694857fc644e674dc05d3f3b207b3ff590ef7",
            "venue": "ArXiv",
            "year": 2020
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "46365371",
                    "name": "Junta Wu"
                },
                {
                    "authorId": "47892761",
                    "name": "Huiyun Li"
                }
            ],
            "doi": "10.1155/2020/4275623",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "dcc02065f3f51a6bc4117adc431801e3be8a2362",
            "title": "Deep Ensemble Reinforcement Learning with Multiple Deep Deterministic Policy Gradient Algorithm",
            "url": "https://www.semanticscholar.org/paper/dcc02065f3f51a6bc4117adc431801e3be8a2362",
            "venue": "Mathematical Problems in Engineering",
            "year": 2020
        },
        {
            "arxivId": "1909.03245",
            "authors": [
                {
                    "authorId": "145033252",
                    "name": "Wenjie Shi"
                },
                {
                    "authorId": "1760750",
                    "name": "Shiji Song"
                },
                {
                    "authorId": "144698285",
                    "name": "Hui Wu"
                },
                {
                    "authorId": "1410879684",
                    "name": "Yachu Hsu"
                },
                {
                    "authorId": "145253556",
                    "name": "Cheng Wu"
                },
                {
                    "authorId": "143983679",
                    "name": "Gao Huang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "aab0d785275579038d0944c36c42d6ab8098faa4",
            "title": "Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/aab0d785275579038d0944c36c42d6ab8098faa4",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2282346214",
                    "name": "Rahul Sharma"
                },
                {
                    "authorId": "2302594885",
                    "name": "Shakti Raj Chopra"
                },
                {
                    "authorId": "2282092480",
                    "name": "Akhil Gupta"
                },
                {
                    "authorId": "26215350",
                    "name": "Rupendeep Kaur"
                },
                {
                    "authorId": "49252696",
                    "name": "Sudeep Tanwar"
                },
                {
                    "authorId": "2186910377",
                    "name": "Giovanni Pau"
                },
                {
                    "authorId": "2150072011",
                    "name": "Gulshan Sharma"
                },
                {
                    "authorId": "2165709207",
                    "name": "Fayez Alqahtani"
                },
                {
                    "authorId": "145552317",
                    "name": "Amr M. Tolba"
                }
            ],
            "doi": "10.1109/ACCESS.2024.3401016",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "71b7b90cf03f9fdd790c2994dd147ce6f8f214c5",
            "title": "Deployment of Unmanned Aerial Vehicles in Next-Generation Wireless Communication Network Using Multi-Agent Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/71b7b90cf03f9fdd790c2994dd147ce6f8f214c5",
            "venue": "IEEE Access",
            "year": 2024
        },
        {
            "arxivId": null,
            "authors": [],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "dda2205b4880377682122abbab4ae2127e2b3c44",
            "title": "R EINFORCEMENT L EARNING FOR C ONTROL WITH P ROBABILISTIC S TABILITY G UARANTEE",
            "url": "https://www.semanticscholar.org/paper/dda2205b4880377682122abbab4ae2127e2b3c44",
            "venue": "",
            "year": 2020
        }
    ],
    "corpusId": 199466294,
    "doi": "10.24963/ijcai.2019/475",
    "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
    ],
    "influentialCitationCount": 3,
    "isOpenAccess": true,
    "isPublisherLicensed": true,
    "is_open_access": true,
    "is_publisher_licensed": true,
    "numCitedBy": 33,
    "numCiting": 35,
    "paperId": "30a136e860f105604eba58dc880b9253c5f3202c",
    "references": [
        {
            "arxivId": "1801.01290",
            "authors": [
                {
                    "authorId": "2587648",
                    "name": "Tuomas Haarnoja"
                },
                {
                    "authorId": "35499972",
                    "name": "Aurick Zhou"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "811df72e210e20de99719539505da54762a11c6d",
            "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
            "url": "https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d",
            "venue": "ICML",
            "year": 2018
        },
        {
            "arxivId": "1709.06560",
            "authors": [
                {
                    "authorId": "40068904",
                    "name": "Peter Henderson"
                },
                {
                    "authorId": "18014232",
                    "name": "Riashat Islam"
                },
                {
                    "authorId": "143902541",
                    "name": "Philip Bachman"
                },
                {
                    "authorId": "145134886",
                    "name": "Joelle Pineau"
                },
                {
                    "authorId": "144368601",
                    "name": "Doina Precup"
                },
                {
                    "authorId": "2462512",
                    "name": "D. Meger"
                }
            ],
            "doi": "10.1609/aaai.v32i1.11694",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6",
            "title": "Deep Reinforcement Learning that Matters",
            "url": "https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6",
            "venue": "AAAI",
            "year": 2017
        },
        {
            "arxivId": "1707.01891",
            "authors": [
                {
                    "authorId": "7624658",
                    "name": "Ofir Nachum"
                },
                {
                    "authorId": "144739074",
                    "name": "Mohammad Norouzi"
                },
                {
                    "authorId": "36303818",
                    "name": "Kelvin Xu"
                },
                {
                    "authorId": "1714772",
                    "name": "Dale Schuurmans"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "bc8eb0c66f8977d3b23dedb247607a8af2360859",
            "title": "Trust-PCL: An Off-Policy Trust Region Method for Continuous Control",
            "url": "https://www.semanticscholar.org/paper/bc8eb0c66f8977d3b23dedb247607a8af2360859",
            "venue": "ICLR",
            "year": 2017
        },
        {
            "arxivId": "1704.06440",
            "authors": [
                {
                    "authorId": "47971768",
                    "name": "John Schulman"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "41192764",
                    "name": "Xi Chen"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "d0352057e2b99f65f8b5244a0b912026c86d7b21",
            "title": "Equivalence Between Policy Gradients and Soft Q-Learning",
            "url": "https://www.semanticscholar.org/paper/d0352057e2b99f65f8b5244a0b912026c86d7b21",
            "venue": "ArXiv",
            "year": 2017
        },
        {
            "arxivId": "1702.08892",
            "authors": [
                {
                    "authorId": "7624658",
                    "name": "Ofir Nachum"
                },
                {
                    "authorId": "144739074",
                    "name": "Mohammad Norouzi"
                },
                {
                    "authorId": "36303818",
                    "name": "Kelvin Xu"
                },
                {
                    "authorId": "1714772",
                    "name": "Dale Schuurmans"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "96a067e188f1c89db9faea1fea2314a15ae51bbc",
            "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/96a067e188f1c89db9faea1fea2314a15ae51bbc",
            "venue": "NIPS",
            "year": 2017
        },
        {
            "arxivId": "1702.08165",
            "authors": [
                {
                    "authorId": "2587648",
                    "name": "Tuomas Haarnoja"
                },
                {
                    "authorId": "4990833",
                    "name": "Haoran Tang"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
            "title": "Reinforcement Learning with Deep Energy-Based Policies",
            "url": "https://www.semanticscholar.org/paper/9172cd6c253edf7c3a1568e03577db20648ad0c4",
            "venue": "ICML",
            "year": 2017
        },
        {
            "arxivId": "1611.01626",
            "authors": [
                {
                    "authorId": "1389654226",
                    "name": "Brendan O'Donoghue"
                },
                {
                    "authorId": "1708654",
                    "name": "R. Munos"
                },
                {
                    "authorId": "2645384",
                    "name": "K. Kavukcuoglu"
                },
                {
                    "authorId": "3255983",
                    "name": "Volodymyr Mnih"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "c40dd8f235aabe6efbb93c59c0536adf491f9ead",
            "title": "PGQ: Combining policy gradient and Q-learning",
            "url": "https://www.semanticscholar.org/paper/c40dd8f235aabe6efbb93c59c0536adf491f9ead",
            "venue": "ICLR",
            "year": 2016
        },
        {
            "arxivId": "1611.02247",
            "authors": [
                {
                    "authorId": "2046135",
                    "name": "S. Gu"
                },
                {
                    "authorId": "2542999",
                    "name": "T. Lillicrap"
                },
                {
                    "authorId": "1744700",
                    "name": "Zoubin Ghahramani"
                },
                {
                    "authorId": "145369890",
                    "name": "Richard E. Turner"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": "10.17863/CAM.21294",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "524513b6f4ddca331c33bcc70a9f677fa240cfa3",
            "title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic",
            "url": "https://www.semanticscholar.org/paper/524513b6f4ddca331c33bcc70a9f677fa240cfa3",
            "venue": "ICLR",
            "year": 2016
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "1885349",
                    "name": "Aja Huang"
                },
                {
                    "authorId": "2772217",
                    "name": "Chris J. Maddison"
                },
                {
                    "authorId": "35099444",
                    "name": "A. Guez"
                },
                {
                    "authorId": "2175946",
                    "name": "L. Sifre"
                },
                {
                    "authorId": "47568983",
                    "name": "George van den Driessche"
                },
                {
                    "authorId": "4337102",
                    "name": "Julian Schrittwieser"
                },
                {
                    "authorId": "2460849",
                    "name": "Ioannis Antonoglou"
                },
                {
                    "authorId": "2749418",
                    "name": "Vedavyas Panneershelvam"
                },
                {
                    "authorId": "1975889",
                    "name": "Marc Lanctot"
                },
                {
                    "authorId": "48373216",
                    "name": "S. Dieleman"
                },
                {
                    "authorId": "2401609",
                    "name": "Dominik Grewe"
                },
                {
                    "authorId": "4111313",
                    "name": "John Nham"
                },
                {
                    "authorId": "2583391",
                    "name": "Nal Kalchbrenner"
                },
                {
                    "authorId": "1701686",
                    "name": "I. Sutskever"
                },
                {
                    "authorId": "2542999",
                    "name": "T. Lillicrap"
                },
                {
                    "authorId": "40662181",
                    "name": "M. Leach"
                },
                {
                    "authorId": "2645384",
                    "name": "K. Kavukcuoglu"
                },
                {
                    "authorId": "1686971",
                    "name": "T. Graepel"
                },
                {
                    "authorId": "48987704",
                    "name": "D. Hassabis"
                }
            ],
            "doi": "10.1038/nature16961",
            "intent": [],
            "isInfluential": false,
            "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
            "title": "Mastering the game of Go with deep neural networks and tree search",
            "url": "https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490",
            "venue": "Nature",
            "year": 2016
        },
        {
            "arxivId": "1512.08562",
            "authors": [
                {
                    "authorId": "145609073",
                    "name": "Roy Fox"
                },
                {
                    "authorId": "3314041",
                    "name": "Ari Pakman"
                },
                {
                    "authorId": "1777660",
                    "name": "Naftali Tishby"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4a026fd65af4ba3575e64174de56fee093fa3330",
            "title": "Taming the Noise in Reinforcement Learning via Soft Updates",
            "url": "https://www.semanticscholar.org/paper/4a026fd65af4ba3575e64174de56fee093fa3330",
            "venue": "UAI",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2279912847",
                    "name": "Roy Fox"
                },
                {
                    "authorId": "2279910374",
                    "name": "Ari Pakman"
                },
                {
                    "authorId": "2252012071",
                    "name": "Naftali Tishby"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "48e6a68d7bd4f775343631b38e87e6e27dd18bc8",
            "title": "G-Learning: Taming the Noise in Reinforcement Learning via Soft Updates",
            "url": "https://www.semanticscholar.org/paper/48e6a68d7bd4f775343631b38e87e6e27dd18bc8",
            "venue": "ArXiv",
            "year": 2015
        },
        {
            "arxivId": "1509.02971",
            "authors": [
                {
                    "authorId": "2542999",
                    "name": "T. Lillicrap"
                },
                {
                    "authorId": "2323922",
                    "name": "Jonathan J. Hunt"
                },
                {
                    "authorId": "1863250",
                    "name": "A. Pritzel"
                },
                {
                    "authorId": "2801204",
                    "name": "N. Heess"
                },
                {
                    "authorId": "1968210",
                    "name": "Tom Erez"
                },
                {
                    "authorId": "2109481",
                    "name": "Yuval Tassa"
                },
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "1688276",
                    "name": "Daan Wierstra"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
            "title": "Continuous control with deep reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f",
            "venue": "ICLR",
            "year": 2015
        },
        {
            "arxivId": "1506.02632",
            "authors": [
                {
                    "authorId": "1401932095",
                    "name": "A. PrashanthL."
                },
                {
                    "authorId": "2052847627",
                    "name": "Cheng Jie"
                },
                {
                    "authorId": "1716042",
                    "name": "M. Fu"
                },
                {
                    "authorId": "2666238",
                    "name": "S. Marcus"
                },
                {
                    "authorId": "40868287",
                    "name": "Csaba Szepesvari"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "1c36a38f9cd2f257cea352ff98d815c0060f1bb0",
            "title": "Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control",
            "url": "https://www.semanticscholar.org/paper/1c36a38f9cd2f257cea352ff98d815c0060f1bb0",
            "venue": "ICML",
            "year": 2015
        },
        {
            "arxivId": "1504.00702",
            "authors": [
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "doi": "10.5555/2946645.2946684",
            "intent": [],
            "isInfluential": false,
            "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
            "title": "End-to-End Training of Deep Visuomotor Policies",
            "url": "https://www.semanticscholar.org/paper/b6b8a1b80891c96c28cc6340267b58186157e536",
            "venue": "J. Mach. Learn. Res.",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3255983",
                    "name": "Volodymyr Mnih"
                },
                {
                    "authorId": "2645384",
                    "name": "K. Kavukcuoglu"
                },
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "2228824",
                    "name": "Andrei A. Rusu"
                },
                {
                    "authorId": "144056327",
                    "name": "J. Veness"
                },
                {
                    "authorId": "1792298",
                    "name": "Marc G. Bellemare"
                },
                {
                    "authorId": "1753223",
                    "name": "Alex Graves"
                },
                {
                    "authorId": "3137672",
                    "name": "Martin A. Riedmiller"
                },
                {
                    "authorId": "145600108",
                    "name": "A. Fidjeland"
                },
                {
                    "authorId": "2273072",
                    "name": "Georg Ostrovski"
                },
                {
                    "authorId": "48348688",
                    "name": "Stig Petersen"
                },
                {
                    "authorId": "50388928",
                    "name": "Charlie Beattie"
                },
                {
                    "authorId": "49813280",
                    "name": "Amir Sadik"
                },
                {
                    "authorId": "2460849",
                    "name": "Ioannis Antonoglou"
                },
                {
                    "authorId": "143776287",
                    "name": "Helen King"
                },
                {
                    "authorId": "2106164",
                    "name": "D. Kumaran"
                },
                {
                    "authorId": "1688276",
                    "name": "Daan Wierstra"
                },
                {
                    "authorId": "34313265",
                    "name": "S. Legg"
                },
                {
                    "authorId": "48987704",
                    "name": "D. Hassabis"
                }
            ],
            "doi": "10.1038/nature14236",
            "intent": [],
            "isInfluential": false,
            "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
            "title": "Human-level control through deep reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/340f48901f72278f6bf78a04ee5b01df208cc508",
            "venue": "Nature",
            "year": 2015
        },
        {
            "arxivId": "1502.05477",
            "authors": [
                {
                    "authorId": "47971768",
                    "name": "John Schulman"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "1694621",
                    "name": "Michael I. Jordan"
                },
                {
                    "authorId": "29912342",
                    "name": "Philipp Moritz"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
            "title": "Trust Region Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/449532187c94af3dd3aa55e16d2c50f7854d2199",
            "venue": "ICML",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "3276293",
                    "name": "Guy Lever"
                },
                {
                    "authorId": "2801204",
                    "name": "N. Heess"
                },
                {
                    "authorId": "1804488",
                    "name": "T. Degris"
                },
                {
                    "authorId": "1688276",
                    "name": "Daan Wierstra"
                },
                {
                    "authorId": "3137672",
                    "name": "Martin A. Riedmiller"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
            "title": "Deterministic Policy Gradient Algorithms",
            "url": "https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94",
            "venue": "ICML",
            "year": 2014
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2278681",
                    "name": "S. Shortreed"
                },
                {
                    "authorId": "32734155",
                    "name": "Eric B. Laber"
                },
                {
                    "authorId": "1690162",
                    "name": "D. Lizotte"
                },
                {
                    "authorId": "6456546",
                    "name": "T. Stroup"
                },
                {
                    "authorId": "145134886",
                    "name": "Joelle Pineau"
                },
                {
                    "authorId": "144180010",
                    "name": "S. Murphy"
                }
            ],
            "doi": "10.1007/s10994-010-5229-0",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "8acc06ebabba26b831787c08cba3c9ab7caee850",
            "title": "Informing sequential clinical decision-making through\u00a0reinforcement learning: an empirical study",
            "url": "https://www.semanticscholar.org/paper/8acc06ebabba26b831787c08cba3c9ab7caee850",
            "venue": "Machine Learning",
            "year": 2010
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144918851",
                    "name": "Marc Toussaint"
                }
            ],
            "doi": "10.1145/1553374.1553508",
            "intent": [],
            "isInfluential": false,
            "paperId": "7a7a23f2c39f9b1526bc8853c6c71a5b7f89e68c",
            "title": "Robot trajectory optimization using approximate inference",
            "url": "https://www.semanticscholar.org/paper/7a7a23f2c39f9b1526bc8853c6c71a5b7f89e68c",
            "venue": "ICML '09",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2525772",
                    "name": "A. Danyluk"
                },
                {
                    "authorId": "2249176014",
                    "name": "L\u00e9on Bottou"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "94524df841bb7aaa4cbe73ca2a31bac56146f9f3",
            "title": "Proceedings of the 26th Annual International Conference on Machine Learning",
            "url": "https://www.semanticscholar.org/paper/94524df841bb7aaa4cbe73ca2a31bac56146f9f3",
            "venue": "ICML 2009",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144832491",
                    "name": "E. Todorov"
                }
            ],
            "doi": "10.1109/CDC.2008.4739438",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "1df88f9f0e599b79b1fef6d1bc8dd46271844a0a",
            "title": "General duality between optimal control and estimation",
            "url": "https://www.semanticscholar.org/paper/1df88f9f0e599b79b1fef6d1bc8dd46271844a0a",
            "venue": "2008 47th IEEE Conference on Decision and Control",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2059358552",
                    "name": "P. Cochat"
                },
                {
                    "authorId": "13267685",
                    "name": "L. Vaucoret"
                },
                {
                    "authorId": "2097644863",
                    "name": "J. Sarles"
                }
            ],
            "doi": "10.1136/ebmh.11.4.102",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "bc6dff14a130c57a91d5a21339c23471faf1d46f",
            "title": "Et al",
            "url": "https://www.semanticscholar.org/paper/bc6dff14a130c57a91d5a21339c23471faf1d46f",
            "venue": "Archives de pediatrie : organe officiel de la Societe francaise de pediatrie",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1753269",
                    "name": "Brian D. Ziebart"
                },
                {
                    "authorId": "2306081367",
                    "name": "Andrew L. Maas"
                },
                {
                    "authorId": "1756566",
                    "name": "J. Bagnell"
                },
                {
                    "authorId": "144021446",
                    "name": "A. Dey"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
            "title": "Maximum Entropy Inverse Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/c8221c054459e37edbf313668523d667fe5c1536",
            "venue": "AAAI",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "143683893",
                    "name": "S. Bhatnagar"
                },
                {
                    "authorId": "1699645",
                    "name": "R. Sutton"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "2115790916",
                    "name": "Mark Lee"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "1a74fef3639f99a67fb90460091b05f0916dd054",
            "title": "Incremental Natural Actor-Critic Algorithms",
            "url": "https://www.semanticscholar.org/paper/1a74fef3639f99a67fb90460091b05f0916dd054",
            "venue": "NIPS",
            "year": 2007
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1492009633",
                    "name": "Patrick J. Roa"
                }
            ],
            "doi": "10.1023/A:1017153816538",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "8e6d789ee714d29c9b5156ba9d61b2170d7a315f",
            "title": "Volume 8",
            "url": "https://www.semanticscholar.org/paper/8e6d789ee714d29c9b5156ba9d61b2170d7a315f",
            "venue": "",
            "year": 2001
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1699645",
                    "name": "R. Sutton"
                },
                {
                    "authorId": "145689002",
                    "name": "David A. McAllester"
                },
                {
                    "authorId": "1699868",
                    "name": "Satinder Singh"
                },
                {
                    "authorId": "144830983",
                    "name": "Y. Mansour"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
            "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation",
            "url": "https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b",
            "venue": "NIPS",
            "year": 1999
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2248834664",
                    "name": "Christopher K. I. Williams"
                },
                {
                    "authorId": "2247411478",
                    "name": "Carl E. Rasmussen"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
            "title": "Gaussian Processes for Regression",
            "url": "https://www.semanticscholar.org/paper/4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
            "venue": "NIPS",
            "year": 1995
        },
        {
            "arxivId": null,
            "authors": [],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "a75fe5ebd7bc2f0271b8a0742b70f29bbc4be3fa",
            "title": "Author manuscript, published in \"American Control Conference (2012)\" Model-Free Reinforcement Learning with Continuous Action in Practice",
            "url": "https://www.semanticscholar.org/paper/a75fe5ebd7bc2f0271b8a0742b70f29bbc4be3fa",
            "venue": "",
            "year": 2012
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1786249",
                    "name": "D. Bertsekas"
                }
            ],
            "doi": "10.1007/978-0-387-74759-0_440",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "b225a9eb169a3530289bf834d3b6e785947959ee",
            "title": "Neuro-Dynamic Programming",
            "url": "https://www.semanticscholar.org/paper/b225a9eb169a3530289bf834d3b6e785947959ee",
            "venue": "Encyclopedia of Optimization",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2116648700",
                    "name": "Ronald J. Williams"
                }
            ],
            "doi": "10.1023/A:1022672621406",
            "intent": [],
            "isInfluential": false,
            "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "venue": "Machine Learning",
            "year": 2004
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1699868",
                    "name": "Satinder Singh"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                },
                {
                    "authorId": "1694621",
                    "name": "Michael I. Jordan"
                }
            ],
            "doi": "10.1016/b978-1-55860-335-6.50042-8",
            "intent": [],
            "isInfluential": false,
            "paperId": "a579d06ac278e14948f67748cd651e4eb617ae4e",
            "title": "Learning Without State-Estimation in Partially Observable Markovian Decision Processes",
            "url": "https://www.semanticscholar.org/paper/a579d06ac278e14948f67748cd651e4eb617ae4e",
            "venue": "ICML",
            "year": 1994
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2111256783",
                    "name": "David Wang"
                },
                {
                    "authorId": "72549128",
                    "name": "Guo Yang"
                },
                {
                    "authorId": "2267865",
                    "name": "M. Donath"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "68f21296c9ae0cd1660a4e469dec5db35b6c8920",
            "title": "American Control Conference",
            "url": "https://www.semanticscholar.org/paper/68f21296c9ae0cd1660a4e469dec5db35b6c8920",
            "venue": "",
            "year": 1993
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "title": "Soft Policy Gradient Method for Maximum Entropy Deep Reinforcement Learning",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/30a136e860f105604eba58dc880b9253c5f3202c",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2019
}