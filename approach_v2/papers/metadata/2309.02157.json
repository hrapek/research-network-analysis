{
    "abstract": "Model-based offline reinforcement learning (RL), which builds a supervised transition model with logging dataset to avoid costly interactions with the online environment, has been a promising approach for offline policy optimization. As the discrepancy between the logging data and online environment may result in a distributional shift problem, many prior works have studied how to build robust transition models conservatively and estimate the model uncertainty accurately. However, the over-conservatism can limit the exploration of the agent, and the uncertainty estimates may be unreliable. In this work, we propose a novel Model-based Offline policy optimization framework with Adversarial Network (MOAN). The key idea is to use adversarial learning to build a transition model with better generalization, where an adversary is introduced to distinguish between in-distribution and out-of-distribution samples. Moreover, the adversary can naturally provide a quantification of the model's uncertainty with theoretical guarantees. Extensive experiments showed that our approach outperforms existing state-of-the-art baselines on widely studied offline RL benchmarks. It can also generate diverse in-distribution samples, and quantify the uncertainty more accurately.",
    "arxivId": "2309.02157",
    "authors": [
        {
            "authorId": "2238194434",
            "name": "Junming Yang",
            "url": "https://www.semanticscholar.org/author/2238194434"
        },
        {
            "authorId": "2238108483",
            "name": "Xingguo Chen",
            "url": "https://www.semanticscholar.org/author/2238108483"
        },
        {
            "authorId": "2238039184",
            "name": "Shengyuan Wang",
            "url": "https://www.semanticscholar.org/author/2238039184"
        },
        {
            "authorId": "2238028608",
            "name": "Bolei Zhang",
            "url": "https://www.semanticscholar.org/author/2238028608"
        }
    ],
    "citationVelocity": 0,
    "citations": [
        {
            "arxivId": "2403.07262",
            "authors": [
                {
                    "authorId": "2190750836",
                    "name": "Yunpeng Qing"
                },
                {
                    "authorId": "2128786021",
                    "name": "Shunyu Liu"
                },
                {
                    "authorId": "2220094380",
                    "name": "Jingyuan Cong"
                },
                {
                    "authorId": "145937448",
                    "name": "Kaixuan Chen"
                },
                {
                    "authorId": "2176029606",
                    "name": "Yihe Zhou"
                },
                {
                    "authorId": "2290081578",
                    "name": "Mingli Song"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "a3603df2514a49c0bdad9f3c532fba5fa25d9792",
            "title": "A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective",
            "url": "https://www.semanticscholar.org/paper/a3603df2514a49c0bdad9f3c532fba5fa25d9792",
            "venue": "",
            "year": 2024
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2305401442",
                    "name": "Wandi Qiao"
                },
                {
                    "authorId": "2305458174",
                    "name": "Rui Yang"
                }
            ],
            "doi": "10.1145/3651671.3651762",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "2c1674d05bddd6da11147f9feffd22ea6325482e",
            "title": "Soft Adversarial Offline Reinforcement Learning via Reducing the Attack Strength for Generalization",
            "url": "https://www.semanticscholar.org/paper/2c1674d05bddd6da11147f9feffd22ea6325482e",
            "venue": "ICMLC",
            "year": 2024
        }
    ],
    "corpusId": 261557636,
    "doi": "10.48550/arXiv.2309.02157",
    "fieldsOfStudy": [
        "Computer Science"
    ],
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "isPublisherLicensed": true,
    "is_open_access": true,
    "is_publisher_licensed": true,
    "numCitedBy": 2,
    "numCiting": 40,
    "paperId": "3e4b4d984282b49ac3dd456717944b00ab98ed6b",
    "references": [
        {
            "arxivId": "2210.00030",
            "authors": [
                {
                    "authorId": "2130215451",
                    "name": "Yecheng Jason Ma"
                },
                {
                    "authorId": "2462516",
                    "name": "Shagun Sodhani"
                },
                {
                    "authorId": "144348441",
                    "name": "Dinesh Jayaraman"
                },
                {
                    "authorId": "1697444",
                    "name": "O. Bastani"
                },
                {
                    "authorId": "2109446216",
                    "name": "Vikash Kumar"
                },
                {
                    "authorId": "2111672235",
                    "name": "Amy Zhang"
                }
            ],
            "doi": "10.48550/arXiv.2210.00030",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3",
            "title": "VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training",
            "url": "https://www.semanticscholar.org/paper/3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3",
            "venue": "ICLR",
            "year": 2022
        },
        {
            "arxivId": "2208.05129",
            "authors": [
                {
                    "authorId": "1518267866",
                    "name": "Kishan Panaganti"
                },
                {
                    "authorId": "2181133564",
                    "name": "Zaiyan Xu"
                },
                {
                    "authorId": "1956081",
                    "name": "D. Kalathil"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                }
            ],
            "doi": "10.48550/arXiv.2208.05129",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "ff7ce0b0d2efedf319c5a3002d0286d6a9f7031d",
            "title": "Robust Reinforcement Learning using Offline Data",
            "url": "https://www.semanticscholar.org/paper/ff7ce0b0d2efedf319c5a3002d0286d6a9f7031d",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "arxivId": "2204.12581",
            "authors": [
                {
                    "authorId": "51300315",
                    "name": "Marc Rigter"
                },
                {
                    "authorId": "145350537",
                    "name": "Bruno Lacerda"
                },
                {
                    "authorId": "2072387078",
                    "name": "Nick Hawes"
                }
            ],
            "doi": "10.48550/arXiv.2204.12581",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "447ad78823fbf3369e72e8d1c36467f76f8dc427",
            "title": "RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/447ad78823fbf3369e72e8d1c36467f76f8dc427",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "arxivId": "2202.13890",
            "authors": [
                {
                    "authorId": "51347064",
                    "name": "Laixi Shi"
                },
                {
                    "authorId": "2108548762",
                    "name": "Gen Li"
                },
                {
                    "authorId": "3444254",
                    "name": "Yuting Wei"
                },
                {
                    "authorId": "47557914",
                    "name": "Yuxin Chen"
                },
                {
                    "authorId": "1784472",
                    "name": "Yuejie Chi"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "e1ac9023f2991ebc840b99d6f0203201a3adfaf4",
            "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity",
            "url": "https://www.semanticscholar.org/paper/e1ac9023f2991ebc840b99d6f0203201a3adfaf4",
            "venue": "ICML",
            "year": 2022
        },
        {
            "arxivId": "2202.02446",
            "authors": [
                {
                    "authorId": "2109943279",
                    "name": "Ching-An Cheng"
                },
                {
                    "authorId": "51437774",
                    "name": "Tengyang Xie"
                },
                {
                    "authorId": "48272707",
                    "name": "Nan Jiang"
                },
                {
                    "authorId": "40333747",
                    "name": "Alekh Agarwal"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "277e63a452cb1fb6c5ad88d110d5f18401e840c0",
            "title": "Adversarially Trained Actor Critic for Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/277e63a452cb1fb6c5ad88d110d5f18401e840c0",
            "venue": "ICML",
            "year": 2022
        },
        {
            "arxivId": "2110.06169",
            "authors": [
                {
                    "authorId": "2000906",
                    "name": "Ilya Kostrikov"
                },
                {
                    "authorId": "3422774",
                    "name": "Ashvin Nair"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
            "title": "Offline Reinforcement Learning with Implicit Q-Learning",
            "url": "https://www.semanticscholar.org/paper/348a855fe01f3f4273bf0ecf851ca688686dbfcc",
            "venue": "ICLR",
            "year": 2021
        },
        {
            "arxivId": "2110.00188",
            "authors": [
                {
                    "authorId": "2110364952",
                    "name": "Jianhao Wang"
                },
                {
                    "authorId": "2135914554",
                    "name": "Wenzhe Li"
                },
                {
                    "authorId": "2152631650",
                    "name": "Haozhe Jiang"
                },
                {
                    "authorId": "48924218",
                    "name": "Guangxiang Zhu"
                },
                {
                    "authorId": "2118155623",
                    "name": "Siyuan Li"
                },
                {
                    "authorId": "2111387140",
                    "name": "Chongjie Zhang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "9e8df3b1fad0e219bd96d254784dee40736057ff",
            "title": "Offline Reinforcement Learning with Reverse Model-based Imagination",
            "url": "https://www.semanticscholar.org/paper/9e8df3b1fad0e219bd96d254784dee40736057ff",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "arxivId": "2106.06926",
            "authors": [
                {
                    "authorId": "51437774",
                    "name": "Tengyang Xie"
                },
                {
                    "authorId": "2109943279",
                    "name": "Ching-An Cheng"
                },
                {
                    "authorId": "48272707",
                    "name": "Nan Jiang"
                },
                {
                    "authorId": "3040175",
                    "name": "Paul Mineiro"
                },
                {
                    "authorId": "40333747",
                    "name": "Alekh Agarwal"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "e2ad21dae85950ab3631f65a0f142924c99fb9c4",
            "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/e2ad21dae85950ab3631f65a0f142924c99fb9c4",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "arxivId": "2106.06860",
            "authors": [
                {
                    "authorId": "14637819",
                    "name": "Scott Fujimoto"
                },
                {
                    "authorId": "2046135",
                    "name": "S. Gu"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
            "title": "A Minimalist Approach to Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/c879b25308026d6538e52b27bcf4fd3cb60855f3",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "arxivId": "2102.08363",
            "authors": [
                {
                    "authorId": "10909315",
                    "name": "Tianhe Yu"
                },
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "102801230",
                    "name": "Rafael Rafailov"
                },
                {
                    "authorId": "19275599",
                    "name": "A. Rajeswaran"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
            "title": "COMBO: Conservative Offline Model-Based Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/245682e8b3fa76f4a3e2991b5497577af95cbb3f",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3072035",
                    "name": "A. Coronato"
                },
                {
                    "authorId": "34343572",
                    "name": "Muddasar Naeem"
                },
                {
                    "authorId": "144840023",
                    "name": "G. Pietro"
                },
                {
                    "authorId": "2770509",
                    "name": "Giovanni Paragliola"
                }
            ],
            "doi": "10.1016/J.ARTMED.2020.101964",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "2bed17dc270c5c185278c262b2424295b6a21fce",
            "title": "Reinforcement learning for intelligent healthcare applications: A survey",
            "url": "https://www.semanticscholar.org/paper/2bed17dc270c5c185278c262b2424295b6a21fce",
            "venue": "Artif. Intell. Medicine",
            "year": 2020
        },
        {
            "arxivId": "2006.04779",
            "authors": [
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "35499972",
                    "name": "Aurick Zhou"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
            "title": "Conservative Q-Learning for Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/28db20a81eec74a50204686c3cf796c42a020d2e",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2005.13239",
            "authors": [
                {
                    "authorId": "10909315",
                    "name": "Tianhe Yu"
                },
                {
                    "authorId": "8234443",
                    "name": "G. Thomas"
                },
                {
                    "authorId": "3469209",
                    "name": "Lantao Yu"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "145085305",
                    "name": "James Y. Zou"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                },
                {
                    "authorId": "1901958",
                    "name": "Tengyu Ma"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c",
            "title": "MOPO: Model-based Offline Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/dea0f1c5949f8d898b9b6ff68226a781558e413c",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2005.05951",
            "authors": [
                {
                    "authorId": "1891978",
                    "name": "Rahul Kidambi"
                },
                {
                    "authorId": "19275599",
                    "name": "A. Rajeswaran"
                },
                {
                    "authorId": "1751626",
                    "name": "Praneeth Netrapalli"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
            "title": "MOReL : Model-Based Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/309c2c5ee60e725244da09180f913cd8d4b8d4e9",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2005.01643",
            "authors": [
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "2550764",
                    "name": "Justin Fu"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
            "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
            "url": "https://www.semanticscholar.org/paper/5e7bc93622416f14e6948a500278bfbe58cd3890",
            "venue": "ArXiv",
            "year": 2020
        },
        {
            "arxivId": "2004.07219",
            "authors": [
                {
                    "authorId": "2550764",
                    "name": "Justin Fu"
                },
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "7624658",
                    "name": "Ofir Nachum"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
            "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/a326d9f2d2d351001fece788165dbcbb524da2e4",
            "venue": "ArXiv",
            "year": 2020
        },
        {
            "arxivId": "2003.00722",
            "authors": [
                {
                    "authorId": "38378048",
                    "name": "Junfeng Wen"
                },
                {
                    "authorId": "144445933",
                    "name": "Bo Dai"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "50319359",
                    "name": "D. Schuurmans"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "50d18a58adbfa995fb172b942dd3c9f31b490fc9",
            "title": "Batch Stationary Distribution Estimation",
            "url": "https://www.semanticscholar.org/paper/50d18a58adbfa995fb172b942dd3c9f31b490fc9",
            "venue": "ICML",
            "year": 2020
        },
        {
            "arxivId": "1911.03845",
            "authors": [
                {
                    "authorId": "8536136",
                    "name": "Xueying Bai"
                },
                {
                    "authorId": "2323534462",
                    "name": "Jian Guan"
                },
                {
                    "authorId": "31825390",
                    "name": "Hongning Wang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "3e49c93b89fa1616281fccd78b4a49c7b133bd53",
            "title": "Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation",
            "url": "https://www.semanticscholar.org/paper/3e49c93b89fa1616281fccd78b4a49c7b133bd53",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": "1910.00177",
            "authors": [
                {
                    "authorId": "32200465",
                    "name": "X. B. Peng"
                },
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "119557909",
                    "name": "Grace Zhang"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "ad14227e4f51276892ffc37aa43fd8750bb5eba8",
            "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/ad14227e4f51276892ffc37aa43fd8750bb5eba8",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": "1911.11361",
            "authors": [
                {
                    "authorId": "2013680445",
                    "name": "Yifan Wu"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "7624658",
                    "name": "Ofir Nachum"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
            "title": "Behavior Regularized Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/9be492858863c8c7c24be1ecb75724de5086bd8e",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": "1906.00949",
            "authors": [
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "2550764",
                    "name": "Justin Fu"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
            "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction",
            "url": "https://www.semanticscholar.org/paper/82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": "1905.08188",
            "authors": [
                {
                    "authorId": "46818360",
                    "name": "E. Derman"
                },
                {
                    "authorId": "3187297",
                    "name": "D. Mankowitz"
                },
                {
                    "authorId": "2554720",
                    "name": "Timothy A. Mann"
                },
                {
                    "authorId": "1712535",
                    "name": "Shie Mannor"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "03367b0458e08105475dba32dbacee1fe0b7f311",
            "title": "A Bayesian Approach to Robust Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/03367b0458e08105475dba32dbacee1fe0b7f311",
            "venue": "UAI",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "2067208983",
                    "name": "T. Hubert"
                },
                {
                    "authorId": "4337102",
                    "name": "Julian Schrittwieser"
                },
                {
                    "authorId": "2460849",
                    "name": "Ioannis Antonoglou"
                },
                {
                    "authorId": "40227832",
                    "name": "Matthew Lai"
                },
                {
                    "authorId": "35099444",
                    "name": "A. Guez"
                },
                {
                    "authorId": "1975889",
                    "name": "Marc Lanctot"
                },
                {
                    "authorId": "2175946",
                    "name": "L. Sifre"
                },
                {
                    "authorId": "2106164",
                    "name": "D. Kumaran"
                },
                {
                    "authorId": "1686971",
                    "name": "T. Graepel"
                },
                {
                    "authorId": "2542999",
                    "name": "T. Lillicrap"
                },
                {
                    "authorId": "34838386",
                    "name": "K. Simonyan"
                },
                {
                    "authorId": "48987704",
                    "name": "D. Hassabis"
                }
            ],
            "doi": "10.1126/science.aar6404",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
            "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play",
            "url": "https://www.semanticscholar.org/paper/f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
            "venue": "Science",
            "year": 2018
        },
        {
            "arxivId": "1810.08693",
            "authors": [
                {
                    "authorId": "2477489",
                    "name": "L. Devroye"
                },
                {
                    "authorId": "1685567",
                    "name": "Abbas Mehrabian"
                },
                {
                    "authorId": "50816670",
                    "name": "Tommy Reddad"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "b1048213518de9b7dba8225cfc5fe350c1557759",
            "title": "The total variation distance between high-dimensional Gaussians",
            "url": "https://www.semanticscholar.org/paper/b1048213518de9b7dba8225cfc5fe350c1557759",
            "venue": "",
            "year": 2018
        },
        {
            "arxivId": "1807.03858",
            "authors": [
                {
                    "authorId": "3286703",
                    "name": "Huazhe Xu"
                },
                {
                    "authorId": "2110486765",
                    "name": "Yuanzhi Li"
                },
                {
                    "authorId": "39402399",
                    "name": "Yuandong Tian"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "1901958",
                    "name": "Tengyu Ma"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "d333f99881b09426283a9c7a1d25f7ac30d63062",
            "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees",
            "url": "https://www.semanticscholar.org/paper/d333f99881b09426283a9c7a1d25f7ac30d63062",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "arxivId": "1806.02448",
            "authors": [
                {
                    "authorId": "15099359",
                    "name": "R. Torrado"
                },
                {
                    "authorId": "14171685",
                    "name": "Philip Bontrager"
                },
                {
                    "authorId": "1810053",
                    "name": "J. Togelius"
                },
                {
                    "authorId": "46701122",
                    "name": "Jialin Liu"
                },
                {
                    "authorId": "1389741275",
                    "name": "Diego P\u00e9rez-Li\u00e9bana"
                }
            ],
            "doi": "10.1109/CIG.2018.8490422",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4d357ffc1cf60d3f34b5345899619882791474bb",
            "title": "Deep Reinforcement Learning for General Video Game AI",
            "url": "https://www.semanticscholar.org/paper/4d357ffc1cf60d3f34b5345899619882791474bb",
            "venue": "2018 IEEE Conference on Computational Intelligence and Games (CIG)",
            "year": 2018
        },
        {
            "arxivId": "1805.12114",
            "authors": [
                {
                    "authorId": "46224156",
                    "name": "Kurtland Chua"
                },
                {
                    "authorId": "35159852",
                    "name": "R. Calandra"
                },
                {
                    "authorId": "49686609",
                    "name": "R. McAllister"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
            "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
            "url": "https://www.semanticscholar.org/paper/56136aa0b2c347cbcf3d50821f310c4253155026",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "22698655",
                    "name": "Guanjie Zheng"
                },
                {
                    "authorId": "2642200",
                    "name": "Fuzheng Zhang"
                },
                {
                    "authorId": "2109675154",
                    "name": "Zihan Zheng"
                },
                {
                    "authorId": "70402545",
                    "name": "Yang Xiang"
                },
                {
                    "authorId": "2123146",
                    "name": "Nicholas Jing Yuan"
                },
                {
                    "authorId": "144076239",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2109640666",
                    "name": "Z. Li"
                }
            ],
            "doi": "10.1145/3178876.3185994",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "fa54b47df8641dff1579b5e8e0f18f057de68e73",
            "title": "DRN: A Deep Reinforcement Learning Framework for News Recommendation",
            "url": "https://www.semanticscholar.org/paper/fa54b47df8641dff1579b5e8e0f18f057de68e73",
            "venue": "WWW",
            "year": 2018
        },
        {
            "arxivId": "1801.01290",
            "authors": [
                {
                    "authorId": "2587648",
                    "name": "Tuomas Haarnoja"
                },
                {
                    "authorId": "35499972",
                    "name": "Aurick Zhou"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "811df72e210e20de99719539505da54762a11c6d",
            "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
            "url": "https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d",
            "venue": "ICML",
            "year": 2018
        },
        {
            "arxivId": "1703.03864",
            "authors": [
                {
                    "authorId": "2887364",
                    "name": "Tim Salimans"
                },
                {
                    "authorId": "2126278",
                    "name": "Jonathan Ho"
                },
                {
                    "authorId": "41192764",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "1701686",
                    "name": "I. Sutskever"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4ee802a58d32aa049d549d06be440ac947b53987",
            "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/4ee802a58d32aa049d549d06be440ac947b53987",
            "venue": "ArXiv",
            "year": 2017
        },
        {
            "arxivId": "1606.03476",
            "authors": [
                {
                    "authorId": "2126278",
                    "name": "Jonathan Ho"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
            "title": "Generative Adversarial Imitation Learning",
            "url": "https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
            "venue": "NIPS",
            "year": 2016
        },
        {
            "arxivId": "1502.05477",
            "authors": [
                {
                    "authorId": "47971768",
                    "name": "John Schulman"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "1694621",
                    "name": "Michael I. Jordan"
                },
                {
                    "authorId": "29912342",
                    "name": "Philipp Moritz"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
            "title": "Trust Region Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/449532187c94af3dd3aa55e16d2c50f7854d2199",
            "venue": "ICML",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1708497",
                    "name": "A. Gretton"
                },
                {
                    "authorId": "1704422",
                    "name": "Karsten M. Borgwardt"
                },
                {
                    "authorId": "1733256",
                    "name": "M. Rasch"
                },
                {
                    "authorId": "1707625",
                    "name": "B. Scholkopf"
                },
                {
                    "authorId": "46234526",
                    "name": "Alex Smola"
                }
            ],
            "doi": "10.5555/2503308.2188410",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "225f78ae8a44723c136646044fd5c5d7f1d3d15a",
            "title": "A Kernel Two-Sample Test",
            "url": "https://www.semanticscholar.org/paper/225f78ae8a44723c136646044fd5c5d7f1d3d15a",
            "venue": "J. Mach. Learn. Res.",
            "year": 2012
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2359139",
                    "name": "B. Fuglede"
                },
                {
                    "authorId": "9300024",
                    "name": "F. Tops\u00f8e"
                }
            ],
            "doi": "10.1109/ISIT.2004.1365067",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "f150d0dc94098561b5372cad50b4610589a934e6",
            "title": "Jensen-Shannon divergence and Hilbert space embedding",
            "url": "https://www.semanticscholar.org/paper/f150d0dc94098561b5372cad50b4610589a934e6",
            "venue": "International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings.",
            "year": 2004
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "150011608",
                    "name": "A. A. Fedotov"
                },
                {
                    "authorId": "2621446",
                    "name": "P. Harremo\u00ebs"
                },
                {
                    "authorId": "9300024",
                    "name": "F. Tops\u00f8e"
                }
            ],
            "doi": "10.1109/TIT.2003.811927",
            "intent": [],
            "isInfluential": false,
            "paperId": "f92d5e553b8514e56b1c77340faabaec996bd45c",
            "title": "Refinements of Pinsker's inequality",
            "url": "https://www.semanticscholar.org/paper/f92d5e553b8514e56b1c77340faabaec996bd45c",
            "venue": "IEEE Trans. Inf. Theory",
            "year": 2003
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "143965435",
                    "name": "A. M\u00fcller"
                }
            ],
            "doi": "10.2307/1428011",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "f3d4c52a3cfcc6c550fa30cc20c516e285eabf00",
            "title": "Integral Probability Metrics and Their Generating Classes of Functions",
            "url": "https://www.semanticscholar.org/paper/f3d4c52a3cfcc6c550fa30cc20c516e285eabf00",
            "venue": "Advances in Applied Probability",
            "year": 1997
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2118652926",
                    "name": "Xingguo Chen"
                },
                {
                    "authorId": "2349946",
                    "name": "Xingzhou Ma"
                },
                {
                    "authorId": "2154898207",
                    "name": "Yang Li"
                },
                {
                    "authorId": "2149521083",
                    "name": "Guang Yang"
                },
                {
                    "authorId": "3385090",
                    "name": "Shangdong Yang"
                },
                {
                    "authorId": "2256636612",
                    "name": "Yang Gao"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "7ee282e982d00c152b86f24fce965304008f3eb3",
            "title": "Modified Retrace for Off-Policy Temporal Difference Learning",
            "url": "https://www.semanticscholar.org/paper/7ee282e982d00c152b86f24fce965304008f3eb3",
            "venue": "UAI",
            "year": 2023
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2115732606",
                    "name": "Jian Shen"
                },
                {
                    "authorId": "2389257",
                    "name": "Mingcheng Chen"
                },
                {
                    "authorId": "2116706397",
                    "name": "Zhicheng Zhang"
                },
                {
                    "authorId": "2149232039",
                    "name": "Zhengyu Yang"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2111510213",
                    "name": "Yong Yu"
                }
            ],
            "doi": "10.1007/978-3-030-86486-6_11",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "38453770ee3e54550aefcd6c4ccd99448dd21441",
            "title": "Model-Based Offline Policy Optimization with Distribution Correcting Regularization",
            "url": "https://www.semanticscholar.org/paper/38453770ee3e54550aefcd6c4ccd99448dd21441",
            "venue": "ECML/PKDD",
            "year": 2021
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "51228100",
                    "name": "Masatoshi Uehara"
                },
                {
                    "authorId": "144426657",
                    "name": "Wen Sun"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "3289944a9626301fec08960dae8cfd3ab74987d9",
            "title": "Pessimistic Model-based Offline RL: PAC Bounds and Posterior Sampling under Partial Coverage",
            "url": "https://www.semanticscholar.org/paper/3289944a9626301fec08960dae8cfd3ab74987d9",
            "venue": "ArXiv",
            "year": 2021
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "title": "Model-based Offline Policy Optimization with Adversarial Network",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/3e4b4d984282b49ac3dd456717944b00ab98ed6b",
    "venue": "European Conference on Artificial Intelligence",
    "year": 2023
}