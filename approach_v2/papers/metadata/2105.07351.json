{
    "abstract": "The recent offline reinforcement learning (RL) studies have achieved much progress to make RL usable in real-world systems by learning policies from pre-collected datasets without environment interaction. Unfortunately, existing offline RL methods still face many practical challenges in real-world system control tasks, such as computational restriction during agent training and the requirement of extra control flexibility. The model-based planning framework provides an attractive alternative. However, most model-based planning algorithms are not designed for offline settings. Simply combining the ingredients of offline RL with existing methods either provides over-restrictive planning or leads to inferior performance. We propose a new light-weighted model-based offline planning framework, namely MOPP, which tackles the dilemma between the restrictions of offline learning and high-performance planning. MOPP encourages more aggressive trajectory rollout guided by the behavior policy learned from data, and prunes out problematic trajectories to avoid potential out-of-distribution samples. Experimental results show that MOPP provides competitive performance compared with existing model-based offline planning and RL approaches.",
    "arxivId": "2105.07351",
    "authors": [
        {
            "authorId": "2242851906",
            "name": "Xianyuan Zhan",
            "url": "https://www.semanticscholar.org/author/2242851906"
        },
        {
            "authorId": "2144104090",
            "name": "Xiangyu Zhu",
            "url": "https://www.semanticscholar.org/author/2144104090"
        },
        {
            "authorId": "49507262",
            "name": "Haoran Xu",
            "url": "https://www.semanticscholar.org/author/49507262"
        }
    ],
    "citationVelocity": 10,
    "citations": [
        {
            "arxivId": "2410.11234",
            "authors": [
                {
                    "authorId": "2325992798",
                    "name": "Jiayu Chen"
                },
                {
                    "authorId": "2276657486",
                    "name": "Wentse Chen"
                },
                {
                    "authorId": "2307561296",
                    "name": "Jeff Schneider"
                }
            ],
            "doi": "10.48550/arXiv.2410.11234",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "753989375fed1c872d6f367de3320f37490a01cc",
            "title": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/753989375fed1c872d6f367de3320f37490a01cc",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": "2410.11711",
            "authors": [
                {
                    "authorId": "2256991069",
                    "name": "Abdelhakim Benechehab"
                },
                {
                    "authorId": "2325953321",
                    "name": "Youssef Attia El Hili"
                },
                {
                    "authorId": "2261363007",
                    "name": "Ambroise Odonnat"
                },
                {
                    "authorId": "2314829950",
                    "name": "Oussama Zekri"
                },
                {
                    "authorId": "2257432917",
                    "name": "Albert Thomas"
                },
                {
                    "authorId": "2256994533",
                    "name": "Giuseppe Paolo"
                },
                {
                    "authorId": "2256996157",
                    "name": "Maurizio Filippone"
                },
                {
                    "authorId": "145898069",
                    "name": "I. Redko"
                },
                {
                    "authorId": "2256623448",
                    "name": "Bal'azs K'egl"
                }
            ],
            "doi": "10.48550/arXiv.2410.11711",
            "intent": [],
            "isInfluential": false,
            "paperId": "369bd8479daa33df68567e93e00a52155320ed88",
            "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models",
            "url": "https://www.semanticscholar.org/paper/369bd8479daa33df68567e93e00a52155320ed88",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3342918",
                    "name": "Qichao Zhang"
                },
                {
                    "authorId": "2189551109",
                    "name": "Xing Fang"
                },
                {
                    "authorId": "2320392545",
                    "name": "Kaixuan Xu"
                },
                {
                    "authorId": "2320534750",
                    "name": "Weixin Zhao"
                },
                {
                    "authorId": "2145539850",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2259831523",
                    "name": "Dongbin Zhao"
                }
            ],
            "doi": "10.1109/IJCNN60899.2024.10651451",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "9670017de23a58fb5084728c1758889317dbb13a",
            "title": "High-quality Synthetic Data is Efficient for Model-based Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/9670017de23a58fb5084728c1758889317dbb13a",
            "venue": "2024 International Joint Conference on Neural Networks (IJCNN)",
            "year": 2024
        },
        {
            "arxivId": "2405.14374",
            "authors": [
                {
                    "authorId": "2191626656",
                    "name": "Charles A. Hepburn"
                },
                {
                    "authorId": "2302991696",
                    "name": "Yue Jin"
                },
                {
                    "authorId": "2302793525",
                    "name": "Giovanni Montana"
                }
            ],
            "doi": "10.48550/arXiv.2405.14374",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "699d6ba5a173b30df697bc7514b8b1b03d35d762",
            "title": "State-Constrained Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/699d6ba5a173b30df697bc7514b8b1b03d35d762",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2130143749",
                    "name": "Imran Adham"
                },
                {
                    "authorId": "2301538906",
                    "name": "Hang Wang"
                },
                {
                    "authorId": "1641386905",
                    "name": "Sen Lin"
                },
                {
                    "authorId": "2301415609",
                    "name": "Junshan Zhang"
                }
            ],
            "doi": "10.1109/MOST60774.2024.00026",
            "intent": [],
            "isInfluential": false,
            "paperId": "a29649e3a79ea1d350103c3a55e4067c4c9a4f88",
            "title": "L-MBOP-E: Latent-Model Based Offline Planning with Extrinsic Policy Guided Exploration",
            "url": "https://www.semanticscholar.org/paper/a29649e3a79ea1d350103c3a55e4067c4c9a4f88",
            "venue": "2024 IEEE International Conference on Mobility, Operations, Services and Technologies (MOST)",
            "year": 2024
        },
        {
            "arxivId": "2402.02858",
            "authors": [
                {
                    "authorId": "2256991069",
                    "name": "Abdelhakim Benechehab"
                },
                {
                    "authorId": "2257432917",
                    "name": "Albert Thomas"
                },
                {
                    "authorId": "2256623448",
                    "name": "Bal'azs K'egl"
                }
            ],
            "doi": "10.48550/arXiv.2402.02858",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "d26a3d340965aa58c1bc9c1e74d909a26d1ac346",
            "title": "Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/d26a3d340965aa58c1bc9c1e74d909a26d1ac346",
            "venue": "ArXiv",
            "year": 2024
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2266876559",
                    "name": "Weiwei Wang"
                },
                {
                    "authorId": "48513533",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2267086360",
                    "name": "Xianyi Wu"
                }
            ],
            "doi": "10.1007/s11222-023-10351-y",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "a9ded3c45f3f0c9b52799a6fb9d6de1f8cbdc960",
            "title": "Off-policy evaluation for tabular reinforcement learning with synthetic trajectories",
            "url": "https://www.semanticscholar.org/paper/a9ded3c45f3f0c9b52799a6fb9d6de1f8cbdc960",
            "venue": "Statistics and Computing",
            "year": 2023
        },
        {
            "arxivId": "2310.20025",
            "authors": [
                {
                    "authorId": "2211737372",
                    "name": "Mianchu Wang"
                },
                {
                    "authorId": "145094495",
                    "name": "Rui Yang"
                },
                {
                    "authorId": "2265458453",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2263482497",
                    "name": "Meng Fang"
                }
            ],
            "doi": "10.48550/arXiv.2310.20025",
            "intent": [],
            "isInfluential": false,
            "paperId": "1d97ed822e13688ef48eb005f428081fb08d6f28",
            "title": "GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models",
            "url": "https://www.semanticscholar.org/paper/1d97ed822e13688ef48eb005f428081fb08d6f28",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2023
        },
        {
            "arxivId": "2310.05672",
            "authors": [
                {
                    "authorId": "2256991069",
                    "name": "Abdelhakim Benechehab"
                },
                {
                    "authorId": "2256994533",
                    "name": "Giuseppe Paolo"
                },
                {
                    "authorId": "2257432917",
                    "name": "Albert Thomas"
                },
                {
                    "authorId": "2256996157",
                    "name": "Maurizio Filippone"
                },
                {
                    "authorId": "2256623448",
                    "name": "Bal'azs K'egl"
                }
            ],
            "doi": "10.48550/arXiv.2310.05672",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "182c8e48d08b8ff29f0854fe903b3bae8e0c999d",
            "title": "Multi-timestep models for Model-based Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/182c8e48d08b8ff29f0854fe903b3bae8e0c999d",
            "venue": "ArXiv",
            "year": 2023
        },
        {
            "arxivId": "2307.11620",
            "authors": [
                {
                    "authorId": "2210534187",
                    "name": "Xiangsen Wang"
                },
                {
                    "authorId": "49507262",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "2224472670",
                    "name": "Yinan Zheng"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                }
            ],
            "doi": "10.48550/arXiv.2307.11620",
            "intent": [],
            "isInfluential": false,
            "paperId": "e6a54f56883e009f09259d98010981d8cf0b80f3",
            "title": "Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization",
            "url": "https://www.semanticscholar.org/paper/e6a54f56883e009f09259d98010981d8cf0b80f3",
            "venue": "NeurIPS",
            "year": 2023
        },
        {
            "arxivId": "2306.14069",
            "authors": [
                {
                    "authorId": "2087062017",
                    "name": "Anirudhan Badrinath"
                },
                {
                    "authorId": "1412725415",
                    "name": "Yannis Flet-Berliac"
                },
                {
                    "authorId": "21771052",
                    "name": "Allen Nie"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": "10.48550/arXiv.2306.14069",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "13bbd1055e9a56aaa3ee8879375e12fdc00d52f8",
            "title": "Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets",
            "url": "https://www.semanticscholar.org/paper/13bbd1055e9a56aaa3ee8879375e12fdc00d52f8",
            "venue": "NeurIPS",
            "year": 2023
        },
        {
            "arxivId": "2306.08900",
            "authors": [
                {
                    "authorId": "2210534187",
                    "name": "Xiangsen Wang"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                }
            ],
            "doi": "10.5555/3545946.3599076",
            "intent": [],
            "isInfluential": false,
            "paperId": "33d92b568e79176ab607a3b9d991c686d8300765",
            "title": "Offline Multi-Agent Reinforcement Learning with Coupled Value Factorization",
            "url": "https://www.semanticscholar.org/paper/33d92b568e79176ab607a3b9d991c686d8300765",
            "venue": "AAMAS",
            "year": 2023
        },
        {
            "arxivId": "2306.04220",
            "authors": [
                {
                    "authorId": "2219274266",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                },
                {
                    "authorId": "2146254134",
                    "name": "Zhihao Wu"
                },
                {
                    "authorId": null,
                    "name": "Wenjia Zhang"
                },
                {
                    "authorId": "2219667539",
                    "name": "Shoucheng Song"
                },
                {
                    "authorId": "2144396405",
                    "name": "Han Wang"
                },
                {
                    "authorId": "2624174",
                    "name": "Youfang Lin"
                },
                {
                    "authorId": "2148655709",
                    "name": "Li Jiang"
                }
            ],
            "doi": "10.48550/arXiv.2306.04220",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "157f71d01fbd1799e30e1b1bf68d53a60c2ad107",
            "title": "Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL",
            "url": "https://www.semanticscholar.org/paper/157f71d01fbd1799e30e1b1bf68d53a60c2ad107",
            "venue": "NeurIPS",
            "year": 2023
        },
        {
            "arxivId": "2305.01400",
            "authors": [
                {
                    "authorId": "1379931813",
                    "name": "Geoffrey Cideron"
                },
                {
                    "authorId": "2215873836",
                    "name": "B. Tabanpour"
                },
                {
                    "authorId": "12646136",
                    "name": "Sebastian Curi"
                },
                {
                    "authorId": "35022714",
                    "name": "Sertan Girgin"
                },
                {
                    "authorId": "122562941",
                    "name": "L'eonard Hussenot"
                },
                {
                    "authorId": "1387885286",
                    "name": "Gabriel Dulac-Arnold"
                },
                {
                    "authorId": "1737555",
                    "name": "M. Geist"
                },
                {
                    "authorId": "1721354",
                    "name": "O. Pietquin"
                },
                {
                    "authorId": "51914693",
                    "name": "Robert Dadashi"
                }
            ],
            "doi": "10.48550/arXiv.2305.01400",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "79e1d443805db1e655daf55a231a180c3be383ae",
            "title": "Get Back Here: Robust Imitation by Return-to-Distribution Planning",
            "url": "https://www.semanticscholar.org/paper/79e1d443805db1e655daf55a231a180c3be383ae",
            "venue": "ArXiv",
            "year": 2023
        },
        {
            "arxivId": "2304.04660",
            "authors": [
                {
                    "authorId": "2155570419",
                    "name": "Junjie Zhang"
                },
                {
                    "authorId": "2008151131",
                    "name": "Jiafei Lyu"
                },
                {
                    "authorId": "2125106047",
                    "name": "Xiaoteng Ma"
                },
                {
                    "authorId": "30411824",
                    "name": "Jiangpeng Yan"
                },
                {
                    "authorId": "2146157882",
                    "name": "Jun Yang"
                },
                {
                    "authorId": "2187301367",
                    "name": "Le Wan"
                },
                {
                    "authorId": "2116523082",
                    "name": "Xiu Li"
                }
            ],
            "doi": "10.3233/FAIA230618",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "bfd9d1b5d0b4af68d393ee4865aaa2eeebd97ae6",
            "title": "Uncertainty-Driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/bfd9d1b5d0b4af68d393ee4865aaa2eeebd97ae6",
            "venue": "ECAI",
            "year": 2023
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "119973883",
                    "name": "Christopher Diehl"
                },
                {
                    "authorId": "2037489529",
                    "name": "Timo Sievernich"
                },
                {
                    "authorId": "1379617041",
                    "name": "Martin Kr\u00fcger"
                },
                {
                    "authorId": "143762003",
                    "name": "F. Hoffmann"
                },
                {
                    "authorId": "4637299",
                    "name": "Torsten Bertram"
                }
            ],
            "doi": "10.1109/LRA.2023.3236579",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "134c2b434296eff29d56f1b83b6c60e068fba23c",
            "title": "Uncertainty-Aware Model-Based Offline Reinforcement Learning for Automated Driving",
            "url": "https://www.semanticscholar.org/paper/134c2b434296eff29d56f1b83b6c60e068fba23c",
            "venue": "IEEE Robotics and Automation Letters",
            "year": 2023
        },
        {
            "arxivId": "2301.11426",
            "authors": [
                {
                    "authorId": "66742509",
                    "name": "Kefan Dong"
                },
                {
                    "authorId": "1412725415",
                    "name": "Yannis Flet-Berliac"
                },
                {
                    "authorId": "21771052",
                    "name": "Allen Nie"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": "10.48550/arXiv.2301.11426",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "3aa1eafd0198ef0f49f79ff376ac60b170bb10e0",
            "title": "Model-based Offline Reinforcement Learning with Local Misspecification",
            "url": "https://www.semanticscholar.org/paper/3aa1eafd0198ef0f49f79ff376ac60b170bb10e0",
            "venue": "AAAI",
            "year": 2023
        },
        {
            "arxivId": "2212.04280",
            "authors": [
                {
                    "authorId": "2191626656",
                    "name": "Charles A. Hepburn"
                },
                {
                    "authorId": "1699650",
                    "name": "G. Montana"
                }
            ],
            "doi": "10.48550/arXiv.2212.04280",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "782c3235b6e9a3f9711274eedffa114792657f93",
            "title": "Model-based trajectory stitching for improved behavioural cloning and its applications",
            "url": "https://www.semanticscholar.org/paper/782c3235b6e9a3f9711274eedffa114792657f93",
            "venue": "Mach. Learn.",
            "year": 2022
        },
        {
            "arxivId": "2211.16078",
            "authors": [
                {
                    "authorId": "2141508648",
                    "name": "Guoxi Zhang"
                },
                {
                    "authorId": "2785830",
                    "name": "H. Kashima"
                }
            ],
            "doi": "10.48550/arXiv.2211.16078",
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "6616965ea639e90c2413cff00aedf0133ce80fd9",
            "title": "Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/6616965ea639e90c2413cff00aedf0133ce80fd9",
            "venue": "AAAI",
            "year": 2022
        },
        {
            "arxivId": "2211.11603",
            "authors": [
                {
                    "authorId": "2191626656",
                    "name": "Charles A. Hepburn"
                },
                {
                    "authorId": "1699650",
                    "name": "G. Montana"
                }
            ],
            "doi": "10.48550/arXiv.2211.11603",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "b9ecf9fb3b2d5a70451fa30911142e55848fde2d",
            "title": "Model-based Trajectory Stitching for Improved Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/b9ecf9fb3b2d5a70451fa30911142e55848fde2d",
            "venue": "ArXiv",
            "year": 2022
        },
        {
            "arxivId": "2210.08323",
            "authors": [
                {
                    "authorId": "49507262",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "2188128317",
                    "name": "Li Jiang"
                },
                {
                    "authorId": "2136086524",
                    "name": "Jianxiong Li"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                }
            ],
            "doi": "10.48550/arXiv.2210.08323",
            "intent": [],
            "isInfluential": true,
            "paperId": "60380ee913d20e722368245f23e0d4baf52e139a",
            "title": "A Policy-Guided Imitation Approach for Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/60380ee913d20e722368245f23e0d4baf52e139a",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "arxivId": "2207.00244",
            "authors": [
                {
                    "authorId": null,
                    "name": "Wenjia Zhang"
                },
                {
                    "authorId": "49507262",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "122919426",
                    "name": "Haoyi Niu"
                },
                {
                    "authorId": "2167325163",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "35834541",
                    "name": "Ming Li"
                },
                {
                    "authorId": "2153525920",
                    "name": "Heming Zhang"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                }
            ],
            "doi": "10.48550/arXiv.2207.00244",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "81b17aea4278a31cbf35c3f19b42ba856e22e934",
            "title": "Discriminator-Guided Model-Based Offline Imitation Learning",
            "url": "https://www.semanticscholar.org/paper/81b17aea4278a31cbf35c3f19b42ba856e22e934",
            "venue": "CoRL",
            "year": 2022
        },
        {
            "arxivId": "2206.13464",
            "authors": [
                {
                    "authorId": "122919426",
                    "name": "Haoyi Niu"
                },
                {
                    "authorId": "2034354321",
                    "name": "Shubham Sharma"
                },
                {
                    "authorId": "2114967084",
                    "name": "Yiwen Qiu"
                },
                {
                    "authorId": "35834541",
                    "name": "Ming Li"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2219045220",
                    "name": "Jianming Hu"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                }
            ],
            "doi": "10.48550/arXiv.2206.13464",
            "intent": [],
            "isInfluential": false,
            "paperId": "b3fe209a99d92a05d5d5dfbad7bb5ac72989ff39",
            "title": "When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/b3fe209a99d92a05d5d5dfbad7bb5ac72989ff39",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "arxivId": "2206.07989",
            "authors": [
                {
                    "authorId": "2008151131",
                    "name": "Jiafei Lyu"
                },
                {
                    "authorId": "2180539270",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "2265693",
                    "name": "Zongqing Lu"
                }
            ],
            "doi": "10.48550/arXiv.2206.07989",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "57899d8ecaec316ce3a3dc46b4c6fe80037664a0",
            "title": "Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination",
            "url": "https://www.semanticscholar.org/paper/57899d8ecaec316ce3a3dc46b4c6fe80037664a0",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "arxivId": "2111.11097",
            "authors": [
                {
                    "authorId": "2514974",
                    "name": "Christopher P. Diehl"
                },
                {
                    "authorId": "2037489529",
                    "name": "Timo Sievernich"
                },
                {
                    "authorId": "1379617041",
                    "name": "Martin Kr\u00fcger"
                },
                {
                    "authorId": "143762003",
                    "name": "F. Hoffmann"
                },
                {
                    "authorId": "145492266",
                    "name": "T. Bertram"
                }
            ],
            "doi": null,
            "intent": [
                "result"
            ],
            "isInfluential": false,
            "paperId": "68106f87835772052ab0c5ab2493af14c1f9aef4",
            "title": "UMBRELLA: Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging Planning",
            "url": "https://www.semanticscholar.org/paper/68106f87835772052ab0c5ab2493af14c1f9aef4",
            "venue": "ArXiv",
            "year": 2021
        },
        {
            "arxivId": "2107.09003",
            "authors": [
                {
                    "authorId": "49507262",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                },
                {
                    "authorId": "2144104090",
                    "name": "Xiangyu Zhu"
                }
            ],
            "doi": "10.1609/aaai.v36i8.20855",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "63572d61745c4c09709463d9c2df00bf9bf8e054",
            "title": "Constraints Penalized Q-Learning for Safe Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/63572d61745c4c09709463d9c2df00bf9bf8e054",
            "venue": "AAAI",
            "year": 2021
        },
        {
            "arxivId": "2102.11492",
            "authors": [
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                },
                {
                    "authorId": "49507262",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "49890778",
                    "name": "Yueying Zhang"
                },
                {
                    "authorId": "13772718",
                    "name": "Yusen Huo"
                },
                {
                    "authorId": "8362374",
                    "name": "Xiangyu Zhu"
                },
                {
                    "authorId": "1919918492",
                    "name": "Honglei Yin"
                },
                {
                    "authorId": "2149515044",
                    "name": "Yu Zheng"
                }
            ],
            "doi": "10.1609/aaai.v36i4.20393",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4e90ab5310055d1cc9a951a476ed241f97fe76b0",
            "title": "DeepThermal: Combustion Optimization for Thermal Power Generating Units Using Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/4e90ab5310055d1cc9a951a476ed241f97fe76b0",
            "venue": "AAAI",
            "year": 2021
        },
        {
            "arxivId": "2102.08363",
            "authors": [
                {
                    "authorId": "10909315",
                    "name": "Tianhe Yu"
                },
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "102801230",
                    "name": "Rafael Rafailov"
                },
                {
                    "authorId": "19275599",
                    "name": "A. Rajeswaran"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
            "title": "COMBO: Conservative Offline Model-Based Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/245682e8b3fa76f4a3e2991b5497577af95cbb3f",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2155570419",
                    "name": "Junjie Zhang"
                },
                {
                    "authorId": "2008151131",
                    "name": "Jiafei Lyu"
                },
                {
                    "authorId": "2125106047",
                    "name": "Xiaoteng Ma"
                },
                {
                    "authorId": "30411824",
                    "name": "Jiangpeng Yan"
                },
                {
                    "authorId": "2146157882",
                    "name": "Jun Yang"
                },
                {
                    "authorId": "2187301367",
                    "name": "Le Wan"
                },
                {
                    "authorId": "2116523082",
                    "name": "Xiu Li"
                }
            ],
            "doi": "10.48550/arXiv.2304.04660",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "3ed097d7351c32bb971a00d981281b5a9c28facd",
            "title": "Uncertainty-driven Trajectory Truncation for Model-based Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/3ed097d7351c32bb971a00d981281b5a9c28facd",
            "venue": "ArXiv",
            "year": 2023
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2247730541",
                    "name": "Jiankai Sun"
                },
                {
                    "authorId": "2290029044",
                    "name": "Yiqi Jiang"
                },
                {
                    "authorId": "2266388333",
                    "name": "Jianing Qiu"
                },
                {
                    "authorId": "2288577796",
                    "name": "Parth Nobel"
                },
                {
                    "authorId": "79262652",
                    "name": "Mykel J. Kochenderfer"
                },
                {
                    "authorId": "2243338895",
                    "name": "Mac Schwager"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "f01b7d88743a0833bc101b8b7e13730dbf49de2b",
            "title": "Conformal Prediction for Uncertainty-Aware Planning with Diffusion Dynamics Model",
            "url": "https://www.semanticscholar.org/paper/f01b7d88743a0833bc101b8b7e13730dbf49de2b",
            "venue": "NeurIPS",
            "year": 2023
        },
        {
            "arxivId": null,
            "authors": [],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "3f14defdaf32541312c1378ac4c1dcf1e685789b",
            "title": "Squeezing more value out of your historical data: data-augmented behavioural cloning as launchpad for reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/3f14defdaf32541312c1378ac4c1dcf1e685789b",
            "venue": "",
            "year": 2022
        },
        {
            "arxivId": null,
            "authors": [],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "6bd8ceca5c38521ee928bed68e07a6c277632d13",
            "title": "E NTROPY -R EGULARIZED M ODEL -B ASED O FFLINE R EINFORCEMENT L EARNING",
            "url": "https://www.semanticscholar.org/paper/6bd8ceca5c38521ee928bed68e07a6c277632d13",
            "venue": "",
            "year": null
        }
    ],
    "corpusId": 234742314,
    "doi": "10.24963/ijcai.2022/516",
    "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
    ],
    "influentialCitationCount": 3,
    "isOpenAccess": true,
    "isPublisherLicensed": true,
    "is_open_access": true,
    "is_publisher_licensed": true,
    "numCitedBy": 32,
    "numCiting": 34,
    "paperId": "25009d389885645c1210ae6382a4c8b2439230af",
    "references": [
        {
            "arxivId": "2110.07395",
            "authors": [
                {
                    "authorId": null,
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "3415564",
                    "name": "Xianyuan Zhan"
                },
                {
                    "authorId": "2136086524",
                    "name": "Jianxiong Li"
                },
                {
                    "authorId": "1919918492",
                    "name": "Honglei Yin"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "0407889beacd6add97aa7b13f4a98724acc9cbef",
            "title": "Offline Reinforcement Learning with Soft Behavior Regularization",
            "url": "https://www.semanticscholar.org/paper/0407889beacd6add97aa7b13f4a98724acc9cbef",
            "venue": "ArXiv",
            "year": 2021
        },
        {
            "arxivId": "2107.09003",
            "authors": [
                {
                    "authorId": "49507262",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                },
                {
                    "authorId": "2144104090",
                    "name": "Xiangyu Zhu"
                }
            ],
            "doi": "10.1609/aaai.v36i8.20855",
            "intent": [],
            "isInfluential": false,
            "paperId": "63572d61745c4c09709463d9c2df00bf9bf8e054",
            "title": "Constraints Penalized Q-Learning for Safe Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/63572d61745c4c09709463d9c2df00bf9bf8e054",
            "venue": "AAAI",
            "year": 2021
        },
        {
            "arxivId": "2106.06860",
            "authors": [
                {
                    "authorId": "14637819",
                    "name": "Scott Fujimoto"
                },
                {
                    "authorId": "2046135",
                    "name": "S. Gu"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "c879b25308026d6538e52b27bcf4fd3cb60855f3",
            "title": "A Minimalist Approach to Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/c879b25308026d6538e52b27bcf4fd3cb60855f3",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "arxivId": "2103.08050",
            "authors": [
                {
                    "authorId": "2000906",
                    "name": "Ilya Kostrikov"
                },
                {
                    "authorId": "2704494",
                    "name": "Jonathan Tompson"
                },
                {
                    "authorId": "2276554",
                    "name": "R. Fergus"
                },
                {
                    "authorId": "7624658",
                    "name": "Ofir Nachum"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "362cc80481b288874af0428107ab31e955dcf09f",
            "title": "Offline Reinforcement Learning with Fisher Divergence Critic Regularization",
            "url": "https://www.semanticscholar.org/paper/362cc80481b288874af0428107ab31e955dcf09f",
            "venue": "ICML",
            "year": 2021
        },
        {
            "arxivId": "2102.11492",
            "authors": [
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                },
                {
                    "authorId": "49507262",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "49890778",
                    "name": "Yueying Zhang"
                },
                {
                    "authorId": "13772718",
                    "name": "Yusen Huo"
                },
                {
                    "authorId": "8362374",
                    "name": "Xiangyu Zhu"
                },
                {
                    "authorId": "1919918492",
                    "name": "Honglei Yin"
                },
                {
                    "authorId": "2149515044",
                    "name": "Yu Zheng"
                }
            ],
            "doi": "10.1609/aaai.v36i4.20393",
            "intent": [],
            "isInfluential": false,
            "paperId": "4e90ab5310055d1cc9a951a476ed241f97fe76b0",
            "title": "DeepThermal: Combustion Optimization for Thermal Power Generating Units Using Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/4e90ab5310055d1cc9a951a476ed241f97fe76b0",
            "venue": "AAAI",
            "year": 2021
        },
        {
            "arxivId": "2009.06799",
            "authors": [
                {
                    "authorId": "47619311",
                    "name": "Jacob Buckman"
                },
                {
                    "authorId": "52382152",
                    "name": "Carles Gelada"
                },
                {
                    "authorId": "1792298",
                    "name": "Marc G. Bellemare"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "86fa0f350b0ede3a86e31aa2900af551531ee570",
            "title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/86fa0f350b0ede3a86e31aa2900af551531ee570",
            "venue": "ICLR",
            "year": 2020
        },
        {
            "arxivId": "2008.05556",
            "authors": [
                {
                    "authorId": "1877004999",
                    "name": "Arthur Argenson"
                },
                {
                    "authorId": "1387885286",
                    "name": "Gabriel Dulac-Arnold"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d",
            "title": "Model-Based Offline Planning",
            "url": "https://www.semanticscholar.org/paper/3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d",
            "venue": "ICLR",
            "year": 2020
        },
        {
            "arxivId": "2007.11091",
            "authors": [
                {
                    "authorId": "81419386",
                    "name": "Seyed Kamyar Seyed Ghasemipour"
                },
                {
                    "authorId": "50319359",
                    "name": "D. Schuurmans"
                },
                {
                    "authorId": "2046135",
                    "name": "S. Gu"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "fa7f88f77de02ae9389e514a1cd13083a624ec78",
            "title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL",
            "url": "https://www.semanticscholar.org/paper/fa7f88f77de02ae9389e514a1cd13083a624ec78",
            "venue": "ICML",
            "year": 2020
        },
        {
            "arxivId": "2006.04779",
            "authors": [
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "35499972",
                    "name": "Aurick Zhou"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
            "title": "Conservative Q-Learning for Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/28db20a81eec74a50204686c3cf796c42a020d2e",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2005.13239",
            "authors": [
                {
                    "authorId": "10909315",
                    "name": "Tianhe Yu"
                },
                {
                    "authorId": "8234443",
                    "name": "G. Thomas"
                },
                {
                    "authorId": "3469209",
                    "name": "Lantao Yu"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "145085305",
                    "name": "James Y. Zou"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                },
                {
                    "authorId": "1901958",
                    "name": "Tengyu Ma"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c",
            "title": "MOPO: Model-based Offline Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/dea0f1c5949f8d898b9b6ff68226a781558e413c",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2005.05951",
            "authors": [
                {
                    "authorId": "1891978",
                    "name": "Rahul Kidambi"
                },
                {
                    "authorId": "19275599",
                    "name": "A. Rajeswaran"
                },
                {
                    "authorId": "1751626",
                    "name": "Praneeth Netrapalli"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
            "title": "MOReL : Model-Based Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/309c2c5ee60e725244da09180f913cd8d4b8d4e9",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "arxivId": "2004.07219",
            "authors": [
                {
                    "authorId": "2550764",
                    "name": "Justin Fu"
                },
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "7624658",
                    "name": "Ofir Nachum"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4",
            "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/a326d9f2d2d351001fece788165dbcbb524da2e4",
            "venue": "ArXiv",
            "year": 2020
        },
        {
            "arxivId": "1911.11361",
            "authors": [
                {
                    "authorId": "2013680445",
                    "name": "Yifan Wu"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "7624658",
                    "name": "Ofir Nachum"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e",
            "title": "Behavior Regularized Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/9be492858863c8c7c24be1ecb75724de5086bd8e",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": "1909.11652",
            "authors": [
                {
                    "authorId": "3195183",
                    "name": "Anusha Nagabandi"
                },
                {
                    "authorId": "70162540",
                    "name": "K. Konolige"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "2109446216",
                    "name": "Vikash Kumar"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "7a450675968d31b8363e21fb5d5b72474c128076",
            "title": "Deep Dynamics Models for Learning Dexterous Manipulation",
            "url": "https://www.semanticscholar.org/paper/7a450675968d31b8363e21fb5d5b72474c128076",
            "venue": "CoRL",
            "year": 2019
        },
        {
            "arxivId": "1907.00456",
            "authors": [
                {
                    "authorId": "3106683",
                    "name": "Natasha Jaques"
                },
                {
                    "authorId": "2214185",
                    "name": "Asma Ghandeharioun"
                },
                {
                    "authorId": "40385687",
                    "name": "Judy Hanwen Shen"
                },
                {
                    "authorId": "145727063",
                    "name": "Craig Ferguson"
                },
                {
                    "authorId": "2677488",
                    "name": "\u00c0gata Lapedriza"
                },
                {
                    "authorId": "49522657",
                    "name": "Noah J. Jones"
                },
                {
                    "authorId": "2046135",
                    "name": "S. Gu"
                },
                {
                    "authorId": "1719389",
                    "name": "Rosalind W. Picard"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "57daffd65a5d73a439903f3e50950c21c9eba687",
            "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog",
            "url": "https://www.semanticscholar.org/paper/57daffd65a5d73a439903f3e50950c21c9eba687",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": "1906.08649",
            "authors": [
                {
                    "authorId": "3428549",
                    "name": "Tingwu Wang"
                },
                {
                    "authorId": "2503659",
                    "name": "Jimmy Ba"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "db1614b97aec1e0ead9554a038aa0f8dd9a26e30",
            "title": "Exploring Model-based Planning with Policy Networks",
            "url": "https://www.semanticscholar.org/paper/db1614b97aec1e0ead9554a038aa0f8dd9a26e30",
            "venue": "ICLR",
            "year": 2019
        },
        {
            "arxivId": "1906.08253",
            "authors": [
                {
                    "authorId": "35163402",
                    "name": "Michael Janner"
                },
                {
                    "authorId": "2550764",
                    "name": "Justin Fu"
                },
                {
                    "authorId": "2634261",
                    "name": "Marvin Zhang"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
            "title": "When to Trust Your Model: Model-Based Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/9001698e033524864d4d45f051a5ba362d4afd9e",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": "1906.00949",
            "authors": [
                {
                    "authorId": "1488785534",
                    "name": "Aviral Kumar"
                },
                {
                    "authorId": "2550764",
                    "name": "Justin Fu"
                },
                {
                    "authorId": "145499435",
                    "name": "G. Tucker"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
            "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction",
            "url": "https://www.semanticscholar.org/paper/82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": "1903.08738",
            "authors": [
                {
                    "authorId": "145000658",
                    "name": "Hoang Minh Le"
                },
                {
                    "authorId": "88658266",
                    "name": "Cameron Voloshin"
                },
                {
                    "authorId": "1740159",
                    "name": "Yisong Yue"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "3203be64597368d356b3ebca54a15699a3714e24",
            "title": "Batch Policy Learning under Constraints",
            "url": "https://www.semanticscholar.org/paper/3203be64597368d356b3ebca54a15699a3714e24",
            "venue": "ICML",
            "year": 2019
        },
        {
            "arxivId": "1812.02900",
            "authors": [
                {
                    "authorId": "14637819",
                    "name": "Scott Fujimoto"
                },
                {
                    "authorId": "2462512",
                    "name": "D. Meger"
                },
                {
                    "authorId": "144368601",
                    "name": "Doina Precup"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
            "title": "Off-Policy Deep Reinforcement Learning without Exploration",
            "url": "https://www.semanticscholar.org/paper/5285cb8faada5de8a92a47622950f6cfd476ac1d",
            "venue": "ICML",
            "year": 2018
        },
        {
            "arxivId": "1811.01848",
            "authors": [
                {
                    "authorId": "33557393",
                    "name": "Kendall Lowrey"
                },
                {
                    "authorId": "19275599",
                    "name": "A. Rajeswaran"
                },
                {
                    "authorId": "144695232",
                    "name": "S. Kakade"
                },
                {
                    "authorId": "144832491",
                    "name": "E. Todorov"
                },
                {
                    "authorId": "2316241382",
                    "name": "Igor Mordatch"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "6a9013a8cdd84e423223f76a903028011c84c4ab",
            "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
            "url": "https://www.semanticscholar.org/paper/6a9013a8cdd84e423223f76a903028011c84c4ab",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "arxivId": "1805.12114",
            "authors": [
                {
                    "authorId": "46224156",
                    "name": "Kurtland Chua"
                },
                {
                    "authorId": "35159852",
                    "name": "R. Calandra"
                },
                {
                    "authorId": "49686609",
                    "name": "R. McAllister"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
            "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
            "url": "https://www.semanticscholar.org/paper/56136aa0b2c347cbcf3d50821f310c4253155026",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "arxivId": "1801.01290",
            "authors": [
                {
                    "authorId": "2587648",
                    "name": "Tuomas Haarnoja"
                },
                {
                    "authorId": "35499972",
                    "name": "Aurick Zhou"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "811df72e210e20de99719539505da54762a11c6d",
            "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
            "url": "https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d",
            "venue": "ICML",
            "year": 2018
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "4337102",
                    "name": "Julian Schrittwieser"
                },
                {
                    "authorId": "34838386",
                    "name": "K. Simonyan"
                },
                {
                    "authorId": "2460849",
                    "name": "Ioannis Antonoglou"
                },
                {
                    "authorId": "1885349",
                    "name": "Aja Huang"
                },
                {
                    "authorId": "35099444",
                    "name": "A. Guez"
                },
                {
                    "authorId": "2067208983",
                    "name": "T. Hubert"
                },
                {
                    "authorId": "2067596385",
                    "name": "Lucas baker"
                },
                {
                    "authorId": "40227832",
                    "name": "Matthew Lai"
                },
                {
                    "authorId": "34848283",
                    "name": "Adrian Bolton"
                },
                {
                    "authorId": "2275897",
                    "name": "Yutian Chen"
                },
                {
                    "authorId": "2542999",
                    "name": "T. Lillicrap"
                },
                {
                    "authorId": "2059412590",
                    "name": "Fan Hui"
                },
                {
                    "authorId": "2175946",
                    "name": "L. Sifre"
                },
                {
                    "authorId": "47568983",
                    "name": "George van den Driessche"
                },
                {
                    "authorId": "1686971",
                    "name": "T. Graepel"
                },
                {
                    "authorId": "48987704",
                    "name": "D. Hassabis"
                }
            ],
            "doi": "10.1038/nature24270",
            "intent": [],
            "isInfluential": false,
            "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
            "title": "Mastering the game of Go without human knowledge",
            "url": "https://www.semanticscholar.org/paper/c27db32efa8137cbf654902f8f728f338e55cd1c",
            "venue": "Nature",
            "year": 2017
        },
        {
            "arxivId": "1709.10087",
            "authors": [
                {
                    "authorId": "19275599",
                    "name": "A. Rajeswaran"
                },
                {
                    "authorId": "2109446216",
                    "name": "Vikash Kumar"
                },
                {
                    "authorId": "2129458064",
                    "name": "Abhishek Gupta"
                },
                {
                    "authorId": "47971768",
                    "name": "John Schulman"
                },
                {
                    "authorId": "144832491",
                    "name": "E. Todorov"
                },
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                }
            ],
            "doi": "10.15607/RSS.2018.XIV.049",
            "intent": [],
            "isInfluential": false,
            "paperId": "e010ba3ff5744604cdbfe44a733e2a98649ee907",
            "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations",
            "url": "https://www.semanticscholar.org/paper/e010ba3ff5744604cdbfe44a733e2a98649ee907",
            "venue": "Robotics: Science and Systems",
            "year": 2017
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2284321",
                    "name": "Grady Williams"
                },
                {
                    "authorId": "39369130",
                    "name": "Andrew Aldrich"
                },
                {
                    "authorId": "1751063",
                    "name": "Evangelos A. Theodorou"
                }
            ],
            "doi": "10.2514/1.G001921",
            "intent": [],
            "isInfluential": false,
            "paperId": "ddc1d2830c5979bb07ca631fa711e1e384d5ddfa",
            "title": "Model Predictive Path Integral Control: From Theory to Parallel Computation",
            "url": "https://www.semanticscholar.org/paper/ddc1d2830c5979bb07ca631fa711e1e384d5ddfa",
            "venue": "",
            "year": 2017
        },
        {
            "arxivId": "1504.00702",
            "authors": [
                {
                    "authorId": "1736651",
                    "name": "S. Levine"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "doi": "10.5555/2946645.2946684",
            "intent": [],
            "isInfluential": false,
            "paperId": "b6b8a1b80891c96c28cc6340267b58186157e536",
            "title": "End-to-End Training of Deep Visuomotor Policies",
            "url": "https://www.semanticscholar.org/paper/b6b8a1b80891c96c28cc6340267b58186157e536",
            "venue": "J. Mach. Learn. Res.",
            "year": 2015
        },
        {
            "arxivId": "1502.03509",
            "authors": [
                {
                    "authorId": "39844381",
                    "name": "M. Germain"
                },
                {
                    "authorId": "144717963",
                    "name": "Karol Gregor"
                },
                {
                    "authorId": "145797336",
                    "name": "Iain Murray"
                },
                {
                    "authorId": "1777528",
                    "name": "H. Larochelle"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "90f72fbbe5f0a29e627db28999e01a30a9655bc6",
            "title": "MADE: Masked Autoencoder for Distribution Estimation",
            "url": "https://www.semanticscholar.org/paper/90f72fbbe5f0a29e627db28999e01a30a9655bc6",
            "venue": "ICML",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1929279",
                    "name": "Z. Botev"
                },
                {
                    "authorId": "1704242",
                    "name": "Dirk P. Kroese"
                },
                {
                    "authorId": "35064932",
                    "name": "R. Rubinstein"
                },
                {
                    "authorId": "115292111",
                    "name": "P. L'Ecuyer"
                }
            ],
            "doi": "10.1016/B978-0-444-53859-8.00003-5",
            "intent": [],
            "isInfluential": false,
            "paperId": "6b6b078ee9aabf3c17220a2da1e3f0dd822956b7",
            "title": "Chapter 3 \u2013 The Cross-Entropy Method for Optimization",
            "url": "https://www.semanticscholar.org/paper/6b6b078ee9aabf3c17220a2da1e3f0dd822956b7",
            "venue": "",
            "year": 2013
        },
        {
            "arxivId": "1312.5602",
            "authors": [
                {
                    "authorId": "3255983",
                    "name": "Volodymyr Mnih"
                },
                {
                    "authorId": "2645384",
                    "name": "K. Kavukcuoglu"
                },
                {
                    "authorId": "145824029",
                    "name": "David Silver"
                },
                {
                    "authorId": "1753223",
                    "name": "Alex Graves"
                },
                {
                    "authorId": "2460849",
                    "name": "Ioannis Antonoglou"
                },
                {
                    "authorId": "1688276",
                    "name": "Daan Wierstra"
                },
                {
                    "authorId": "3137672",
                    "name": "Martin A. Riedmiller"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "2319a491378867c7049b3da055c5df60e1671158",
            "title": "Playing Atari with Deep Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158",
            "venue": "ArXiv",
            "year": 2013
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "145590039",
                    "name": "Carlos E. Garcia"
                },
                {
                    "authorId": "2399538",
                    "name": "D. M. Prett"
                },
                {
                    "authorId": "144746954",
                    "name": "M. Morari"
                }
            ],
            "doi": "10.1016/0005-1098(89)90002-2",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "8c11def05d9701db9a78589e2289e12f8262b928",
            "title": "Model predictive control: Theory and practice - A survey",
            "url": "https://www.semanticscholar.org/paper/8c11def05d9701db9a78589e2289e12f8262b928",
            "venue": "Autom.",
            "year": 1989
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "71222706",
                    "name": "Yao Liu"
                },
                {
                    "authorId": "40261572",
                    "name": "Adith Swaminathan"
                },
                {
                    "authorId": "40333747",
                    "name": "Alekh Agarwal"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "2919e4ff0ab126e8bf00492f00ce7363288cb1df",
            "title": "Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration",
            "url": "https://www.semanticscholar.org/paper/2919e4ff0ab126e8bf00492f00ce7363288cb1df",
            "venue": "NeurIPS",
            "year": 2020
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Engineering",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Engineering",
            "source": "s2-fos-model"
        }
    ],
    "title": "Model-Based Offline Planning with Trajectory Pruning",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/25009d389885645c1210ae6382a4c8b2439230af",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2021
}