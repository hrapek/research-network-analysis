{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cheatsheet: paper datatype attributes\n",
    "\n",
    "- paper.title: Title of the paper\n",
    "- paper.authors: List of authors (each author has a .name attribute)\n",
    "- paper.summary: Abstract/summary of the paper\n",
    "- paper.published: Publication date\n",
    "- paper.updated: Last update date\n",
    "- paper.pdf_url: URL to download the PDF\n",
    "- paper.entry_id: arXiv ID\n",
    "- paper.primary_category: Main category\n",
    "- paper.categories: All categories the paper belongs to\n",
    "- paper.links: All related links\n",
    "- paper.comment: Author comments (if any)\n",
    "- paper.journal_ref: Journal reference (if published)\n",
    "- paper.doi: Digital Object Identifier (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install arxiv\n",
    "! pip install networkx\n",
    "! pip install PyPDF2\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "import arxiv\n",
    "import networkx as nx\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import PyPDF2\n",
    "import requests\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API call\n",
    "client = arxiv.Client()\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query =\"artificial intelligence\",\n",
    "    max_results = 100,\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build nx graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "results = client.results(search)\n",
    "\n",
    "for paper in results:\n",
    "  G.add_node(paper.entry_id, title=paper.title, authors=paper.authors, summary=paper.summary, url=paper.pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs():\n",
    "    # Create a directory for the PDFs if it doesn't exist\n",
    "    pdf_dir = \"arxiv_papers\"\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        os.makedirs(pdf_dir)\n",
    "\n",
    "    # Download PDFs for each node\n",
    "    for node in G.nodes():\n",
    "        pdf_url = G.nodes[node]['url']\n",
    "        # Extract paper ID from the URL to use as filename\n",
    "        paper_id = node.split('/')[-1]\n",
    "        pdf_path = os.path.join(pdf_dir, f\"{paper_id}.pdf\")\n",
    "        \n",
    "        # Download if file doesn't already exist\n",
    "        if not os.path.exists(pdf_path):\n",
    "            try:\n",
    "                response = requests.get(pdf_url)\n",
    "                if response.status_code == 200:\n",
    "                    with open(pdf_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"Downloaded: {paper_id}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {paper_id}: HTTP {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {paper_id}: {str(e)}\")\n",
    "\n",
    "#download_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# extract text from PDF using PyPDF2\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# find reference section\n",
    "def extract_references(text):\n",
    "    # Find all occurrences of the word \"References\" in the text (case insensitive)\n",
    "    matches = list(re.finditer(r'\\bReferences\\b', text, re.IGNORECASE))\n",
    "    \n",
    "    # If no matches are found, return None\n",
    "    if not matches:\n",
    "        return None\n",
    "    \n",
    "    # Get the last match\n",
    "    last_match = matches[-1]\n",
    "    \n",
    "    # Return all text that comes after the last occurrence of \"References\"\n",
    "    return text[last_match.end():]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin,\n",
      "M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur,\n",
      "M., Levenberg, J., Man´ e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\n",
      "Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi´ egas, F., Vinyals, O., Warden, P., Wattenberg,\n",
      "M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\n",
      "https://www.tensorflow.org/, software available from tensorflow.org\n",
      "2. Alhaija, H., Mustikovela, S., Mescheder, L., Geiger, A., Rother, C.: Augmented reality meets computer vision:\n",
      "Efficient data generation for urban driving scenes. International Journal of Computer Vision (IJCV) (2018)\n",
      "3. AnandTech: Arm reveals cortex-a72 architecture details (2015), https://www.anandtech.com/show/9184/\n",
      "arm-reveals-cortex-a72-architecture-details, 10.10.2022\n",
      "4. ARM: Learn the architecture - introducing neon. https://developer.arm.com/documentation/102474/0100/?lang=en\n",
      "(2020)\n",
      "5. Bai, J., Shi, W., Xiao, Z., Ali, T.A.A., Ye, F., Jiao, L.: Achieving better category separability for hyperspectral image\n",
      "classification: A spatial–spectral approach. IEEE Transactions on Neural Networks and Learning Systems (2023)\n",
      "6. Basterretxea, K., Mart´ ınez, V., Echanobe, J., Guti´ errez–Zaballa, J., Del Campo, I.: Hsi-drive: A dataset for the\n",
      "research of hyperspectral image processing applied to autonomous driving systems. In: 2021 IEEE Intelligent Vehicles\n",
      "Symposium (IV). pp. 866–873 (2021). https://doi.org/10.1109/IV48863.2021.9575298\n",
      "7. Borges, P., Peynot, T., Liang, S., Arain, B., Wildie, M., Minareci, M., Lichman, S., Samvedi, G., Sa, I., Hudson, N.,\n",
      "et al.: A survey on terrain traversability analysis for autonomous ground vehicles: Methods, sensors, and challenges.\n",
      "Field Robotics 2(1), 1567–1627 (2022)\n",
      "8. Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: A high-definition ground truth database.\n",
      "Pattern Recognition Letters xx(x), xx–xx (2008)\n",
      "9. Brostow, G.J., Shotton, J., Fauqueur, J., Cipolla, R.: Segmentation and recognition using structure from motion\n",
      "point clouds. In: ECCV (1). pp. 44–57 (2008)\n",
      "10. Cavigelli, L., Bernath, D., Magno, M., Benini, L.: Computationally efficient target classification in multispectral\n",
      "image data with deep neural networks. In: Target and Background Signatures II. vol. 9997, pp. 191–202. SPIE\n",
      "(2016)\n",
      "11. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with\n",
      "deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and\n",
      "machine intelligence 40(4), 834–848 (2017)\n",
      "12. Colomb, M., Duthon, P., Bernardin, F.: Spectral reflectance characterization of the road environment to optimize\n",
      "the choice of autonomous vehicle sensors. In: 2019 IEEE Intelligent Transportation Systems Conference (ITSC). pp.\n",
      "1085–1090. IEEE (2019)Title Suppressed Due to Excessive Length 35\n",
      "13. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The\n",
      "cityscapes dataset for semantic urban scene understanding. In: Proceedings of the IEEE conference on computer\n",
      "vision and pattern recognition. pp. 3213–3223 (2016)\n",
      "14. Courdier, E., Fleuret, F.: Real-time segmentation networks should be latency aware. In: Proceedings of the Asian\n",
      "Conference on Computer Vision (2020)\n",
      "15. Cui, X., Zheng, K., Gao, L., Zhang, B., Yang, D., Ren, J.: Multiscale spatial-spectral convolutional network with\n",
      "image-based framework for hyperspectral imagery classification. Remote Sensing 11(19), 2220 (2019)\n",
      "16. Du, Q., Yang, H.: Similarity-based unsupervised band selection for hyperspectral image analysis. IEEE geoscience\n",
      "and remote sensing letters 5(4), 564–568 (2008)\n",
      "17. Fan, M., Lai, S., Huang, J., Wei, X., Chai, Z., Luo, J., Wei, X.: Rethinking bisenet for real-time semantic segmen-\n",
      "tation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9716–9725\n",
      "(2021)\n",
      "18. Forestier, G., Inglada, J., Wemmert, C., Gan¸ carski, P.: Comparison of optical sensors discrimination ability using\n",
      "spectral libraries. International journal of remote sensing 34(7), 2327–2349 (2013)\n",
      "19. Fricker, G.A., Ventura, J.D., Wolf, J.A., North, M.P., Davis, F.W., Franklin, J.: A convolutional neural network\n",
      "classifier identifies tree species in mixed-conifer forest from hyperspectral imagery. Remote Sensing 11(19), 2326\n",
      "(2019)\n",
      "20. Govender, M., Chetty, K., Bulcock, H.: A review of hyperspectral remote sensing and its application in vegetation\n",
      "and water resource studies. Water Sa 33(2), 145–151 (2007)\n",
      "21. Guti´ errez-Zaballa, J., Basterretxea, K., Echanobe, J., Mart´ ınez, M.V., del Campo, I.: Exploring fully convolutional\n",
      "networks for the segmentation of hyperspectral imaging applied to advanced driver assistance systems. In: Design\n",
      "and Architecture for Signal and Image Processing: 15th International Workshop, DASIP 2022, Budapest, Hungary,\n",
      "June 20–22, 2022, Proceedings. p. 136–148. Springer-Verlag, Berlin, Heidelberg (2022). https://doi.org/10.1007/\n",
      "978-3-031-12748-9 11, https://doi.org/10.1007/978-3-031-12748-9 11\n",
      "22. Hanhirova, J., K¨ am¨ ar¨ ainen, T., Sepp¨ al¨ a, S., Siekkinen, M., Hirvisalo, V., Yl¨ a-J¨ a¨ aski, A.: Latency and throughput\n",
      "characterization of convolutional neural networks for mobile computer vision. In: Proceedings of the 9th ACM\n",
      "Multimedia Systems Conference. pp. 204–215 (2018)\n",
      "23. Herweg, J., Kerekes, J., Eismann, M.: Separability between pedestrians in hyperspectral imagery. Appl. Opt.\n",
      "52(6), 1330–1338 (Feb 2013). https://doi.org/10.1364/AO.52.001330, https://opg.optica.org/ao/abstract.cfm?URI=\n",
      "ao-52-6-1330\n",
      "24. Holly, S., Wendt, A., Lechner, M.: Profiling energy consumption of deep neural networks on nvidia jetson nano. In:\n",
      "2020 11th International Green and Sustainable Computing Workshops (IGSC). pp. 1–6. IEEE (2020)\n",
      "25. Huang, Y., Shen, Q., Fu, Y., You, S.: Weakly-supervised semantic segmentation in cityscape via hyperspectral image.\n",
      "In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1117–1126 (2021)\n",
      "26. Infineon: Usb005 user guide. https://www.infineon.com/cms/en/product/power/dc-dc-converters/\n",
      "digital-multiphase-controllers/gang-programmers/usb005/ (2014)\n",
      "27. Infineon: Irps5401 pmic flexible power management unit. https://www.infineon.com/dgdl/\n",
      "Infineon-IRPS5401M-DataSheet-v02 06-EN.pdf?fileId=5546d4625cc9456a015cd69d402139db (2021)\n",
      "28. Iqbal, H.: Harisiqbal88/plotneuralnet v1. 0.0. URL: https://doi. org/10.5281/Zenodo (2018)\n",
      "29. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and\n",
      "training of neural networks for efficient integer-arithmetic-only inference. In: Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition. pp. 2704–2713 (2018)\n",
      "30. Kim, B., Yim, J., Kim, J.: Highway driving dataset for semantic video segmentation. arXiv preprint arXiv:2011.00674\n",
      "(2020)\n",
      "31. Li, P., Dong, X., Yu, X., Yang, Y.: When humans meet machines: Towards efficient segmentation networks. In: The\n",
      "31st British Machine Vision Virtual Conference (2020)\n",
      "32. Liyanage, D.C., Hudjakov, R., Tamre, M.: Hyperspectral imaging methods improve rgb image semantic segmentation\n",
      "of unstructured terrains. In: 2020 International Conference Mechatronic Systems and Materials (MSM). pp. 1–5\n",
      "(2020). https://doi.org/10.1109/MSM49833.2020.9201738\n",
      "33. Lu, J., Liu, H., Yao, Y., Tao, S., Tang, Z., Lu, J.: Hsi road: A hyper spectral image dataset for road segmentation.\n",
      "In: 2020 IEEE International Conference on Multimedia and Expo (ICME). pp. 1–6 (2020). https://doi.org/10.1109/\n",
      "ICME46284.2020.9102890\n",
      "34. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research 9(11) (2008)\n",
      "35. Malivenko, G.: onnx2keras 0.0.24. https://pypi.org/project/onnx2keras/ (2021)\n",
      "36. MATLAB: Deep learning toolbox. https://es.mathworks.com/products/deep-learning.html (2022)36 J. Guti´ errez-Zaballa et al.\n",
      "37. NVIDIA: Nvidia jetson nano system-on-module. https://developer.download.nvidia.\n",
      "com/assets/embedded/secure/jetson/Nano/docs/JetsonNano DataSheet DS09366001v1.1.pdf?\n",
      "MpmY78SZFEFH-jJG2aGI3bYEw75LZBVZJnkH7Bx-4huO6qS6b4u-9T33O-jPr9XU3MLlbBnF\n",
      "lfY5GTCVIn7wUCwW5pHfZjouudAa9FcSOHi1CB0qyQ40PzYgABdhZ9d-smLolN-UV67\n",
      "e8KvMzRjSMahO2ebQ8RZOxboLa GZcNQUaMtBGVa94dW67a7g&t=eyJscyI6ImdzZW8iLCJsc2QiOiJodHRwczpcL1wvd3d3Lmdvb2dsZS5jb21cLyJ9\n",
      "(2022)\n",
      "38. NVIDIA: Nvidia tensorrt. https://developer.nvidia.com/tensorrt (2022)\n",
      "39. OpenMP: About us. https://www.openmp.org/about/about-us/ (2022)\n",
      "40. Orsic, M., Kreso, I., Bevandic, P., Segvic, S.: In defense of pre-trained imagenet architectures for real-time semantic\n",
      "segmentation of road-driving images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition. pp. 12607–12616 (2019)\n",
      "41. Photonfocus: MV1-D2048x1088-HS02-96-G2. https://www.photonfocus.com/products/camerafinder/camera/\n",
      "mv1-d2048x1088-hs02-96-g2\n",
      "42. Pi, R.: Raspberry pi 4 model b datasheet. https://datasheets.raspberrypi.com/rpi4/raspberry-pi-4-datasheet.pdf\n",
      "(2019)\n",
      "43. Pinchon, N., Cassignol, O., Nicolas, A., Bernardin, F., Leduc, P., Tarel, J.P., Br´ emond, R., Bercier, E., Brunet, J.:\n",
      "All-weather vision for automotive safety: which spectral band? In: International Forum on Advanced Microsystems\n",
      "for Automotive Applications. pp. 3–15. Springer (2018)\n",
      "44. Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. In:\n",
      "International Conference on Medical image computing and computer-assisted intervention. pp. 234–241. Springer\n",
      "(2015)\n",
      "45. Seidlitz, S., Sellner, J., Odenthal, J., ¨Ozdemir, B., Studier-Fischer, A., Kn¨ odler, S., Ayala, L., Adler, T.J., Kenngott,\n",
      "H.G., Tizabi, M., et al.: Robust deep learning-based semantic organ segmentation in hyperspectral images. Medical\n",
      "Image Analysis p. 102488 (2022)\n",
      "46. Son, G.J., Kwak, D.H., Park, M.K., Kim, Y.D., Jung, H.C.: U-net-based foreign object detection method using\n",
      "effective image acquisition system: A case of almond and green onion flake food process. Sustainability 13(24),\n",
      "13834 (2021)\n",
      "47. Taghizadeh, M., Gowen, A.A., O’Donnell, C.P.: Comparison of hyperspectral imaging with conventional rgb imaging\n",
      "for quality evaluation of agaricus bisporus mushrooms. Biosystems engineering 108(2), 191–194 (2011)\n",
      "48. Tensorflow: Xnnpack backend for tensorflow lite. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/\n",
      "lite/delegates/xnnpack (2013)\n",
      "49. Tools, K.: Usb digital meter, usb-a and usb-c. https://www.kleintools.com/catalog/usb-digital-meters/\n",
      "usb-digital-meter-usb-and-usb-c (2021)\n",
      "50. Vehicle, A..A.: Terranet showcases motion awareness system with 20ms re-\n",
      "action time (2021), https://www.autonomousvehicleinternational.com/news/adas/\n",
      "terranet-showcases-motion-awareness-system-with-20ms-reaction-time.html, 21.10.2022\n",
      "51. Wang, S., Chen, W., Xie, S.M., Azzari, G., Lobell, D.B.: Weakly supervised deep learning for segmentation of remote\n",
      "sensing imagery. Remote Sensing 12(2), 207 (2020)\n",
      "52. Weikl, K., Schroeder, D., Stechele, W.: Potentials of combined visible light and near infrared imaging for driving\n",
      "automation. In: Electronic Imaging Conference (2022)\n",
      "53. Winkens, C., Adams, V., Paulus, D.: Automatic shadow detection using hyperspectral data for terrain classification.\n",
      "Electronic Imaging pp. 31–1–31.6 (2019)\n",
      "54. Winkens, C., Kobelt, V., Paulus, D.: Robust features for snapshot hyperspectral terrain classification. In: 17th Int.\n",
      "Conf. Computer Analysis of Images and Patterns. pp. 16–27 (2017)\n",
      "55. Winkens, C., Paulus, D.: Context aware hyperspectral scene analysis. In: Int. Symp. Electronic Imaging Science and\n",
      "Technology. pp. 346.1–346.7 (2018)\n",
      "56. Winkens, C., Sattler, F., Paulus, D.: Deep dimension reduction for spatial-spectral road scene classification. Electronic\n",
      "Imaging pp. 49.1–49.9 (2019)\n",
      "57. Xilinx: Hw-z1-zcu104 evaluation board (xczu7ev-2ffvc1156) schematic. https://www.xilinx.com/products/\n",
      "boards-and-kits/zcu104.html#resources (2018)\n",
      "58. Xilinx: Zcu104 evaluation board. https://www.xilinx.com/support/documents/boards andkits/zcu104/\n",
      "ug1267-zcu104-eval-bd.pdf (2018)\n",
      "59. Xilinx: Quantizing the model. https://www.xilinx.com/html docs/vitis ai/1 4/quantize.html#uim1570695919827\n",
      "(2021)\n",
      "60. Xilinx: Dpuczdx8g for zynq ultrascale+ mpsocs product guide (pg338). https://docs.xilinx.com/r/en-US/pg338-dpu?\n",
      "tocId=Bd4R4bhnWgMYE6wUISXDLw (2022)\n"
     ]
    }
   ],
   "source": [
    "# Try out references extraction for first paper\n",
    "pdf_path = \"/Users/beatweichsler/Documents/1_Uni/Master/Semester3/SocialGraphs/FinalProject/repo1/arxiv_papers/2411.19274v1.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "references = extract_references(text)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TITLE EXTRACTION APPROACH 1\n",
    "\n",
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TITLE EXTRACTION APPROACH 1: NLTK\n",
    "\n",
    "# extract paper titles from reference section\n",
    "def extract_titles_nltk(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Split text into lines to process reference entries individually\n",
    "    lines = text.split('\\n')\n",
    "    titles = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "            \n",
    "        # Pattern 1: Look for titles after year in brackets\n",
    "        year_match = re.search(r'\\((?:19|20)\\d{2}\\)\\s*[.,]?\\s*([^.]+)', line)\n",
    "        if year_match:\n",
    "            title = year_match.group(1).strip()\n",
    "            if len(title) > 10:  # Minimum length to avoid fragments\n",
    "                titles.append(title)\n",
    "            continue\n",
    "            \n",
    "        # Pattern 2: Look for titles after colon (common in citation formats)\n",
    "        colon_match = re.search(r':\\s*([^.]+)', line)\n",
    "        if colon_match:\n",
    "            title = colon_match.group(1).strip()\n",
    "            if len(title) > 10:  # Minimum length to avoid fragments\n",
    "                titles.append(title)\n",
    "            continue\n",
    "        \n",
    "        # Pattern 3: Look for quotes (often containing titles)\n",
    "        quote_match = re.search(r'\"([^\"]+)\"', line)\n",
    "        if quote_match:\n",
    "            title = quote_match.group(1).strip()\n",
    "            if len(title) > 10:  # Minimum length to avoid fragments\n",
    "                titles.append(title)\n",
    "            continue\n",
    "        \n",
    "        # Pattern 4: Look for titles after a period and authors\n",
    "        # This assumes authors are typically listed first\n",
    "        period_match = re.search(r'\\.\\s+([^.]+?)\\s*(?:\\(|$)', line)\n",
    "        if period_match:\n",
    "            title = period_match.group(1).strip()\n",
    "            if len(title) > 10 and not re.search(r'^(19|20)\\d{2}', title):  # Avoid years\n",
    "                titles.append(title)\n",
    "    \n",
    "    # Clean up titles\n",
    "    cleaned_titles = []\n",
    "    for title in titles:\n",
    "        # Remove common prefixes like \"titled\", \"called\", etc.\n",
    "        title = re.sub(r'^(titled|called|entitled|on|in|the)\\s+', '', title, flags=re.IGNORECASE)\n",
    "        # Remove trailing separators and whitespace\n",
    "        title = re.sub(r'[,;:]$', '', title.strip())\n",
    "        if title and len(title) > 10:  # Final length check\n",
    "            cleaned_titles.append(title)\n",
    "    \n",
    "    return cleaned_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Augmented reality meets computer vision', 'International Journal of Computer Vision', 'https://www', 'Learn the architecture - introducing neon', 'Achieving better category separability for hyperspectral image', 'A spatial–spectral approach', 'Hsi-drive: A dataset for the', '2021 IEEE Intelligent Vehicles', 'https://doi', 'A survey on terrain traversability analysis for autonomous ground vehicles: Methods, sensors, and challenges', 'Semantic object classes in video: A high-definition ground truth database', 'Segmentation and recognition using structure from motion', 'Computationally efficient target classification in multispectral', 'Target and Background Signatures II', 'Deeplab: Semantic image segmentation with', 'IEEE transactions on pattern analysis and', 'Spectral reflectance characterization of the road environment to optimize', '2019 IEEE Intelligent Transportation Systems Conference (ITSC)', 'Title Suppressed Due to Excessive Length 35', 'Proceedings of the IEEE conference on computer', 'Real-time segmentation networks should be latency aware', 'Multiscale spatial-spectral convolutional network with', 'Remote Sensing 11', 'Similarity-based unsupervised band selection for hyperspectral image analysis', 'Rethinking bisenet for real-time semantic segmen-', 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', 'Comparison of optical sensors discrimination ability using', 'International journal of remote sensing 34', 'A convolutional neural network', 'Remote Sensing 11', 'A review of hyperspectral remote sensing and its application in vegetation', 'Water Sa 33', 'Exploring fully convolutional', '15th International Workshop, DASIP 2022, Budapest, Hungary', 'https://doi', 'Latency and throughput', 'Proceedings of the 9th ACM', 'Separability between pedestrians in hyperspectral imagery', 'Profiling energy consumption of deep neural networks on nvidia jetson nano', 'Weakly-supervised semantic segmentation in cityscape via hyperspectral image', 'Proceedings of the IEEE/CVF International Conference on Computer Vision', 'Usb005 user guide', 'Irps5401 pmic flexible power management unit', 'Harisiqbal88/plotneuralnet v1', 'Quantization and', 'Proceedings of the IEEE conference on', 'Highway driving dataset for semantic video segmentation', 'When humans meet machines: Towards efficient segmentation networks', 'Hyperspectral imaging methods improve rgb image semantic segmentation', '2020 International Conference Mechatronic Systems and Materials (MSM)', 'https://doi', 'Hsi road: A hyper spectral image dataset for road segmentation', 'https://doi', 'Visualizing data using t-sne', 'onnx2keras 0', 'Nvidia jetson nano system-on-module', 'Nvidia tensorrt', 'defense of pre-trained imagenet architectures for real-time semantic', 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern', '12607–12616', 'MV1-D2048x1088-HS02-96-G2', 'Raspberry pi 4 model b datasheet', 'which spectral band? In: International Forum on Advanced Microsystems', 'U-Net: Convolutional networks for biomedical image segmentation', 'Robust deep learning-based semantic organ segmentation in hyperspectral images', 'U-net-based foreign object detection method using', 'A case of almond and green onion flake food process', 'Comparison of hyperspectral imaging with conventional rgb imaging', 'Biosystems engineering 108', 'Xnnpack backend for tensorflow lite', 'Usb digital meter, usb-a and usb-c', 'Terranet showcases motion awareness system with 20ms re-', 'https://www', 'Weakly supervised deep learning for segmentation of remote', 'Remote Sensing 12', 'Potentials of combined visible light and near infrared imaging for driving', 'Electronic Imaging Conference (2022)', 'Automatic shadow detection using hyperspectral data for terrain classification', 'Robust features for snapshot hyperspectral terrain classification', 'Context aware hyperspectral scene analysis', 'Deep dimension reduction for spatial-spectral road scene classification', 'Hw-z1-zcu104 evaluation board (xczu7ev-2ffvc1156) schematic', 'Zcu104 evaluation board', 'Quantizing the model', 'Dpuczdx8g for zynq ultrascale+ mpsocs product guide (pg338)']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk_titles = extract_titles_nltk(references)\n",
    "print(nltk_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TITLE EXTRACTION APPROACH 2\n",
    "\n",
    "running llama model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_titles_with_llm(references_text):\n",
    "    # Initialize the pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"meta-llama/Llama-2-7b-chat-hf\",  # or another suitable model\n",
    "        token=True  # You'll need a HuggingFace token\n",
    "    )\n",
    "    \n",
    "    # Craft the prompt\n",
    "    prompt = \"\"\"Extract the titles of the academic papers from the following references section. \n",
    "    Output only the titles, one per line.\n",
    "    \n",
    "    References:\n",
    "    {references}\n",
    "    \n",
    "    Titles:\"\"\".format(references=references_text[:2000])  # Limit input length\n",
    "    \n",
    "    # Generate response\n",
    "    response = pipe(\n",
    "        prompt,\n",
    "        max_length=2048,\n",
    "        temperature=0.1,  # Keep it focused\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    titles = response[0]['generated_text'].split('\\n')\n",
    "    # Clean up titles (remove empty lines and any artifacts)\n",
    "    titles = [t.strip() for t in titles if t.strip() and len(t.strip()) > 10]\n",
    "    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d55af3fa1843c3b08df638f9f7bb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc9494d588b463b864abbe39ff7b008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb0c13c424c45b18a7170dc0f330bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18226bde2b484a6099858270c767fd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm_titles \u001b[38;5;241m=\u001b[39m extract_titles_with_llm(references)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm_titles)\n",
      "Cell \u001b[0;32mIn[64], line 6\u001b[0m, in \u001b[0;36mextract_titles_with_llm\u001b[0;34m(references_text)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_titles_with_llm\u001b[39m(references_text):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Initialize the pipeline\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# or another suitable model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# You'll need a HuggingFace token\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Craft the prompt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mExtract the titles of the academic papers from the following references section. \u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m    Output only the titles, one per line.\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m    Titles:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(references\u001b[38;5;241m=\u001b[39mreferences_text[:\u001b[38;5;241m2000\u001b[39m])  \u001b[38;5;66;03m# Limit input length\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:896\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 896\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    897\u001b[0m         model,\n\u001b[1;32m    898\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[1;32m    899\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    900\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[1;32m    901\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    904\u001b[0m     )\n\u001b[1;32m    906\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    907\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:288\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    290\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3769\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3768\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3769\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   3770\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3771\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3772\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   3773\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   3774\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   3775\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   3776\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   3777\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   3778\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   3779\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   3780\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   3781\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   3782\u001b[0m     )\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3785\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3786\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3787\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3788\u001b[0m ):\n\u001b[1;32m   3789\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   1099\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1100\u001b[0m             shard_filename,\n\u001b[1;32m   1101\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1102\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1103\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1104\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   1105\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1106\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1107\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   1108\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1109\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   1110\u001b[0m             _commit_hash\u001b[38;5;241m=\u001b[39m_commit_hash,\n\u001b[1;32m   1111\u001b[0m         )\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    404\u001b[0m         path_or_repo_id,\n\u001b[1;32m    405\u001b[0m         filename,\n\u001b[1;32m    406\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    407\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    408\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    409\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    410\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    411\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    412\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    413\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    414\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    415\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1235\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1237\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1238\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1239\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1240\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1242\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1243\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1244\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1245\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1248\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1249\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1381\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1379\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1381\u001b[0m     _download_to_tmp_and_move(\n\u001b[1;32m   1382\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1383\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[1;32m   1384\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[1;32m   1385\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1386\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1387\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1388\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1389\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1390\u001b[0m     )\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1392\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1915\u001b[0m     http_get(\n\u001b[1;32m   1916\u001b[0m         url_to_download,\n\u001b[1;32m   1917\u001b[0m         f,\n\u001b[1;32m   1918\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1919\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[1;32m   1920\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1921\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1922\u001b[0m     )\n\u001b[1;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm_titles = extract_titles_with_llm(references)\n",
    "print(llm_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TITLE EXTRACTION APPROACH 3\n",
    "\n",
    "calling llama model from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_titles_with_llm_api(references_text):\n",
    "    # API configuration for Llama-2\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b-chat-hf\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 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\"\n",
    "    }\n",
    "    \n",
    "    # Craft the prompt using Llama-2's instruction format\n",
    "    prompt = f\"\"\"[INST] You are a helpful assistant that extracts paper titles from academic references. Given the following references section, list ONLY the titles of the papers. Output one title per line. Do not include any other information like authors, years, or numbers.\n",
    "\n",
    "    References:\n",
    "    {references_text[:3900]}  \n",
    "\n",
    "    [/INST]\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            API_URL,\n",
    "            headers=headers,\n",
    "            json={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"max_length\": 4096,  # Increased max_length\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"top_p\": 0.95\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse response\n",
    "        output = response.json()[0]['generated_text']\n",
    "        titles = [\n",
    "            title.strip() \n",
    "            for title in output.split('\\n') \n",
    "            if title.strip() and len(title.strip()) > 10\n",
    "        ]\n",
    "        \n",
    "        return titles\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making API call: {e}\")\n",
    "        return []\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Error parsing API response: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error making API call: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b-chat-hf\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "titles = extract_titles_with_llm_api(references)\n",
    "print(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin,\n",
      "M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur,\n",
      "M., Levenberg, J., Man´ e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\n",
      "Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi´ egas, F., Vinyals, O., Warden, P., Wattenberg,\n",
      "M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\n",
      "https://www.tensorflow.org/, software available from tensorflow.org\n",
      "2. Alhaija, H., Mustikovela, S., Mescheder, L., Geiger, A., Rother, C.: Augmented reality meets computer vision:\n",
      "Efficient data generation for urban driving scenes. International Journal of Computer Vision (IJCV) (2018)\n",
      "3. AnandTech: Arm reveals cortex-a72 architecture details (2015), https://www.anandtech.com/show/9184/\n",
      "arm-reveals-cortex-a72-architecture-details, 10.10.2022\n",
      "4. ARM: Learn the architecture - introducing neon. https://developer.arm.com/documentation/102474/0100/?lang=en\n",
      "(2020)\n",
      "5. Bai, J., Shi, W., Xiao, Z., Ali, T.A.A., Ye, F., Jiao, L.: Achieving better category separability for hyperspectral image\n",
      "classification: A spatial–spectral approach. IEEE Transactions on Neural Networks and Learning Systems (2023)\n",
      "6. Basterretxea, K., Mart´ ınez, V., Echanobe, J., Guti´ errez–Zaballa, J., Del Campo, I.: Hsi-drive: A dataset for the\n",
      "research of hyperspectral image processing applied to autonomous driving systems. In: 2021 IEEE Intelligent Vehicles\n",
      "Symposium (IV). pp. 866–873 (2021). https://doi.org/10.1109/IV48863.2021.9575298\n",
      "7. Borges, P., Peynot, T., Liang, S., Arain, B., Wildie, M., Minareci, M., Lichman, S., Samvedi, G., Sa, I., Hudson, N.,\n",
      "et al.: A survey on terrain traversability analysis for autonomous ground vehicles: Methods, sensors, and challenges.\n",
      "Field Robotics 2(1), 1567–1627 (2022)\n",
      "8. Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: A high-definition ground truth database.\n",
      "Pattern Recognition Letters xx(x), xx–xx (2008)\n",
      "9. Brostow, G.J., Shotton, J., Fauqueur, J., Cipolla, R.: Segmentation and recognition using structure from motion\n",
      "point clouds. In: ECCV (1). pp. 44–57 (2008)\n",
      "10. Cavigelli, L., Bernath, D., Magno, M., Benini, L.: Computationally efficient target classification in multispectral\n",
      "image data with deep neural networks. In: Target and Background Signatures II. vol. 9997, pp. 191–202. SPIE\n",
      "(2016)\n",
      "11. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with\n",
      "deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and\n",
      "machine intelligence 40(4), 834–848 (2017)\n",
      "12. Colomb, M., Duthon, P., Bernardin, F.: Spectral reflectance characterization of the road environment to optimize\n",
      "the choice of autonomous vehicle sensors. In: 2019 IEEE Intelligent Transportation Systems Conference (ITSC). pp.\n",
      "1085–1090. IEEE (2019)Title Suppressed Due to Excessive Length 35\n",
      "13. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The\n",
      "cityscapes dataset for semantic urban scene understanding. In: Proceedings of the IEEE conference on computer\n",
      "vision and pattern recognition. pp. 3213–3223 (2016)\n",
      "14. Courdier, E., Fleuret, F.: Real-time segmentation networks should be latency aware. In: Proceedings of the Asian\n",
      "Conference on Computer Vision (2020)\n",
      "15. Cui, X., Zheng, K., Gao, L., Zhang, B., Yang, D., Ren, J.: Multiscale spatial-spectral convolutional network with\n",
      "image-based framework for hyperspectral imagery classification. Remote Sensing 11(19), 2220 (2019)\n",
      "16. Du, Q., Yang, H.: Similarity-based unsupervised band selection for hyperspectral image analysis. IEEE geoscience\n",
      "and remote sensing letters 5(4), 564–568 (2008)\n",
      "17. Fan, M., Lai, S., Huang, J., Wei, X., Chai, Z., Luo, J., Wei, X.: Rethinking bisenet for real-time semantic segmen-\n",
      "tation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9716–9725\n",
      "(2021)\n",
      "18. Forestier, G., Inglada, J., Wemmert, C., Gan¸ carski, P.: Comparison of optical sensors discrimination ability using\n",
      "spectral libraries. International journal of remote sensing 34(7), 2327–2349 (2013)\n",
      "19. Fricker, G.A., Ventura, J.D., Wolf, J.A., North, M.P., Davis, F.W., Franklin, J.: A convolutional neural network\n",
      "classifier identifies tree species in mixed-conifer forest from hyperspectral imagery. Remote Sensing 11(19), 2326\n",
      "(2019)\n",
      "20. Govender, M., Chetty, K., Bulcock, H.: A review of hyperspectral remote sensing and its application in vegetation\n",
      "and water resource studies. Water Sa 33(2), 145–151 (2007)\n",
      "21. Guti´ errez-Zaballa, J., Basterretxea, K., Echanobe, J., Mart´ ınez, M.V., del Campo, I.: Exploring fully convolutional\n",
      "networks for the segmentation of hyperspectral imaging applied to advanced driver assistance systems. In: Design\n",
      "and Architecture for Signal and Image Processing: 15th International Workshop, DASIP 2022, Budapest, Hungary,\n",
      "June 20–22, 2022, Proceedings. p. 136–148. Springer-Verlag, Berlin, Heidelberg (2022). https://doi.org/10.1007/\n",
      "978-3-031-12748-9 11, https://doi.org/10.1007/978-3-031-12748-9 11\n",
      "22. Hanhirova, J., K¨ am¨ ar¨ ainen, T., Sepp¨ al¨ a, S., Siekkinen, M., Hirvisalo, V., Yl¨ a-J¨ a¨ aski, A.: Latency and throughput\n",
      "characterization of convolutional neural networks for mobile computer vision. In: Proceedings of the 9th ACM\n",
      "Multimedia Systems Conference. pp. 204–215 (2018)\n",
      "23. Herweg, J., Kerekes, J., Eismann, M.: Separability between pedestrians in hyperspectral imagery. Appl. Opt.\n",
      "52(6), 1330–1338 (Feb 2013). https://doi.org/10.1364/AO.52.001330, https://opg.optica.org/ao/abstract.cfm?URI=\n",
      "ao-52-6-1330\n",
      "24. Holly, S., Wendt, A., Lechner, M.: Profiling energy consumption of deep neural networks on nvidia jetson nano. In:\n",
      "2020 11th International Green and Sustainable Computing Workshops (IGSC). pp. 1–6. IEEE (2020)\n",
      "25. Huang, Y., Shen, Q., Fu, Y., You, S.: Weakly-supervised semantic segmentation in cityscape via hyperspectral image.\n",
      "In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1117–1126 (2021)\n",
      "26. Infineon: Usb005 user guide. https://www.infineon.com/cms/en/product/power/dc-dc-converters/\n",
      "digital-multiphase-controllers/gang-programmers/usb005/ (2014)\n",
      "27. Infineon: Irps5401 pmic flexible power management unit. https://www.infineon.com/dgdl/\n",
      "Infineon-IRPS5401M-DataSheet-v02 06-EN.pdf?fileId=5546d4625cc9456a015cd69d402139db (2021)\n",
      "28. Iqbal, H.: Harisiqbal88/plotneuralnet v1. 0.0. URL: https://doi. org/10.5281/Zenodo (2018)\n",
      "29. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and\n",
      "training of neural networks for efficient integer-arithmetic-only inference. In: Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition. pp. 2704–2713 (2018)\n",
      "30. Kim, B., Yim, J., Kim, J.: Highway driving dataset for semantic video segmentation. arXiv preprint arXiv:2011.00674\n",
      "(2020)\n",
      "31. Li, P., Dong, X., Yu, X., Yang, Y.: When humans meet machines: Towards efficient segmentation networks. In: The\n",
      "31st British Machine Vision Virtual Conference (2020)\n",
      "32. Liyanage, D.C., Hudjakov, R., Tamre, M.: Hyperspectral imaging methods improve rgb image semantic segmentation\n",
      "of unstructured terrains. In: 2020 International Conference Mechatronic Systems and Materials (MSM). pp. 1–5\n",
      "(2020). https://doi.org/10.1109/MSM49833.2020.9201738\n",
      "33. Lu, J., Liu, H., Yao, Y., Tao, S., Tang, Z., Lu, J.: Hsi road: A hyper spectral image dataset for road segmentation.\n",
      "In: 2020 IEEE International Conference on Multimedia and Expo (ICME). pp. 1–6 (2020). https://doi.org/10.1109/\n",
      "ICME46284.2020.9102890\n",
      "34. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research 9(11) (2008)\n",
      "35. Malivenko, G.: onnx2keras 0.0.24. https://pypi.org/project/onnx2keras/ (2021)\n",
      "36. MATLAB: Deep learning toolbox. https://es.mathworks.com/products/deep-learning.html (2022)36 J. Guti´ errez-Zaballa et al.\n",
      "37. NVIDIA: Nvidia jetson nano system-on-module. https://developer.download.nvidia.\n",
      "com/assets/embedded/secure/jetson/Nano/docs/JetsonNano DataSheet DS09366001v1.1.pdf?\n",
      "MpmY78SZFEFH-jJG2aGI3bYEw75LZBVZJnkH7Bx-4huO6qS6b4u-9T33O-jPr9XU3MLlbBnF\n",
      "lfY5GTCVIn7wUCwW5pHfZjouudAa9FcSOHi1CB0qyQ40PzYgABdhZ9d-smLolN-UV67\n",
      "e8KvMzRjSMahO2ebQ8RZOxboLa GZcNQUaMtBGVa94dW67a7g&t=eyJscyI6ImdzZW8iLCJsc2QiOiJodHRwczpcL1wvd3d3Lmdvb2dsZS5jb21cLyJ9\n",
      "(2022)\n",
      "38. NVIDIA: Nvidia tensorrt. https://developer.nvidia.com/tensorrt (2022)\n",
      "39. OpenMP: About us. https://www.openmp.org/about/about-us/ (2022)\n",
      "40. Orsic, M., Kreso, I., Bevandic, P., Segvic, S.: In defense of pre-trained imagenet architectures for real-time semantic\n",
      "segmentation of road-driving images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition. pp. 12607–12616 (2019)\n",
      "41. Photonfocus: MV1-D2048x1088-HS02-96-G2. https://www.photonfocus.com/products/camerafinder/camera/\n",
      "mv1-d2048x1088-hs02-96-g2\n",
      "42. Pi, R.: Raspberry pi 4 model b datasheet. https://datasheets.raspberrypi.com/rpi4/raspberry-pi-4-datasheet.pdf\n",
      "(2019)\n",
      "43. Pinchon, N., Cassignol, O., Nicolas, A., Bernardin, F., Leduc, P., Tarel, J.P., Br´ emond, R., Bercier, E., Brunet, J.:\n",
      "All-weather vision for automotive safety: which spectral band? In: International Forum on Advanced Microsystems\n",
      "for Automotive Applications. pp. 3–15. Springer (2018)\n",
      "44. Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. In:\n",
      "International Conference on Medical image computing and computer-assisted intervention. pp. 234–241. Springer\n",
      "(2015)\n",
      "45. Seidlitz, S., Sellner, J., Odenthal, J., ¨Ozdemir, B., Studier-Fischer, A., Kn¨ odler, S., Ayala, L., Adler, T.J., Kenngott,\n",
      "H.G., Tizabi, M., et al.: Robust deep learning-based semantic organ segmentation in hyperspectral images. Medical\n",
      "Image Analysis p. 102488 (2022)\n",
      "46. Son, G.J., Kwak, D.H., Park, M.K., Kim, Y.D., Jung, H.C.: U-net-based foreign object detection method using\n",
      "effective image acquisition system: A case of almond and green onion flake food process. Sustainability 13(24),\n",
      "13834 (2021)\n",
      "47. Taghizadeh, M., Gowen, A.A., O’Donnell, C.P.: Comparison of hyperspectral imaging with conventional rgb imaging\n",
      "for quality evaluation of agaricus bisporus mushrooms. Biosystems engineering 108(2), 191–194 (2011)\n",
      "48. Tensorflow: Xnnpack backend for tensorflow lite. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/\n",
      "lite/delegates/xnnpack (2013)\n",
      "49. Tools, K.: Usb digital meter, usb-a and usb-c. https://www.kleintools.com/catalog/usb-digital-meters/\n",
      "usb-digital-meter-usb-and-usb-c (2021)\n",
      "50. Vehicle, A..A.: Terranet showcases motion awareness system with 20ms re-\n",
      "action time (2021), https://www.autonomousvehicleinternational.com/news/adas/\n",
      "terranet-showcases-motion-awareness-system-with-20ms-reaction-time.html, 21.10.2022\n",
      "51. Wang, S., Chen, W., Xie, S.M., Azzari, G., Lobell, D.B.: Weakly supervised deep learning for segmentation of remote\n",
      "sensing imagery. Remote Sensing 12(2), 207 (2020)\n",
      "52. Weikl, K., Schroeder, D., Stechele, W.: Potentials of combined visible light and near infrared imaging for driving\n",
      "automation. In: Electronic Imaging Conference (2022)\n",
      "53. Winkens, C., Adams, V., Paulus, D.: Automatic shadow detection using hyperspectral data for terrain classification.\n",
      "Electronic Imaging pp. 31–1–31.6 (2019)\n",
      "54. Winkens, C., Kobelt, V., Paulus, D.: Robust features for snapshot hyperspectral terrain classification. In: 17th Int.\n",
      "Conf. Computer Analysis of Images and Patterns. pp. 16–27 (2017)\n",
      "55. Winkens, C., Paulus, D.: Context aware hyperspectral scene analysis. In: Int. Symp. Electronic Imaging Science and\n",
      "Technology. pp. 346.1–346.7 (2018)\n",
      "56. Winkens, C., Sattler, F., Paulus, D.: Deep dimension reduction for spatial-spectral road scene classification. Electronic\n",
      "Imaging pp. 49.1–49.9 (2019)\n",
      "57. Xilinx: Hw-z1-zcu104 evaluation board (xczu7ev-2ffvc1156) schematic. https://www.xilinx.com/products/\n",
      "boards-and-kits/zcu104.html#resources (2018)\n",
      "58. Xilinx: Zcu104 evaluation board. https://www.xilinx.com/support/documents/boards andkits/zcu104/\n",
      "ug1267-zcu104-eval-bd.pdf (2018)\n",
      "59. Xilinx: Quantizing the model. https://www.xilinx.com/html docs/vitis ai/1 4/quantize.html#uim1570695919827\n",
      "(2021)\n",
      "60. Xilinx: Dpuczdx8g for zynq ultrascale+ mpsocs product guide (pg338). https://docs.xilinx.com/r/en-US/pg338-dpu?\n",
      "tocId=Bd4R4bhnWgMYE6wUISXDLw (2022)\n"
     ]
    }
   ],
   "source": [
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the titles of academic papers from this references section. \n",
      "Output only the titles, one per line:\n",
      "\n",
      "\n",
      "1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin,\n",
      "M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur,\n",
      "M., Levenberg, J., Man´ e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\n",
      "Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi´ egas, F., Vinyals, O., Warden, P., Wattenberg,\n",
      "M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\n",
      "https://www.tensorflow.org/, software available from tensorflow.org\n",
      "2. Alhaija, H., Mustikovela, S., Mescheder, L., Geiger, A., Rother, C.: Augmented reality meets computer vision:\n",
      "Efficient data generation for urban driving scenes. International Journal of Computer Vision (IJCV) (2018)\n",
      "3. AnandTech: Arm reveals cortex-a72 architecture details (2015), https://www.anandtech.com/show/9184/\n",
      "arm-reveals-cortex-a72-architecture-details, 10.10.2022\n",
      "4. ARM: Learn the architecture - introducing neon. https://developer.arm.com/documentation/102474/0100/?lang=en\n",
      "(2020)\n",
      "5. Bai, J., Shi, W., Xiao, Z., Ali, T.A.A., Ye, F., Jiao, L.: Achieving better category separability for hyperspectral image\n",
      "classification: A spatial–spectral approach. IEEE Transactions on Neural Networks and Learning Systems (2023)\n",
      "6. Basterretxea, K., Mart´ ınez, V., Echanobe, J., Guti´ errez–Zaballa, J., Del Campo, I.: Hsi-drive: A dataset for the\n",
      "research of hyperspectral image processing applied to autonomous driving systems. In: 2021 IEEE Intelligent Vehicles\n",
      "Symposium (IV). pp. 866–873 (2021). https://doi.org/10.1109/IV48863.2021.9575298\n",
      "7. Borges, P., Peynot, T., Liang, S., Arain, B., Wildie, M., Minareci, M., Lichman, S., Samvedi, G., Sa, I., Hudson, N.,\n",
      "et al.: A survey on terrain traversability analysis for autonomous ground vehicles: Methods, sensors, and challenges.\n",
      "Field Robotics 2(1), 1567–1627 (2022)\n",
      "8. Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: A high-definition ground truth database.\n",
      "Pattern Recognition Letters xx(x), xx–xx (2008)\n",
      "9. Brostow, G.J., Shotton, J., Fauqueur, J., Cipolla, R.: Segmentation and recognition using structure from motion\n",
      "point clouds. In: ECCV (1). pp. 44–57 (2008)\n",
      "10. Cavigelli, L., Bernath, D., Magno, M., Benini, L.: Computationally efficient target classification in multispectral\n",
      "image data with deep neural networks. In: Target and Background Signatures II. vol. 9997, pp. 191–202. SPIE\n",
      "(2016)\n",
      "11. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with\n",
      "deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and\n",
      "machine intelligence 40(4), 834–848 (2017)\n",
      "12. Colomb, M., Duthon, P., Bernardin, F.: Spectral reflectance characterization of the road environment to optimize\n",
      "the choice of autonomous vehicle sensors. In: 2019 IEEE Intelligent Transportation Systems Conference (ITSC). pp.\n",
      "1085–1090. IEEE (2019)Title Suppressed Due to Excessive Length 35\n",
      "13. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The\n",
      "cityscapes dataset for semantic urban scene understanding. In: Proceedings of the IEEE conference on computer\n",
      "vision and pattern recognition. pp. 3213–3223 (2016)\n",
      "14. Courdier, E., Fleuret, F.: Real-time segmentation networks should be latency aware. In: Proceedings of the Asian\n",
      "Conference on Computer Vision (2020)\n",
      "15. Cui, X., Zheng, K., Gao, L., Zhang, B., Yang, D., Ren, J.: Multiscale spatial-spectral convolutional network with\n",
      "image-based framework for hyperspectral imagery classification. Remote Sensing 11(19), 2220 (2019)\n",
      "16. Du, Q., Yang, H.: Similarity-based unsupervised band selection for hyperspectral image analysis. IEEE geoscience\n",
      "and remote sensing letters 5(4), 564–568 (2008)\n",
      "17. Fan, M., Lai, S., Huang, J., Wei, X., Chai, Z., Luo, J., Wei, X.: Rethinking bisenet for real-time semantic segmen-\n",
      "tation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9716–9725\n",
      "(2021)\n",
      "18. Forestier, G., Inglada, J., Wemmert, C., Gan¸ carski, P.: Comparison of optical sensors discrimination ability using\n",
      "spectral libraries. International journal of remote sensing 34(7), 2327–2349 (2013)\n",
      "19. Fricker, G.A., Ventura, J.D., Wolf, J.A., North, M.P., Davis, F.W., Franklin, J.: A convolutional neural network\n",
      "classifier identifies tree species in mixed-conifer forest from hyperspectral imagery. Remote Sensing 11(19), 2326\n",
      "(2019)\n",
      "20. Govender, M., Chetty, K., Bulcock, H.: A review of hyperspectral remote sensing and its application in vegetation\n",
      "and water resource studies. Water Sa 33(2), 145–151 (2007)\n",
      "21. Guti´ errez-Zaballa, J., Basterretxea, K., Echanobe, J., Mart´ ınez, M.V., del Campo, I.: Exploring fully convolutional\n",
      "networks for the segmentation of hyperspectral imaging applied to advanced driver assistance systems. In: Design\n",
      "and Architecture for Signal and Image Processing: 15th International Workshop, DASIP 2022, Budapest, Hungary,\n",
      "June 20–22, 2022, Proceedings. p. 136–148. Springer-Verlag, Berlin, Heidelberg (2022). https://doi.org/10.1007/\n",
      "978-3-031-12748-9 11, https://doi.org/10.1007/978-3-031-12748-9 11\n",
      "22. Hanhirova, J., K¨ am¨ ar¨ ainen, T., Sepp¨ al¨ a, S., Siekkinen, M., Hirvisalo, V., Yl¨ a-J¨ a¨ aski, A.: Latency and throughput\n",
      "characterization of convolutional neural networks for mobile computer vision. In: Proceedings of the 9th ACM\n",
      "Multimedia Systems Conference. pp. 204–215 (2018)\n",
      "23. Herweg, J., Kerekes, J., Eismann, M.: Separability between pedestrians in hyperspectral imagery. Appl. Opt.\n",
      "52(6), 1330–1338 (Feb 2013). https://doi.org/10.1364/AO.52.001330, https://opg.optica.org/ao/abstract.cfm?URI=\n",
      "ao-52-6-1330\n",
      "24. Holly, S., Wendt, A., Lechner, M.: Profiling energy consumption of deep neural networks on nvidia jetson nano. In:\n",
      "2020 11th International Green and Sustainable Computing Workshops (IGSC). pp. 1–6. IEEE (2020)\n",
      "25. Huang, Y., Shen, Q., Fu, Y., You, S.: Weakly-supervised semantic segmentation in cityscape via hyperspectral image.\n",
      "In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1117–1126 (2021)\n",
      "26. Infineon: Usb005 user guide. https://www.infineon.com/cms/en/product/power/dc-dc-converters/\n",
      "digital-multiphase-controllers/gang-programmers/usb005/ (2014)\n",
      "27. Infineon: Irps5401 pmic flexible power management unit. https://www.infineon.com/dgdl/\n",
      "Infineon-IRPS5401M-DataSheet-v02 06-EN.pdf?fileId=5546d4625cc9456a015cd69d402139db (2021)\n",
      "28. Iqbal, H.: Harisiqbal88/plotneuralnet v1. 0.0. URL: https://doi. org/10.5281/Zenodo (2018)\n",
      "29. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and\n",
      "training of neural networks for efficient integer-arithmetic-only inference. In: Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition. pp. 2704–2713 (2018)\n",
      "30. Kim, B., Yim, J., Kim, J.: Highway driving dataset for semantic video segmentation. arXiv preprint arXiv:2011.00674\n",
      "(2020)\n",
      "31. Li, P., Dong, X., Yu, X., Yang, Y.: When humans meet machines: Towards efficient segmentation networks. In: The\n",
      "31st British Machine Vision Virtual Conference (2020)\n",
      "32. Liyanage, D.C., Hudjakov, R., Tamre, M.: Hyperspectral imaging methods improve rgb image semantic segmentation\n",
      "of unstructured terrains. In: 2020 International Conference Mechatronic Systems and Materials (MSM). pp. 1–5\n",
      "(2020). https://doi.org/10.1109/MSM49833.2020.9201738\n",
      "33. Lu, J., Liu, H., Yao, Y., Tao, S., Tang, Z., Lu, J.: Hsi road: A hyper spectral image dataset for road segmentation.\n",
      "In: 2020 IEEE International Conference on Multimedia and Expo (ICME). pp. 1–6 (2020). https://doi.org/10.1109/\n",
      "ICME46284.2020.9102890\n",
      "34. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research 9(11) (2008)\n",
      "35. Malivenko, G.: onnx2keras 0.0.24. https://pypi.org/project/onnx2keras/ (2021)\n",
      "36. MATLAB: Deep learning toolbox. https://es.mathworks.com/products/deep-learning.html (2022)36 J. Guti´ errez-Zaballa et al.\n",
      "37. NVIDIA: Nvidia jetson nano system-on-module. https://developer.download.nvidia.\n",
      "com/assets/embedded/secure/jetson/Nano/docs/JetsonNano DataSheet DS09366001v1.1.pdf?\n",
      "MpmY78SZFEFH-jJG2aGI3bYEw75LZBVZJnkH7Bx-4huO6qS6b4u-9T33O-jPr9XU3MLlbBnF\n",
      "lfY5GTCVIn7wUCwW5pHfZjouudAa9FcSOHi1CB0qyQ40PzYgABdhZ9d-smLolN-UV67\n",
      "e8KvMzRjSMahO2ebQ8RZOxboLa GZcNQUaMtBGVa94dW67a7g&t=eyJscyI6ImdzZW8iLCJsc2QiOiJodHRwczpcL1wvd3d3Lmdvb2dsZS5jb21cLyJ9\n",
      "(2022)\n",
      "38. NVIDIA: Nvidia tensorrt. https://developer.nvidia.com/tensorrt (2022)\n",
      "39. OpenMP: About us. https://www.openmp.org/about/about-us/ (2022)\n",
      "40. Orsic, M., Kreso, I., Bevandic, P., Segvic, S.: In defense of pre-trained imagenet architectures for real-time semantic\n",
      "segmentation of road-driving images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition. pp. 12607–12616 (2019)\n",
      "41. Photonfocus: MV1-D2048x1088-HS02-96-G2. https://www.photonfocus.com/products/camerafinder/camera/\n",
      "mv1-d2048x1088-hs02-96-g2\n",
      "42. Pi, R.: Raspberry pi 4 model b datasheet. https://datasheets.raspberrypi.com/rpi4/raspberry-pi-4-datasheet.pdf\n",
      "(2019)\n",
      "43. Pinchon, N., Cassignol, O., Nicolas, A., Bernardin, F., Leduc, P., Tarel, J.P., Br´ emond, R., Bercier, E., Brunet, J.:\n",
      "All-weather vision for automotive safety: which spectral band? In: International Forum on Advanced Microsystems\n",
      "for Automotive Applications. pp. 3–15. Springer (2018)\n",
      "44. Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. In:\n",
      "International Conference on Medical image computing and computer-assisted intervention. pp. 234–241. Springer\n",
      "(2015)\n",
      "45. Seidlitz, S., Sellner, J., Odenthal, J., ¨Ozdemir, B., Studier-Fischer, A., Kn¨ odler, S., Ayala, L., Adler, T.J., Kenngott,\n",
      "H.G., Tizabi, M., et al.: Robust deep learning-based semantic organ segmentation in hyperspectral images. Medic\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Extract the titles of academic papers from this references section. \n",
    "Output only the titles, one per line:\n",
    "\n",
    "{references}\"\"\".format(references=references[:10000])\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3409269d17bd49b3fb2f559d32f861cfddc0bba03741a7e5da2a9ccf61bfecc"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
