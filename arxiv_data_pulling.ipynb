{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API query to be processed: http://export.arxiv.org/api/query?search_query=all:AI\n",
      "Fetching papers 1 to 2000...\n",
      "Saved 2000 papers to the folder 'papers/ai'.\n",
      "Fetching papers 2001 to 4000...\n",
      "Saved 2000 papers to the folder 'papers/ai'.\n",
      "Fetching papers 4001 to 6000...\n",
      "Saved 2000 papers to the folder 'papers/ai'.\n",
      "Fetching papers 6001 to 8000...\n",
      "Saved 2000 papers to the folder 'papers/ai'.\n",
      "Fetching papers 8001 to 10000...\n",
      "Saved 2000 papers to the folder 'papers/ai'.\n",
      "Fetching papers 10001 to 12000...\n",
      "Saved 2000 papers to the folder 'papers/ai'.\n",
      "Fetching papers 12001 to 14000...\n",
      "Saved 2000 papers to the folder 'papers/ai'.\n",
      "Fetching papers 14001 to 16000...\n",
      "No more results found.\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_save_papers(save_dir=\"papers/ai\", batch_size=2000, search_term=\"all:AI\"):\n",
    "    # Constants for API and directory\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    max_results = 30000  # Max results the API allows\n",
    "\n",
    "    print(f\"API query to be processed: {base_url}?search_query={search_term}\")\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    start = 0  # Start index\n",
    "    while start < max_results:\n",
    "        # Construct the query URL\n",
    "        url = f\"{base_url}?search_query={search_term}&start={start}&max_results={batch_size}\"\n",
    "        print(f\"Fetching papers {start + 1} to {start + batch_size}...\")\n",
    "\n",
    "        try:\n",
    "            # Fetch data from the API\n",
    "            response = urllib.request.urlopen(url)\n",
    "            data = response.read().decode('utf-8')\n",
    "\n",
    "            # Parse the XML response\n",
    "            root = ET.fromstring(data)\n",
    "            entries = root.findall(\"{http://www.w3.org/2005/Atom}entry\")\n",
    "            \n",
    "            if not entries:\n",
    "                print(\"No more results found.\")\n",
    "                break\n",
    "\n",
    "            for entry in entries:\n",
    "                paper_id = entry.find(\"{http://www.w3.org/2005/Atom}id\").text.split(\"/\")[-1]\n",
    "                title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text.strip()\n",
    "                summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text.strip()\n",
    "                published_date = entry.find(\"{http://www.w3.org/2005/Atom}published\").text\n",
    "                updated_date = entry.find(\"{http://www.w3.org/2005/Atom}updated\").text\n",
    "                authors = [author.find(\"{http://www.w3.org/2005/Atom}name\").text for author in entry.findall(\"{http://www.w3.org/2005/Atom}author\")]\n",
    "                journal_ref = entry.find(\"{http://arxiv.org/schemas/atom}journal_ref\")\n",
    "                journal_ref = journal_ref.text if journal_ref is not None else \"N/A\"\n",
    "                doi = entry.find(\"{http://arxiv.org/schemas/atom}doi\")\n",
    "                doi = doi.text if doi is not None else \"N/A\"\n",
    "\n",
    "                # Prepare content\n",
    "                content = f\"Paper ID: {paper_id}\\n\"\n",
    "                content += f\"Title: {title}\\n\"\n",
    "                content += f\"Authors: {', '.join(authors)}\\n\"\n",
    "                content += f\"Abstract: {summary}\\n\"\n",
    "                content += f\"Published Date: {published_date}\\n\"\n",
    "                content += f\"Updated Date: {updated_date}\\n\"\n",
    "                content += f\"Journal Reference: {journal_ref}\\n\"\n",
    "                content += f\"DOI: {doi}\\n\"\n",
    "\n",
    "                # Save metadata\n",
    "                file_path = os.path.join(save_dir, f\"{paper_id}_metadata.txt\")\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(content)\n",
    "\n",
    "\n",
    "            print(f\"Saved {len(entries)} papers to the folder '{save_dir}'.\")\n",
    "            start += batch_size\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching or saving papers: {e}\")\n",
    "            break\n",
    "\n",
    "# Execute the function\n",
    "fetch_and_save_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social-graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
