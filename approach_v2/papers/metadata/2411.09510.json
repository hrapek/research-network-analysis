{
    "abstract": "Large Language Models (LLMs) have pushed the frontier of artificial intelligence but are comprised of hundreds of billions of parameters and operations. For faster inference latency, LLMs are deployed on multiple hardware accelerators through various Model Parallelism strategies. Our paper looks into the details on one such strategy - Tensor Parallel - and proposes to reduce latency by compressing inter-accelerator communication. We leverage fine grained quantization techniques to compress selected activations by 3.5 - 4.5x. Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) with negligible model performance degradation.",
    "arxivId": "2411.09510",
    "authors": [
        {
            "authorId": "2330586543",
            "name": "Jan Hansen-Palmus",
            "url": "https://www.semanticscholar.org/author/2330586543"
        },
        {
            "authorId": "2331173031",
            "name": "Michael Truong Le",
            "url": "https://www.semanticscholar.org/author/2331173031"
        },
        {
            "authorId": "2330586541",
            "name": "Oliver Hausdorfer",
            "url": "https://www.semanticscholar.org/author/2330586541"
        },
        {
            "authorId": "2331151389",
            "name": "Alok Verma",
            "url": "https://www.semanticscholar.org/author/2331151389"
        }
    ],
    "citationVelocity": 0,
    "citations": [],
    "corpusId": 274023002,
    "doi": null,
    "fieldsOfStudy": [
        "Computer Science"
    ],
    "influentialCitationCount": 0,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 0,
    "numCiting": 0,
    "paperId": "c5d010c9f3d2a5c76229faed106add7441f8b5fe",
    "references": [],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "title": "Communication Compression for Tensor Parallel LLM Inference",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/c5d010c9f3d2a5c76229faed106add7441f8b5fe",
    "venue": "",
    "year": 2024
}