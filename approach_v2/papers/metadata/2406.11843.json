{
    "abstract": "We argue that the trend toward providing users with feasible and actionable explanations of AI decisions, known as recourse explanations, comes with ethical downsides. Specifically, we argue that recourse explanations face several conceptual pitfalls and can lead to problematic explanation hacking, which undermines their ethical status. As an alternative, we advocate that explanations of AI decisions should aim at understanding.",
    "arxivId": "2406.11843",
    "authors": [
        {
            "authorId": "2301216528",
            "name": "Emily Sullivan",
            "url": "https://www.semanticscholar.org/author/2301216528"
        },
        {
            "authorId": "51880633",
            "name": "Atoosa Kasirzadeh",
            "url": "https://www.semanticscholar.org/author/51880633"
        }
    ],
    "citationVelocity": 0,
    "citations": [
        {
            "arxivId": "2409.03632",
            "authors": [
                {
                    "authorId": "2275053740",
                    "name": "Andrew Smart"
                },
                {
                    "authorId": "51880633",
                    "name": "Atoosa Kasirzadeh"
                }
            ],
            "doi": "10.1007/s00146-024-02056-1",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "2b4a90fba7d92ddffe37755d8942531806841ecd",
            "title": "Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning",
            "url": "https://www.semanticscholar.org/paper/2b4a90fba7d92ddffe37755d8942531806841ecd",
            "venue": "AI &amp; SOCIETY",
            "year": 2024
        },
        {
            "arxivId": "2404.16534",
            "authors": [
                {
                    "authorId": "2260935745",
                    "name": "Emily Sullivan"
                }
            ],
            "doi": "10.1145/3630106.3658999",
            "intent": [],
            "isInfluential": false,
            "paperId": "ef6d3549290399c9ba6e6cefccfe1d19247b93b9",
            "title": "SIDEs: Separating Idealization from Deceptive 'Explanations' in xAI",
            "url": "https://www.semanticscholar.org/paper/ef6d3549290399c9ba6e6cefccfe1d19247b93b9",
            "venue": "FAccT",
            "year": 2024
        }
    ],
    "corpusId": 269763038,
    "doi": "10.48550/arXiv.2406.11843",
    "fieldsOfStudy": [
        "Computer Science"
    ],
    "influentialCitationCount": 0,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 2,
    "numCiting": 0,
    "paperId": "26c9583110d0d701eb9b6b635dc5f806579a24d3",
    "references": [],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Philosophy",
            "source": "s2-fos-model"
        }
    ],
    "title": "Explanation Hacking: The perils of algorithmic recourse",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/26c9583110d0d701eb9b6b635dc5f806579a24d3",
    "venue": "arXiv.org",
    "year": 2024
}