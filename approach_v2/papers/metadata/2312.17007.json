{
    "abstract": "One of the most recent and fascinating breakthroughs in artificial intelligence is ChatGPT, a chatbot which can simulate human conversation. ChatGPT is an instance of GPT4, which is a language model based on generative gredictive gransformers. So if one wants to study from a theoretical point of view, how powerful such artificial intelligence can be, one approach is to consider transformer networks and to study which problems one can solve with these networks theoretically. Here it is not only important what kind of models these network can approximate, or how they can generalize their knowledge learned by choosing the best possible approximation to a concrete data set, but also how well optimization of such transformer network based on concrete data set works. In this article we consider all these three different aspects simultaneously and show a theoretical upper bound on the missclassification probability of a transformer network fitted to the observed data. For simplicity we focus in this context on transformer encoder networks which can be applied to define an estimate in the context of a classification problem involving natural language.",
    "arxivId": "2312.17007",
    "authors": [
        {
            "authorId": "144789007",
            "name": "M. Kohler",
            "url": "https://www.semanticscholar.org/author/144789007"
        },
        {
            "authorId": "9745969",
            "name": "A. Krzy\u017cak",
            "url": "https://www.semanticscholar.org/author/9745969"
        }
    ],
    "citationVelocity": 0,
    "citations": [],
    "corpusId": 266573191,
    "doi": "10.48550/arXiv.2312.17007",
    "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
    ],
    "influentialCitationCount": 0,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 0,
    "numCiting": 0,
    "paperId": "38b4a8c0ae87c0e52dadd0a61dbffc70f7b65a22",
    "references": [],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "title": "On the rate of convergence of an over-parametrized Transformer classifier learned by gradient descent",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/38b4a8c0ae87c0e52dadd0a61dbffc70f7b65a22",
    "venue": "arXiv.org",
    "year": 2023
}