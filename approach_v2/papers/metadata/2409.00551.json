{
    "abstract": "Large language models (LLMs), such as ChatGPT, have rapidly penetrated into people's work and daily lives over the past few years, due to their extraordinary conversational skills and intelligence. ChatGPT has become the fastest-growing software in terms of user numbers in human history and become an important foundational model for the next generation of artificial intelligence applications. However, the generations of LLMs are not entirely reliable, often producing content with factual errors, biases, and toxicity. Given their vast number of users and wide range of application scenarios, these unreliable responses can lead to many serious negative impacts. This thesis introduces the exploratory works in the field of language model reliability during the PhD study, focusing on the correctness, non-toxicity, and fairness of LLMs from both software testing and natural language processing perspectives. First, to measure the correctness of LLMs, we introduce two testing frameworks, FactChecker and LogicAsker, to evaluate factual knowledge and logical reasoning accuracy, respectively. Second, for the non-toxicity of LLMs, we introduce two works for red-teaming LLMs. Third, to evaluate the fairness of LLMs, we introduce two evaluation frameworks, BiasAsker and XCulturalBench, to measure the social bias and cultural bias of LLMs, respectively.",
    "arxivId": "2409.00551",
    "authors": [
        {
            "authorId": "2319393061",
            "name": "Wenxuan Wang",
            "url": "https://www.semanticscholar.org/author/2319393061"
        }
    ],
    "citationVelocity": 0,
    "citations": [],
    "corpusId": 272367251,
    "doi": "10.48550/arXiv.2409.00551",
    "fieldsOfStudy": [
        "Computer Science"
    ],
    "influentialCitationCount": 0,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 0,
    "numCiting": 0,
    "paperId": "52df163d98bddbbc2483a1fde46715eb066b4a73",
    "references": [],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "title": "Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/52df163d98bddbbc2483a1fde46715eb066b4a73",
    "venue": "arXiv.org",
    "year": 2024
}