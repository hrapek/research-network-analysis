{
    "abstract": "This paper specifies a notation for Markov decision processes.",
    "arxivId": "1512.09075",
    "authors": [
        {
            "authorId": "143640165",
            "name": "P. Thomas",
            "url": "https://www.semanticscholar.org/author/143640165"
        }
    ],
    "citationVelocity": 0,
    "citations": [
        {
            "arxivId": "2211.11869",
            "authors": [
                {
                    "authorId": "103053353",
                    "name": "Anton Dereventsov"
                },
                {
                    "authorId": "49084461",
                    "name": "Andrew Starnes"
                },
                {
                    "authorId": "1684168",
                    "name": "C. Webster"
                }
            ],
            "doi": "10.48550/arXiv.2211.11869",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "35f732a6b98d43343e906b06cc7b9379029f354f",
            "title": "Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks",
            "url": "https://www.semanticscholar.org/paper/35f732a6b98d43343e906b06cc7b9379029f354f",
            "venue": "ArXiv",
            "year": 2022
        },
        {
            "arxivId": "2112.13141",
            "authors": [
                {
                    "authorId": "103053353",
                    "name": "Anton Dereventsov"
                },
                {
                    "authorId": "2065936712",
                    "name": "R. Vatsavai"
                },
                {
                    "authorId": "1684168",
                    "name": "C. Webster"
                }
            ],
            "doi": "10.1109/ICDMW53433.2021.00097",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "d0825f0e1b808764f359dce8d4bed91d93f273da",
            "title": "On the Unreasonable Efficiency of State Space Clustering in Personalization Tasks",
            "url": "https://www.semanticscholar.org/paper/d0825f0e1b808764f359dce8d4bed91d93f273da",
            "venue": "2021 International Conference on Data Mining Workshops (ICDMW)",
            "year": 2021
        },
        {
            "arxivId": "2106.03934",
            "authors": [
                {
                    "authorId": "103053353",
                    "name": "Anton Dereventsov"
                },
                {
                    "authorId": "123603087",
                    "name": "Joseph Daws"
                },
                {
                    "authorId": "1684168",
                    "name": "C. Webster"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "788bb49ba8c9af84b1cdebe0cfb9d2df64f63879",
            "title": "Offline Policy Comparison under Limited Historical Agent-Environment Interactions",
            "url": "https://www.semanticscholar.org/paper/788bb49ba8c9af84b1cdebe0cfb9d2df64f63879",
            "venue": "ArXiv",
            "year": 2021
        },
        {
            "arxivId": "2011.06709",
            "authors": [
                {
                    "authorId": "145055042",
                    "name": "David Krueger"
                },
                {
                    "authorId": "2990741",
                    "name": "J. Leike"
                },
                {
                    "authorId": "47107786",
                    "name": "Owain Evans"
                },
                {
                    "authorId": "3373139",
                    "name": "J. Salvatier"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "a47f52b25ce1e56a03876d9c0fd7c45e63270eb4",
            "title": "Active Reinforcement Learning: Observing Rewards at a Cost",
            "url": "https://www.semanticscholar.org/paper/a47f52b25ce1e56a03876d9c0fd7c45e63270eb4",
            "venue": "ArXiv",
            "year": 2020
        },
        {
            "arxivId": "2009.04743",
            "authors": [
                {
                    "authorId": "151751382",
                    "name": "Tom Bewley"
                },
                {
                    "authorId": "1926684",
                    "name": "J. Lawry"
                }
            ],
            "doi": "10.1609/aaai.v35i13.17360",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "de70a19dfbe56258149eae5b2e7a6c230c91071c",
            "title": "TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments",
            "url": "https://www.semanticscholar.org/paper/de70a19dfbe56258149eae5b2e7a6c230c91071c",
            "venue": "AAAI",
            "year": 2020
        },
        {
            "arxivId": "2008.06738",
            "authors": [
                {
                    "authorId": "137071348",
                    "name": "Brahma S. Pavse"
                },
                {
                    "authorId": "9571638",
                    "name": "Ishan Durugkar"
                },
                {
                    "authorId": "34719248",
                    "name": "Josiah P. Hanna"
                },
                {
                    "authorId": "144848112",
                    "name": "P. Stone"
                }
            ],
            "doi": "10.26153/TSW/14853",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "0f9d996c06ea67d9cf1ca09bb714261f18445579",
            "title": "Reducing Sampling Error in Batch Temporal Difference Learning",
            "url": "https://www.semanticscholar.org/paper/0f9d996c06ea67d9cf1ca09bb714261f18445579",
            "venue": "ICML",
            "year": 2020
        },
        {
            "arxivId": "1907.04214",
            "authors": [
                {
                    "authorId": "29505409",
                    "name": "B. Belousov"
                },
                {
                    "authorId": "145197867",
                    "name": "Jan Peters"
                }
            ],
            "doi": "10.3390/e21070674",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "843afef54676bf67adaf4108b8097f0a73d67f54",
            "title": "Entropic Regularization of Markov Decision Processes",
            "url": "https://www.semanticscholar.org/paper/843afef54676bf67adaf4108b8097f0a73d67f54",
            "venue": "Entropy",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1688702",
                    "name": "R. Akrour"
                },
                {
                    "authorId": "34906504",
                    "name": "J. Pajarinen"
                },
                {
                    "authorId": "145197867",
                    "name": "Jan Peters"
                },
                {
                    "authorId": "26599977",
                    "name": "G. Neumann"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "2ed89af84aacaadd0d3abed2517a08adf2262793",
            "title": "Projections for Approximate Policy Iteration Algorithms",
            "url": "https://www.semanticscholar.org/paper/2ed89af84aacaadd0d3abed2517a08adf2262793",
            "venue": "ICML",
            "year": 2019
        },
        {
            "arxivId": "1906.03063",
            "authors": [
                {
                    "authorId": "143640165",
                    "name": "P. Thomas"
                },
                {
                    "authorId": "143900003",
                    "name": "Scott M. Jordan"
                },
                {
                    "authorId": "2232505",
                    "name": "Yash Chandak"
                },
                {
                    "authorId": "71309987",
                    "name": "Chris Nota"
                },
                {
                    "authorId": "2073944141",
                    "name": "James E. Kostas"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "3f6f4b53870edbed5cfbd4ceb70c681a2abe1379",
            "title": "Classical Policy Gradient: Preserving Bellman's Principle of Optimality",
            "url": "https://www.semanticscholar.org/paper/3f6f4b53870edbed5cfbd4ceb70c681a2abe1379",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": "1706.06643",
            "authors": [
                {
                    "authorId": "143640165",
                    "name": "P. Thomas"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "5dc5782ff93cf641463bcfffa14e791317fb2a3b",
            "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation and Action-Dependent Baselines",
            "url": "https://www.semanticscholar.org/paper/5dc5782ff93cf641463bcfffa14e791317fb2a3b",
            "venue": "ArXiv",
            "year": 2017
        },
        {
            "arxivId": "1706.03469",
            "authors": [
                {
                    "authorId": "34719248",
                    "name": "Josiah P. Hanna"
                },
                {
                    "authorId": "143640165",
                    "name": "P. Thomas"
                },
                {
                    "authorId": "144848112",
                    "name": "P. Stone"
                },
                {
                    "authorId": "2791038",
                    "name": "S. Niekum"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "71c978c5ee8420ce928ab94656df2cffd2ae824e",
            "title": "Data-Efficient Policy Evaluation Through Behavior Policy Search",
            "url": "https://www.semanticscholar.org/paper/71c978c5ee8420ce928ab94656df2cffd2ae824e",
            "venue": "ICML",
            "year": 2017
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "143640165",
                    "name": "P. Thomas"
                },
                {
                    "authorId": "1709005",
                    "name": "Georgios Theocharous"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "9571638",
                    "name": "Ishan Durugkar"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": "10.1609/aaai.v31i1.19104",
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "1f81de57298677c98736d8b6f6736a279c75cc35",
            "title": "Predictive Off-Policy Policy Evaluation for Nonstationary Decision Problems, with Applications to Digital Marketing",
            "url": "https://www.semanticscholar.org/paper/1f81de57298677c98736d8b6f6736a279c75cc35",
            "venue": "AAAI",
            "year": 2017
        },
        {
            "arxivId": "1604.00923",
            "authors": [
                {
                    "authorId": "143640165",
                    "name": "P. Thomas"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": null,
            "intent": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "ec8a2f6cfe72309f5f1608d22ec28778d3ee976a",
            "title": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/ec8a2f6cfe72309f5f1608d22ec28778d3ee976a",
            "venue": "ICML",
            "year": 2016
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "80109565",
                    "name": "Jonas Eschmann"
                },
                {
                    "authorId": "81555516",
                    "name": "Fabio Muratore"
                },
                {
                    "authorId": "145197867",
                    "name": "Jan Peters"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "cef84be2be0da13dabaafb34507f4ae10ec6b580",
            "title": "Partially Unsupervised Deep Meta-Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/cef84be2be0da13dabaafb34507f4ae10ec6b580",
            "venue": "",
            "year": 2021
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1381448338",
                    "name": "Pablo Izquierdo Ayala"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "b92f73ff2434b28738bec0fb07d3253b40c82da1",
            "title": "Learning comparison: Reinforcement Learning vs Inverse Reinforcement Learning : How well does inverse reinforcement learning perform in simple markov decision processes in comparison to reinforcement learning?",
            "url": "https://www.semanticscholar.org/paper/b92f73ff2434b28738bec0fb07d3253b40c82da1",
            "venue": "",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2700008",
                    "name": "J. MacGlashan"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "cf249c6d5f8c82273023fa3b42ddda67382d9127",
            "title": "Generalized Inverse Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/cf249c6d5f8c82273023fa3b42ddda67382d9127",
            "venue": "",
            "year": 2017
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "143640165",
                    "name": "P. Thomas"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "724cb050554d255d71c98b8047a9d4987c3d6d1e",
            "title": "Magical Policy Search : Data Efficient Reinforcement Learning with Guarantees of Global Optimality",
            "url": "https://www.semanticscholar.org/paper/724cb050554d255d71c98b8047a9d4987c3d6d1e",
            "venue": "",
            "year": 2016
        }
    ],
    "corpusId": 15360315,
    "doi": null,
    "fieldsOfStudy": [
        "Computer Science"
    ],
    "influentialCitationCount": 2,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 17,
    "numCiting": 1,
    "paperId": "9d610a65203c189045982e656f22ff20204aded4",
    "references": [
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2238176724",
                    "name": "R. S. Sutton"
                },
                {
                    "authorId": "1730590",
                    "name": "A. Barto"
                }
            ],
            "doi": "10.1109/TNN.1998.712192",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "title": "Reinforcement Learning: An Introduction",
            "url": "https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054",
            "venue": "IEEE Trans. Neural Networks",
            "year": 1998
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "title": "A Notation for Markov Decision Processes",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/9d610a65203c189045982e656f22ff20204aded4",
    "venue": "arXiv.org",
    "year": 2015
}