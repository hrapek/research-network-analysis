{
    "abstract": "This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\\mathcal{O}}(d\\sqrt{H^3 T \\log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\\mathcal{O}(\\sqrt{\\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to formalize Bayesian regret bounds more effectively.",
    "arxivId": "2403.11175",
    "authors": [
        {
            "authorId": "2284304409",
            "name": "Yingru Li",
            "url": "https://www.semanticscholar.org/author/2284304409"
        },
        {
            "authorId": "2284286281",
            "name": "Zhi-Quan Luo",
            "url": "https://www.semanticscholar.org/author/2284286281"
        }
    ],
    "citationVelocity": 0,
    "citations": [],
    "corpusId": 268513034,
    "doi": "10.48550/arXiv.2403.11175",
    "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
    ],
    "influentialCitationCount": 0,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 0,
    "numCiting": 56,
    "paperId": "dd089f7e75f8e7fc12b0e8051b328fa6779d9632",
    "references": [
        {
            "arxivId": "2402.10228",
            "authors": [
                {
                    "authorId": "2284304409",
                    "name": "Yingru Li"
                },
                {
                    "authorId": "2260593028",
                    "name": "Jiawei Xu"
                },
                {
                    "authorId": "2284644227",
                    "name": "Lei Han"
                },
                {
                    "authorId": "2284286281",
                    "name": "Zhi-Quan Luo"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "7c8ed72733835684c86420e168b2cb5515d42115",
            "title": "Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent",
            "url": "https://www.semanticscholar.org/paper/7c8ed72733835684c86420e168b2cb5515d42115",
            "venue": "ICML",
            "year": 2024
        },
        {
            "arxivId": "2305.18246",
            "authors": [
                {
                    "authorId": "35652168",
                    "name": "Haque Ishfaq"
                },
                {
                    "authorId": "51305487",
                    "name": "Qingfeng Lan"
                },
                {
                    "authorId": "145612639",
                    "name": "Pan Xu"
                },
                {
                    "authorId": "1759633",
                    "name": "A. Mahmood"
                },
                {
                    "authorId": "144368601",
                    "name": "Doina Precup"
                },
                {
                    "authorId": "2047844",
                    "name": "Anima Anandkumar"
                },
                {
                    "authorId": "3371922",
                    "name": "K. Azizzadenesheli"
                }
            ],
            "doi": "10.48550/arXiv.2305.18246",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
            "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
            "url": "https://www.semanticscholar.org/paper/c90aa0f206c6fd41c490c142f63f7ba046cae6b7",
            "venue": "ICLR",
            "year": 2023
        },
        {
            "arxivId": "2303.04129",
            "authors": [
                {
                    "authorId": "47569072",
                    "name": "Sherry Yang"
                },
                {
                    "authorId": "7624658",
                    "name": "Ofir Nachum"
                },
                {
                    "authorId": "15394275",
                    "name": "Yilun Du"
                },
                {
                    "authorId": "119640649",
                    "name": "Jason Wei"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                },
                {
                    "authorId": "50319359",
                    "name": "D. Schuurmans"
                }
            ],
            "doi": "10.48550/arXiv.2303.04129",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "2ebd5df74980a37370b0bcdf16deff958289c041",
            "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities",
            "url": "https://www.semanticscholar.org/paper/2ebd5df74980a37370b0bcdf16deff958289c041",
            "venue": "ArXiv",
            "year": 2023
        },
        {
            "arxivId": "2212.06069",
            "authors": [
                {
                    "authorId": "40333747",
                    "name": "Alekh Agarwal"
                },
                {
                    "authorId": "2110894927",
                    "name": "Yujia Jin"
                },
                {
                    "authorId": "2146324552",
                    "name": "Tong Zhang"
                }
            ],
            "doi": "10.48550/arXiv.2212.06069",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4b04354e35e558c362ef36cda266f46074158b44",
            "title": "VOQL: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation",
            "url": "https://www.semanticscholar.org/paper/4b04354e35e558c362ef36cda266f46074158b44",
            "venue": "ArXiv",
            "year": 2022
        },
        {
            "arxivId": "2110.00871",
            "authors": [
                {
                    "authorId": "50728655",
                    "name": "Tong Zhang"
                }
            ],
            "doi": "10.1137/21m140924x",
            "intent": [],
            "isInfluential": false,
            "paperId": "dfee8970f6a4fa3e56b9dc7feb29ac721d04bb2a",
            "title": "Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/dfee8970f6a4fa3e56b9dc7feb29ac721d04bb2a",
            "venue": "SIAM J. Math. Data Sci.",
            "year": 2021
        },
        {
            "arxivId": "2106.07841",
            "authors": [
                {
                    "authorId": "35652168",
                    "name": "Haque Ishfaq"
                },
                {
                    "authorId": "1993999973",
                    "name": "Qiwen Cui"
                },
                {
                    "authorId": "144276381",
                    "name": "V. Nguyen"
                },
                {
                    "authorId": "2073147261",
                    "name": "Alex Ayoub"
                },
                {
                    "authorId": "150358650",
                    "name": "Zhuoran Yang"
                },
                {
                    "authorId": "50218397",
                    "name": "Zhaoran Wang"
                },
                {
                    "authorId": "144368601",
                    "name": "Doina Precup"
                },
                {
                    "authorId": "2155556894",
                    "name": "Lin F. Yang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "014e5f875578cbc6de620e47a0666056461f9aa5",
            "title": "Randomized Exploration for Reinforcement Learning with General Value Function Approximation",
            "url": "https://www.semanticscholar.org/paper/014e5f875578cbc6de620e47a0666056461f9aa5",
            "venue": "ArXiv",
            "year": 2021
        },
        {
            "arxivId": "2103.04047",
            "authors": [
                {
                    "authorId": "2110036504",
                    "name": "Xiuyuan Lu"
                },
                {
                    "authorId": "1731282",
                    "name": "Benjamin Van Roy"
                },
                {
                    "authorId": "3427981",
                    "name": "V. Dwaracherla"
                },
                {
                    "authorId": "145424823",
                    "name": "M. Ibrahimi"
                },
                {
                    "authorId": "2561924",
                    "name": "Ian Osband"
                },
                {
                    "authorId": "39761651",
                    "name": "Zheng Wen"
                }
            ],
            "doi": "10.1561/2200000097",
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "da45e961f285fdba9aefb3f4d4270620044eccb3",
            "title": "Reinforcement Learning, Bit by Bit",
            "url": "https://www.semanticscholar.org/paper/da45e961f285fdba9aefb3f4d4270620044eccb3",
            "venue": "Found. Trends Mach. Learn.",
            "year": 2021
        },
        {
            "arxivId": "2012.08507",
            "authors": [
                {
                    "authorId": "35972566",
                    "name": "Dongruo Zhou"
                },
                {
                    "authorId": "9937103",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "40868287",
                    "name": "Csaba Szepesvari"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "0bfc0ce3c40d83f24510ea80e2c3675413e3afa5",
            "title": "Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes",
            "url": "https://www.semanticscholar.org/paper/0bfc0ce3c40d83f24510ea80e2c3675413e3afa5",
            "venue": "COLT",
            "year": 2020
        },
        {
            "arxivId": "2006.01107",
            "authors": [
                {
                    "authorId": "2073147261",
                    "name": "Alex Ayoub"
                },
                {
                    "authorId": "2072782205",
                    "name": "Zeyu Jia"
                },
                {
                    "authorId": "40868287",
                    "name": "Csaba Szepesvari"
                },
                {
                    "authorId": "145731462",
                    "name": "Mengdi Wang"
                },
                {
                    "authorId": "40577530",
                    "name": "Lin F. Yang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "175a9a3f0bb4f31fa235386aff52ad18c67275d3",
            "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
            "url": "https://www.semanticscholar.org/paper/175a9a3f0bb4f31fa235386aff52ad18c67275d3",
            "venue": "L4DC",
            "year": 2020
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1924005367",
                    "name": "Cem Kalkanli"
                },
                {
                    "authorId": "145326369",
                    "name": "Ayfer \u00d6zg\u00fcr"
                }
            ],
            "doi": "10.1109/ISIT44484.2020.9174371",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "d2dd1a4d617c9cd0e54b61278f2ef2b9bcc652b9",
            "title": "An Improved Regret Bound for Thompson Sampling in the Gaussian Linear Bandit Setting",
            "url": "https://www.semanticscholar.org/paper/d2dd1a4d617c9cd0e54b61278f2ef2b9bcc652b9",
            "venue": "2020 IEEE International Symposium on Information Theory (ISIT)",
            "year": 2020
        },
        {
            "arxivId": "1912.05830",
            "authors": [
                {
                    "authorId": "2054915598",
                    "name": "Qi Cai"
                },
                {
                    "authorId": "150358650",
                    "name": "Zhuoran Yang"
                },
                {
                    "authorId": "3335298",
                    "name": "Chi Jin"
                },
                {
                    "authorId": "50218397",
                    "name": "Zhaoran Wang"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84",
            "title": "Provably Efficient Exploration in Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84",
            "venue": "ICML",
            "year": 2019
        },
        {
            "arxivId": "1911.09724",
            "authors": [
                {
                    "authorId": "2110036504",
                    "name": "Xiuyuan Lu"
                },
                {
                    "authorId": "1731282",
                    "name": "Benjamin Van Roy"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "3231ac937b2620cd3ea7c39fdacaf416a558d31c",
            "title": "Information-Theoretic Confidence Bounds for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/3231ac937b2620cd3ea7c39fdacaf416a558d31c",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": "1911.00567",
            "authors": [
                {
                    "authorId": "51122287",
                    "name": "A. Zanette"
                },
                {
                    "authorId": "35402876",
                    "name": "David Brandfonbrener"
                },
                {
                    "authorId": "6234609",
                    "name": "Matteo Pirotta"
                },
                {
                    "authorId": "3254390",
                    "name": "A. Lazaric"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "86687ad06378954f57cc01922b0369d97e75fd19",
            "title": "Frequentist Regret Bounds for Randomized Least-Squares Value Iteration",
            "url": "https://www.semanticscholar.org/paper/86687ad06378954f57cc01922b0369d97e75fd19",
            "venue": "AISTATS",
            "year": 2019
        },
        {
            "arxivId": "1910.10597",
            "authors": [
                {
                    "authorId": "32646210",
                    "name": "Aditya Modi"
                },
                {
                    "authorId": "48272707",
                    "name": "Nan Jiang"
                },
                {
                    "authorId": "3064914",
                    "name": "Ambuj Tewari"
                },
                {
                    "authorId": "2108384183",
                    "name": "Satinder Singh"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "525a8966c0399ec606c345213c6e2111767e67a6",
            "title": "Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles",
            "url": "https://www.semanticscholar.org/paper/525a8966c0399ec606c345213c6e2111767e67a6",
            "venue": "AISTATS",
            "year": 2019
        },
        {
            "arxivId": "1907.05388",
            "authors": [
                {
                    "authorId": "3335298",
                    "name": "Chi Jin"
                },
                {
                    "authorId": "150358650",
                    "name": "Zhuoran Yang"
                },
                {
                    "authorId": "50218397",
                    "name": "Zhaoran Wang"
                },
                {
                    "authorId": "123333909",
                    "name": "Michael I. Jordan"
                }
            ],
            "doi": "10.1287/moor.2022.1309",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
            "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
            "url": "https://www.semanticscholar.org/paper/d423fa6cd0f4d088941d9fe4bebd834d0137c9b9",
            "venue": "COLT",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "29767024",
                    "name": "Rishabh Agarwal"
                },
                {
                    "authorId": "50319359",
                    "name": "D. Schuurmans"
                },
                {
                    "authorId": "144739074",
                    "name": "Mohammad Norouzi"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4012d4ab621f3f5f04b0f91849a60c6eaabe64b4",
            "title": "An Optimistic Perspective on Offline Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/4012d4ab621f3f5f04b0f91849a60c6eaabe64b4",
            "venue": "ICML",
            "year": 2019
        },
        {
            "arxivId": "1904.00242",
            "authors": [
                {
                    "authorId": "2716214",
                    "name": "Yingkai Li"
                },
                {
                    "authorId": "2108741701",
                    "name": "Yining Wang"
                },
                {
                    "authorId": "2046913376",
                    "name": "Yuanshuo Zhou"
                }
            ],
            "doi": "10.1109/TIT.2023.3267732",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "fdb9e9ca290d79aa9051292e0bb060c1bfd5febb",
            "title": "Nearly Minimax-Optimal Regret for Linearly Parameterized Bandits",
            "url": "https://www.semanticscholar.org/paper/fdb9e9ca290d79aa9051292e0bb060c1bfd5febb",
            "venue": "IEEE Transactions on Information Theory",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "40577530",
                    "name": "Lin F. Yang"
                },
                {
                    "authorId": "145731462",
                    "name": "Mengdi Wang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "12cd30cab8281567f5a1e8e3145641336fbb819c",
            "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features",
            "url": "https://www.semanticscholar.org/paper/12cd30cab8281567f5a1e8e3145641336fbb819c",
            "venue": "ICML",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2117944720",
                    "name": "Qing Wang"
                },
                {
                    "authorId": "3081531",
                    "name": "Jiechao Xiong"
                },
                {
                    "authorId": "1390738614",
                    "name": "Lei Han"
                },
                {
                    "authorId": "2075416111",
                    "name": "Peng Sun"
                },
                {
                    "authorId": "2118959751",
                    "name": "Han Liu"
                },
                {
                    "authorId": "2146324552",
                    "name": "Tong Zhang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "caeb088b19b0829551ef4bf3da8b1a5c98bf8e73",
            "title": "Exponentially Weighted Imitation Learning for Batched Historical Data",
            "url": "https://www.semanticscholar.org/paper/caeb088b19b0829551ef4bf3da8b1a5c98bf8e73",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "arxivId": "1703.07608",
            "authors": [
                {
                    "authorId": "2561924",
                    "name": "Ian Osband"
                },
                {
                    "authorId": "145751896",
                    "name": "Daniel Russo"
                },
                {
                    "authorId": "39761651",
                    "name": "Zheng Wen"
                },
                {
                    "authorId": "1731282",
                    "name": "Benjamin Van Roy"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "a441728f9fd6af1946368240162a72c2028c8cb1",
            "title": "Deep Exploration via Randomized Value Functions",
            "url": "https://www.semanticscholar.org/paper/a441728f9fd6af1946368240162a72c2028c8cb1",
            "venue": "J. Mach. Learn. Res.",
            "year": 2017
        },
        {
            "arxivId": "1607.00215",
            "authors": [
                {
                    "authorId": "2561924",
                    "name": "Ian Osband"
                },
                {
                    "authorId": "1731282",
                    "name": "Benjamin Van Roy"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "88909a57da9a43ceb52aae8424b1f348dba99cab",
            "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?",
            "url": "https://www.semanticscholar.org/paper/88909a57da9a43ceb52aae8424b1f348dba99cab",
            "venue": "ICML",
            "year": 2016
        },
        {
            "arxivId": "1406.1853",
            "authors": [
                {
                    "authorId": "2561924",
                    "name": "Ian Osband"
                },
                {
                    "authorId": "1731282",
                    "name": "Benjamin Van Roy"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "paperId": "8783688bfe249bd1cab13146a76ba50fe88128c7",
            "title": "Model-based Reinforcement Learning and the Eluder Dimension",
            "url": "https://www.semanticscholar.org/paper/8783688bfe249bd1cab13146a76ba50fe88128c7",
            "venue": "NIPS",
            "year": 2014
        },
        {
            "arxivId": "1403.5341",
            "authors": [
                {
                    "authorId": "145751896",
                    "name": "Daniel Russo"
                },
                {
                    "authorId": "1731282",
                    "name": "Benjamin Van Roy"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "de6c988f7a6962a09a1c11f41ded0b63a5418559",
            "title": "An Information-Theoretic Analysis of Thompson Sampling",
            "url": "https://www.semanticscholar.org/paper/de6c988f7a6962a09a1c11f41ded0b63a5418559",
            "venue": "J. Mach. Learn. Res.",
            "year": 2014
        },
        {
            "arxivId": "1306.0940",
            "authors": [
                {
                    "authorId": "2561924",
                    "name": "Ian Osband"
                },
                {
                    "authorId": "145751896",
                    "name": "Daniel Russo"
                },
                {
                    "authorId": "1731282",
                    "name": "Benjamin Van Roy"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "789783016fb708abbc061790612ebe91273c05d3",
            "title": "(More) Efficient Reinforcement Learning via Posterior Sampling",
            "url": "https://www.semanticscholar.org/paper/789783016fb708abbc061790612ebe91273c05d3",
            "venue": "NIPS",
            "year": 2013
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "37666967",
                    "name": "M. G. Azar"
                },
                {
                    "authorId": "1708654",
                    "name": "R. Munos"
                },
                {
                    "authorId": "1792269",
                    "name": "H. Kappen"
                }
            ],
            "doi": "10.1007/s10994-013-5368-1",
            "intent": [
                "result"
            ],
            "isInfluential": false,
            "paperId": "cda4c3de398614354e6e627771530bff260f99fa",
            "title": "Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model",
            "url": "https://www.semanticscholar.org/paper/cda4c3de398614354e6e627771530bff260f99fa",
            "venue": "Machine Learning",
            "year": 2013
        },
        {
            "arxivId": "1301.2609",
            "authors": [
                {
                    "authorId": "145751896",
                    "name": "Daniel Russo"
                },
                {
                    "authorId": "1731282",
                    "name": "Benjamin Van Roy"
                }
            ],
            "doi": "10.1287/moor.2014.0650",
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "28cf1bd6110e734e20fc63f727d0d5bba612b921",
            "title": "Learning to Optimize via Posterior Sampling",
            "url": "https://www.semanticscholar.org/paper/28cf1bd6110e734e20fc63f727d0d5bba612b921",
            "venue": "Math. Oper. Res.",
            "year": 2013
        },
        {
            "arxivId": "1209.3352",
            "authors": [
                {
                    "authorId": "1703744",
                    "name": "Shipra Agrawal"
                },
                {
                    "authorId": "144260125",
                    "name": "Navin Goyal"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "f26f1a3c034b96514fc092dee99acacedd9c380b",
            "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs",
            "url": "https://www.semanticscholar.org/paper/f26f1a3c034b96514fc092dee99acacedd9c380b",
            "venue": "ICML",
            "year": 2012
        },
        {
            "arxivId": "1202.3890",
            "authors": [
                {
                    "authorId": "2989692",
                    "name": "Tor Lattimore"
                },
                {
                    "authorId": "144154444",
                    "name": "Marcus Hutter"
                }
            ],
            "doi": "10.1007/978-3-642-34106-9_26",
            "intent": [
                "result"
            ],
            "isInfluential": false,
            "paperId": "8e4d0530499fdbbc0581894371013da9fec8ed95",
            "title": "PAC Bounds for Discounted MDPs",
            "url": "https://www.semanticscholar.org/paper/8e4d0530499fdbbc0581894371013da9fec8ed95",
            "venue": "ALT",
            "year": 2012
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1730609",
                    "name": "O. Chapelle"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "ab867c140d2947511979c87e7ae580d9d3f0aeab",
            "title": "An Empirical Evaluation of Thompson Sampling",
            "url": "https://www.semanticscholar.org/paper/ab867c140d2947511979c87e7ae580d9d3f0aeab",
            "venue": "NIPS",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144543541",
                    "name": "P. Auer"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "103f6fe35033f9327611ddafde74a2b544072980",
            "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs",
            "url": "https://www.semanticscholar.org/paper/103f6fe35033f9327611ddafde74a2b544072980",
            "venue": "J. Mach. Learn. Res.",
            "year": 2003
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "81338045",
                    "name": "Michael Kearns"
                },
                {
                    "authorId": "1699868",
                    "name": "Satinder Singh"
                }
            ],
            "doi": "10.1023/A:1017984413808",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
            "title": "Near-Optimal Reinforcement Learning in Polynomial Time",
            "url": "https://www.semanticscholar.org/paper/dc649486b881e672eea6546da48c46e1f98daf32",
            "venue": "Machine Learning",
            "year": 2002
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2948478",
                    "name": "M. Strens"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "48cce5ee49facf75eeb12832c387452424b645dd",
            "title": "A Bayesian Framework for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/48cce5ee49facf75eeb12832c387452424b645dd",
            "venue": "ICML",
            "year": 2000
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1708654",
                    "name": "R. Munos"
                },
                {
                    "authorId": "1760402",
                    "name": "A. Moore"
                }
            ],
            "doi": "10.1109/CDC.1999.830188",
            "intent": [
                "result"
            ],
            "isInfluential": false,
            "paperId": "5661df6d9f220a43e9acf93b9bed5977f463c0a7",
            "title": "Influence and variance of a Markov chain: application to adaptive discretization in optimal control",
            "url": "https://www.semanticscholar.org/paper/5661df6d9f220a43e9acf93b9bed5977f463c0a7",
            "venue": "Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304)",
            "year": 1999
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "37814588",
                    "name": "M. Puterman"
                }
            ],
            "doi": "10.2307/2291177",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
            "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
            "url": "https://www.semanticscholar.org/paper/a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63",
            "venue": "",
            "year": 1994
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144301316",
                    "name": "W. R. Thompson"
                }
            ],
            "doi": "10.1093/BIOMET/25.3-4.285",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "ee2cd1d17f833d3c157a1016a778c7c22af555a2",
            "title": "ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES",
            "url": "https://www.semanticscholar.org/paper/ee2cd1d17f833d3c157a1016a778c7c22af555a2",
            "venue": "",
            "year": 1933
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "25841722",
                    "name": "Ziniu Li"
                },
                {
                    "authorId": "2163292697",
                    "name": "Yingru Li"
                },
                {
                    "authorId": "2164118584",
                    "name": "Yushun Zhang"
                },
                {
                    "authorId": "2117882779",
                    "name": "Tong Zhang"
                },
                {
                    "authorId": "2114939672",
                    "name": "Zhimin Luo"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
            "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
            "venue": "ICLR",
            "year": 2022
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "35621911",
                    "name": "N. Hamidi"
                },
                {
                    "authorId": "46491690",
                    "name": "M. Bayati"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "e01ed0036f99223eb5441ad950dba3b7c08af6d3",
            "title": "The Randomized Elliptical Potential Lemma with an Application to Linear Thompson Sampling",
            "url": "https://www.semanticscholar.org/paper/e01ed0036f99223eb5441ad950dba3b7c08af6d3",
            "venue": "ArXiv",
            "year": 2021
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2117944720",
                    "name": "Qing Wang"
                },
                {
                    "authorId": "2163292697",
                    "name": "Yingru Li"
                },
                {
                    "authorId": "3081531",
                    "name": "Jiechao Xiong"
                },
                {
                    "authorId": "2146324552",
                    "name": "Tong Zhang"
                }
            ],
            "doi": null,
            "intent": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "paperId": "e9e135ca90fb30fe01c7af0a24a6dcf46bd459e8",
            "title": "Divergence-Augmented Policy Optimization",
            "url": "https://www.semanticscholar.org/paper/e9e135ca90fb30fe01c7af0a24a6dcf46bd459e8",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1786249",
                    "name": "D. Bertsekas"
                }
            ],
            "doi": "10.1007/978-0-387-74759-0_440",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "b225a9eb169a3530289bf834d3b6e785947959ee",
            "title": "Neuro-Dynamic Programming",
            "url": "https://www.semanticscholar.org/paper/b225a9eb169a3530289bf834d3b6e785947959ee",
            "venue": "Encyclopedia of Optimization",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2714811",
                    "name": "Varsha Dani"
                },
                {
                    "authorId": "1806117",
                    "name": "Thomas P. Hayes"
                },
                {
                    "authorId": "144695232",
                    "name": "S. Kakade"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "551e19e5113cdff60a3c545d684fc4b9eb9a7306",
            "title": "Stochastic Linear Optimization under Bandit Feedback",
            "url": "https://www.semanticscholar.org/paper/551e19e5113cdff60a3c545d684fc4b9eb9a7306",
            "venue": "COLT",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2238176724",
                    "name": "R. S. Sutton"
                },
                {
                    "authorId": "1730590",
                    "name": "A. Barto"
                }
            ],
            "doi": "10.1109/TNN.1998.712192",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "title": "Reinforcement Learning: An Introduction",
            "url": "https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054",
            "venue": "IEEE Trans. Neural Networks",
            "year": 1998
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        },
        {
            "category": "Mathematics",
            "source": "s2-fos-model"
        }
    ],
    "title": "Prior-dependent analysis of posterior sampling reinforcement learning with function approximation",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/dd089f7e75f8e7fc12b0e8051b328fa6779d9632",
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "year": 2024
}