{
    "abstract": "Model-based learning algorithms have been shown to use experience efficiently when learning to solve Markov Decision Processes (MDPs) with finite state and action spaces. However, their high computational cost due to repeatedly solving an internal model inhibits their use in large-scale problems. We propose a method based on real-time dynamic programming (RTDP) to speed up two model-based algorithms, RMAX and MBIE (model-based interval estimation), resulting in computationally much faster algorithms with little loss compared to existing bounds. Specifically, our two new learning algorithms, RTDP-RMAX and RTDP-IE, have considerably smaller computational demands than RMAX and MBIE. We develop a general theoretical framework that allows us to prove that both are efficient learners in a PAC (probably approximately correct) sense. We also present an experimental evaluation of these new algorithms that helps quantify the tradeoff between computational and experience demands.",
    "arxivId": "1206.6870",
    "authors": [
        {
            "authorId": "1990806",
            "name": "Alexander L. Strehl",
            "url": "https://www.semanticscholar.org/author/1990806"
        },
        {
            "authorId": "47681372",
            "name": "Lihong Li",
            "url": "https://www.semanticscholar.org/author/47681372"
        },
        {
            "authorId": "144885169",
            "name": "M. Littman",
            "url": "https://www.semanticscholar.org/author/144885169"
        }
    ],
    "citationVelocity": 0,
    "citations": [
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2000356059",
                    "name": "Jianyu Xu"
                },
                {
                    "authorId": "2329492787",
                    "name": "Bin Liu"
                },
                {
                    "authorId": "47040154",
                    "name": "Xiujie Zhao"
                },
                {
                    "authorId": "2220927978",
                    "name": "Xiao-Lin Wang"
                }
            ],
            "doi": "10.1016/j.ejor.2023.11.039",
            "intent": [],
            "isInfluential": false,
            "paperId": "5db8d2e36e4378d86ed33a381cfaae70f6b69556",
            "title": "Online reinforcement learning for condition-based group maintenance using factored Markov decision processes",
            "url": "https://www.semanticscholar.org/paper/5db8d2e36e4378d86ed33a381cfaae70f6b69556",
            "venue": "Eur. J. Oper. Res.",
            "year": 2023
        },
        {
            "arxivId": "2210.11579",
            "authors": [
                {
                    "authorId": "81875788",
                    "name": "Haotian Fu"
                },
                {
                    "authorId": "2148459804",
                    "name": "Shangqun Yu"
                },
                {
                    "authorId": "2056611060",
                    "name": "Michael S. Littman"
                },
                {
                    "authorId": "1765407",
                    "name": "G. Konidaris"
                }
            ],
            "doi": "10.48550/arXiv.2210.11579",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "70e1d6b227fdd605fe61239a953e803df97e521d",
            "title": "Model-based Lifelong Reinforcement Learning with Bayesian Exploration",
            "url": "https://www.semanticscholar.org/paper/70e1d6b227fdd605fe61239a953e803df97e521d",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3195342",
                    "name": "A. Zehfroosh"
                },
                {
                    "authorId": "1748908",
                    "name": "H. Tanner"
                }
            ],
            "doi": "10.3389/frobt.2022.797213",
            "intent": [],
            "isInfluential": false,
            "paperId": "18bcbb4fed55c2c0cf877576433ddacbefa744a7",
            "title": "A Hybrid PAC Reinforcement Learning Algorithm for Human-Robot Interaction",
            "url": "https://www.semanticscholar.org/paper/18bcbb4fed55c2c0cf877576433ddacbefa744a7",
            "venue": "Frontiers in Robotics and AI",
            "year": 2022
        },
        {
            "arxivId": "2111.07395",
            "authors": [
                {
                    "authorId": "70557965",
                    "name": "David M. Bossens"
                },
                {
                    "authorId": "2104728893",
                    "name": "Nick Bishop"
                }
            ],
            "doi": "10.1007/s10994-022-06201-z",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "0ca5b869b2554e98711531fbc2c3ea6436d3ead0",
            "title": "Explicit Explore, Exploit, or Escape (\n \n \n \n $$E^4$$\n \n \n E\n 4\n \n \n ): near-optimal safety-constrained reinforcement learning in polynomial time",
            "url": "https://www.semanticscholar.org/paper/0ca5b869b2554e98711531fbc2c3ea6436d3ead0",
            "venue": "Mach. Learn.",
            "year": 2021
        },
        {
            "arxivId": "2009.02602",
            "authors": [
                {
                    "authorId": "3195342",
                    "name": "A. Zehfroosh"
                },
                {
                    "authorId": "1748908",
                    "name": "H. Tanner"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "6823250c50aaa8cc98de1c1e378b2eeb9ced19ed",
            "title": "A Hybrid PAC Reinforcement Learning Algorithm",
            "url": "https://www.semanticscholar.org/paper/6823250c50aaa8cc98de1c1e378b2eeb9ced19ed",
            "venue": "ArXiv",
            "year": 2020
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3195342",
                    "name": "A. Zehfroosh"
                },
                {
                    "authorId": "1748908",
                    "name": "H. Tanner"
                }
            ],
            "doi": "10.1109/MED48518.2020.9182985",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "8feaefab3681310417c3a6a8f2c03d99f6de65c7",
            "title": "A New Sample-Efficient PAC Reinforcement Learning Algorithm",
            "url": "https://www.semanticscholar.org/paper/8feaefab3681310417c3a6a8f2c03d99f6de65c7",
            "venue": "2020 28th Mediterranean Conference on Control and Automation (MED)",
            "year": 2020
        },
        {
            "arxivId": "2002.06659",
            "authors": [
                {
                    "authorId": "120738683",
                    "name": "Yanchao Sun"
                },
                {
                    "authorId": "31186224",
                    "name": "Xiangyu Yin"
                },
                {
                    "authorId": "2117426487",
                    "name": "Furong Huang"
                }
            ],
            "doi": "10.1609/aaai.v35i11.17174",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "00133d41d5ecef1a9d046d2b92bb2a23a335cb7c",
            "title": "TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL",
            "url": "https://www.semanticscholar.org/paper/00133d41d5ecef1a9d046d2b92bb2a23a335cb7c",
            "venue": "AAAI",
            "year": 2020
        },
        {
            "arxivId": "1912.10329",
            "authors": [
                {
                    "authorId": "120738683",
                    "name": "Yanchao Sun"
                },
                {
                    "authorId": "2117426487",
                    "name": "Furong Huang"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "bc531598440639ee047c48ed1cdca433844ffc6d",
            "title": "Can Agents Learn by Analogy? An Inferable Model for PAC Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/bc531598440639ee047c48ed1cdca433844ffc6d",
            "venue": "ArXiv",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2242846524",
                    "name": "Manfred Jaeger"
                },
                {
                    "authorId": "2242851717",
                    "name": "Peter Gj\u00f8l Jensen"
                },
                {
                    "authorId": "2242842320",
                    "name": "Kim G. Larsen"
                },
                {
                    "authorId": "2198400529",
                    "name": "Axel Legay"
                },
                {
                    "authorId": "1824281",
                    "name": "Sean Sedwards"
                },
                {
                    "authorId": "2426566",
                    "name": "Jakob Haahr Taankvist"
                }
            ],
            "doi": "10.1007/978-3-030-31784-3_5",
            "intent": [],
            "isInfluential": false,
            "paperId": "d613817c4e9a28c451334894b7850b4fd9cd88b7",
            "title": "Teaching Stratego to Play Ball: Optimal Synthesis for Continuous Space MDPs",
            "url": "https://www.semanticscholar.org/paper/d613817c4e9a28c451334894b7850b4fd9cd88b7",
            "venue": "ATVA",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3168195",
                    "name": "T. Adel"
                },
                {
                    "authorId": "145689461",
                    "name": "Adrian Weller"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "6b36776e5c0473d82cbdd2c92cd97cca7925ae08",
            "title": "TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/6b36776e5c0473d82cbdd2c92cd97cca7925ae08",
            "venue": "ICML",
            "year": 2019
        },
        {
            "arxivId": "1905.11527",
            "authors": [
                {
                    "authorId": "27098848",
                    "name": "Yonathan Efroni"
                },
                {
                    "authorId": "81589010",
                    "name": "Nadav Merlis"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "1712535",
                    "name": "Shie Mannor"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "6246b3d27f6a9f211126cafe39d8c8a7f6ed06f4",
            "title": "Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies",
            "url": "https://www.semanticscholar.org/paper/6246b3d27f6a9f211126cafe39d8c8a7f6ed06f4",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "152422014",
                    "name": "David Abel"
                },
                {
                    "authorId": "3387240",
                    "name": "Yuu Jinnai"
                },
                {
                    "authorId": "1390575736",
                    "name": "Yue (Sophie) Guo"
                },
                {
                    "authorId": "1765407",
                    "name": "G. Konidaris"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "12fcadd19ca4652934997ac537b071a4b6b221d2",
            "title": "Policy and Value Transfer in Lifelong Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/12fcadd19ca4652934997ac537b071a4b6b221d2",
            "venue": "ICML",
            "year": 2018
        },
        {
            "arxivId": "1805.09045",
            "authors": [
                {
                    "authorId": "71222706",
                    "name": "Yao Liu"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "bbe99c8d9cc154d4202876011d2702305c85af41",
            "title": "When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms",
            "url": "https://www.semanticscholar.org/paper/bbe99c8d9cc154d4202876011d2702305c85af41",
            "venue": "ArXiv",
            "year": 2018
        },
        {
            "arxivId": "1802.01518",
            "authors": [
                {
                    "authorId": "1832501",
                    "name": "I. Sledge"
                },
                {
                    "authorId": "32818596",
                    "name": "Matthew S. Emigh"
                },
                {
                    "authorId": "143961030",
                    "name": "J. Pr\u00edncipe"
                }
            ],
            "doi": "10.1109/TNNLS.2018.2812709",
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "08a1cb8f4b364a99a793a267b47609b68ea71af9",
            "title": "Guided Policy Exploration for Markov Decision Processes Using an Uncertainty-Based Value-of-Information Criterion",
            "url": "https://www.semanticscholar.org/paper/08a1cb8f4b364a99a793a267b47609b68ea71af9",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2018
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2324666",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "f0c41fed1961c2f52661aca2c02b7c7c357ad655",
            "title": "Towards Practical Theory: Bayesian Optimization and Optimal Exploration",
            "url": "https://www.semanticscholar.org/paper/f0c41fed1961c2f52661aca2c02b7c7c357ad655",
            "venue": "",
            "year": 2016
        },
        {
            "arxivId": "1605.08062",
            "authors": [
                {
                    "authorId": "3407143",
                    "name": "Z. Guo"
                },
                {
                    "authorId": "3396583",
                    "name": "Shayan Doroudi"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": true,
            "paperId": "1d0284f14605a8760c4bf5aac8ece9904a6796fb",
            "title": "A PAC RL Algorithm for Episodic POMDPs",
            "url": "https://www.semanticscholar.org/paper/1d0284f14605a8760c4bf5aac8ece9904a6796fb",
            "venue": "AISTATS",
            "year": 2016
        },
        {
            "arxivId": "1602.04875",
            "authors": [
                {
                    "authorId": "2108556730",
                    "name": "Min Chen"
                },
                {
                    "authorId": "1777799",
                    "name": "Emilio Frazzoli"
                },
                {
                    "authorId": "145463096",
                    "name": "David Hsu"
                },
                {
                    "authorId": "1740222",
                    "name": "Wee Sun Lee"
                }
            ],
            "doi": "10.1109/ICRA.2016.7487754",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "ed3ca6a2de3b0c2f65a918b8620d1b3b84d16181",
            "title": "POMDP-lite for robust robot planning under uncertainty",
            "url": "https://www.semanticscholar.org/paper/ed3ca6a2de3b0c2f65a918b8620d1b3b84d16181",
            "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA)",
            "year": 2016
        },
        {
            "arxivId": "1604.01350",
            "authors": [
                {
                    "authorId": "2324666",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "doi": "10.1609/aaai.v30i1.10230",
            "intent": [],
            "isInfluential": false,
            "paperId": "90a05b53e096541627de50600dfcdf52a2906945",
            "title": "Bounded Optimal Exploration in MDP",
            "url": "https://www.semanticscholar.org/paper/90a05b53e096541627de50600dfcdf52a2906945",
            "venue": "AAAI",
            "year": 2016
        },
        {
            "arxivId": "1510.08906",
            "authors": [
                {
                    "authorId": "2160071",
                    "name": "Christoph Dann"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "f4aa31b7ae2e03ee5a6b94f34cb3b6a554230aef",
            "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/f4aa31b7ae2e03ee5a6b94f34cb3b6a554230aef",
            "venue": "NIPS",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3407143",
                    "name": "Z. Guo"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": "10.1609/aaai.v29i1.9585",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "1b3d8a73cc4cf7c99c04c9234b80fec6c135ce0a",
            "title": "Concurrent PAC RL",
            "url": "https://www.semanticscholar.org/paper/1b3d8a73cc4cf7c99c04c9234b80fec6c135ce0a",
            "venue": "AAAI",
            "year": 2015
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f",
            "title": "PAC-inspired Option Discovery in Lifelong Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f",
            "venue": "ICML",
            "year": 2014
        },
        {
            "arxivId": "1309.6821",
            "authors": [
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "a05da3a9fee38d2c81216bf5b24ba50c84f7d9bc",
            "title": "Sample Complexity of Multi-task Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/a05da3a9fee38d2c81216bf5b24ba50c84f7d9bc",
            "venue": "UAI",
            "year": 2013
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2053956",
                    "name": "Susanne Still"
                },
                {
                    "authorId": "144368601",
                    "name": "Doina Precup"
                }
            ],
            "doi": "10.1007/s12064-011-0142-z",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "66b35f2a58d9cf2804828185c33857a7d46e6424",
            "title": "An information-theoretic approach to curiosity-driven reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/66b35f2a58d9cf2804828185c33857a7d46e6424",
            "venue": "Theory in Biosciences",
            "year": 2012
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144439416",
                    "name": "Karun K. Rao"
                },
                {
                    "authorId": "1766767",
                    "name": "Shimon Whiteson"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "dad5393b1d68d6ec793383757b580bb6cc8d4ac0",
            "title": "V-MAX: tempered optimism for better PAC reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/dad5393b1d68d6ec793383757b580bb6cc8d4ac0",
            "venue": "AAMAS",
            "year": 2012
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2328325626",
                    "name": "Istv\u00e1n Szita"
                },
                {
                    "authorId": "40868287",
                    "name": "Csaba Szepesvari"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "769a88591a60d816d5b58b0350039c80f34f9c1d",
            "title": "Agnostic KWIK learning and efficient approximate reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/769a88591a60d816d5b58b0350039c80f34f9c1d",
            "venue": "COLT",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2486264",
                    "name": "B. Zanuttini"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "8602a1fb47bfeb3cd3727d73c9e87117a1add80d",
            "title": "Computational Aspects of Learning, Reasoning, and Deciding",
            "url": "https://www.semanticscholar.org/paper/8602a1fb47bfeb3cd3727d73c9e87117a1add80d",
            "venue": "",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2961471",
                    "name": "Boris Lesner"
                },
                {
                    "authorId": "2486264",
                    "name": "B. Zanuttini"
                }
            ],
            "doi": "10.1609/icaps.v21i1.13454",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "0df359fcf62785bbd8b705182e974225428e61a2",
            "title": "Efficient Policy Construction for MDPs Represented in Probabilistic PDDL",
            "url": "https://www.semanticscholar.org/paper/0df359fcf62785bbd8b705182e974225428e61a2",
            "venue": "ICAPS",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "40396948",
                    "name": "A. Bernstein"
                },
                {
                    "authorId": "1742179",
                    "name": "N. Shimkin"
                }
            ],
            "doi": "10.1007/s10994-010-5186-7",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "07e3a5831bbd73cf3e40e16bc727451337d8a1f6",
            "title": "Adaptive-resolution reinforcement learning with\u00a0polynomial exploration in deterministic domains",
            "url": "https://www.semanticscholar.org/paper/07e3a5831bbd73cf3e40e16bc727451337d8a1f6",
            "venue": "Machine Learning",
            "year": 2010
        },
        {
            "arxivId": "1203.3518",
            "authors": [
                {
                    "authorId": "144123639",
                    "name": "Jonathan Sorg"
                },
                {
                    "authorId": "1699868",
                    "name": "Satinder Singh"
                },
                {
                    "authorId": "46328485",
                    "name": "Richard L. Lewis"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "1925db1d8b59fcec2293d8adbf8d92ff444a4c32",
            "title": "Variance-Based Rewards for Approximate Bayesian Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/1925db1d8b59fcec2293d8adbf8d92ff444a4c32",
            "venue": "UAI",
            "year": 2010
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": "10.5555/1577069.1755867",
            "intent": [
                "background",
                "result"
            ],
            "isInfluential": false,
            "paperId": "5d8e1eeeb0e4b0e0846a355532d0f9452249e68a",
            "title": "Reinforcement Learning in Finite MDPs: PAC Analysis",
            "url": "https://www.semanticscholar.org/paper/5d8e1eeeb0e4b0e0846a355532d0f9452249e68a",
            "venue": "J. Mach. Learn. Res.",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                },
                {
                    "authorId": "1700606",
                    "name": "Bethany R. Leffler"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "143724999",
                    "name": "N. Roy"
                }
            ],
            "doi": "10.5555/1577069.1755851",
            "intent": [],
            "isInfluential": false,
            "paperId": "9e6f4510e865c10d2d7a0a2c36ee671fe156e729",
            "title": "Provably Efficient Learning with Typed Parametric Models",
            "url": "https://www.semanticscholar.org/paper/9e6f4510e865c10d2d7a0a2c36ee671fe156e729",
            "venue": "J. Mach. Learn. Res.",
            "year": 2009
        },
        {
            "arxivId": "1205.2664",
            "authors": [
                {
                    "authorId": "31659886",
                    "name": "J. Asmuth"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "2758123",
                    "name": "A. Nouri"
                },
                {
                    "authorId": "30585164",
                    "name": "D. Wingate"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "606c3108fe948d9a8a0da8759f88de4df53b5d94",
            "title": "A Bayesian Sampling Approach to Exploration in Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/606c3108fe948d9a8a0da8759f88de4df53b5d94",
            "venue": "UAI",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1681946",
                    "name": "Carlos Diuk"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "1700606",
                    "name": "Bethany R. Leffler"
                }
            ],
            "doi": "10.1145/1553374.1553406",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "6316daea76f71804a0f3cd1e150d4df77a1bf517",
            "title": "The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning",
            "url": "https://www.semanticscholar.org/paper/6316daea76f71804a0f3cd1e150d4df77a1bf517",
            "venue": "ICML '09",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1737555",
                    "name": "M. Geist"
                },
                {
                    "authorId": "1721354",
                    "name": "O. Pietquin"
                },
                {
                    "authorId": "2582626",
                    "name": "G. Fricout"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "a2827d7718b718f242d3114f6f249a064efd37b0",
            "title": "From Supervised to Reinforcement Learning: a Kernel-based Bayesian Filtering Framework",
            "url": "https://www.semanticscholar.org/paper/a2827d7718b718f242d3114f6f249a064efd37b0",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "50258677",
                    "name": "Colin McMillen"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "4491e637f84ec00c5bbab860f61f6cb652d16fc5",
            "title": "Thresholded-rewards decision problems: Acting effectively in timed domains",
            "url": "https://www.semanticscholar.org/paper/4491e637f84ec00c5bbab860f61f6cb652d16fc5",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": "10.1016/j.jcss.2007.08.009",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
            "title": "An analysis of model-based Interval Estimation for Markov Decision Processes",
            "url": "https://www.semanticscholar.org/paper/237a1cf18ed83bb3ad852b34f443c6c1ff3336c1",
            "venue": "J. Comput. Syst. Sci.",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1737555",
                    "name": "M. Geist"
                },
                {
                    "authorId": "1721354",
                    "name": "O. Pietquin"
                },
                {
                    "authorId": "2582626",
                    "name": "G. Fricout"
                }
            ],
            "doi": "10.1109/ADVCOMP.2008.7",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "dde1a0e7d9fb1d5fb4e2619545546447af14450a",
            "title": "A Sparse Nonlinear Bayesian Online Kernel Regression",
            "url": "https://www.semanticscholar.org/paper/dde1a0e7d9fb1d5fb4e2619545546447af14450a",
            "venue": "2008 The Second International Conference on Advanced Engineering Computing and Applications in Sciences",
            "year": 2008
        },
        {
            "arxivId": "1206.3231",
            "authors": [
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                },
                {
                    "authorId": "1700606",
                    "name": "Bethany R. Leffler"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "143724999",
                    "name": "N. Roy"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "ae187b46c2a71f5e38c9ecc9018ffab4679bf4b5",
            "title": "CORL: A Continuous-state Offset-dynamics Reinforcement Learner",
            "url": "https://www.semanticscholar.org/paper/ae187b46c2a71f5e38c9ecc9018ffab4679bf4b5",
            "venue": "UAI",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "144926179",
                    "name": "Thomas J. Walsh"
                },
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                }
            ],
            "doi": "10.1007/s10994-010-5225-4",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "03d3fbad90f047d238b35a742e44b6cd5946d591",
            "title": "Knows what it knows: a\u00a0framework for\u00a0self-aware learning",
            "url": "https://www.semanticscholar.org/paper/03d3fbad90f047d238b35a742e44b6cd5946d591",
            "venue": "ICML '08",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1388372395",
                    "name": "F. Doshi-Velez"
                },
                {
                    "authorId": "145134886",
                    "name": "Joelle Pineau"
                },
                {
                    "authorId": "143724999",
                    "name": "N. Roy"
                }
            ],
            "doi": "10.1145/1390156.1390189",
            "intent": [],
            "isInfluential": false,
            "paperId": "05e1ca5d5e12f9fdea1925e76b6828010dc647a1",
            "title": "Reinforcement learning with limited reinforcement: using Bayes risk for active learning in POMDPs",
            "url": "https://www.semanticscholar.org/paper/05e1ca5d5e12f9fdea1925e76b6828010dc647a1",
            "venue": "ICML '08",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": "10.7282/T3TX3JSX",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "782411081e754b72da6565c41f3904de843b31d1",
            "title": "Prioritized Sweeping Converges to the Optimal Value Function",
            "url": "https://www.semanticscholar.org/paper/782411081e754b72da6565c41f3904de843b31d1",
            "venue": "",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                },
                {
                    "authorId": "1681946",
                    "name": "Carlos Diuk"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "15ba832dbb1882b2c516e06cb0f8f002d16bc4cf",
            "title": "Efficient Structure Learning in Factored-State MDPs",
            "url": "https://www.semanticscholar.org/paper/15ba832dbb1882b2c516e06cb0f8f002d16bc4cf",
            "venue": "AAAI",
            "year": 2007
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1700606",
                    "name": "Bethany R. Leffler"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "145957225",
                    "name": "T. Edmunds"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "c9282ac55cf2fb6a9123ac30fc108843ca8a44c4",
            "title": "Efficient Reinforcement Learning with Relocatable Action Models",
            "url": "https://www.semanticscholar.org/paper/c9282ac55cf2fb6a9123ac30fc108843ca8a44c4",
            "venue": "AAAI",
            "year": 2007
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                }
            ],
            "doi": "10.1109/ADPRL.2007.368176",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "2a7cb4ceb55f09daa6dd70cb13e2ae9e5f4994ce",
            "title": "Model-Based Reinforcement Learning in Factored-State MDPs",
            "url": "https://www.semanticscholar.org/paper/2a7cb4ceb55f09daa6dd70cb13e2ae9e5f4994ce",
            "venue": "2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning",
            "year": 2007
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "39163115",
                    "name": "P. S. Castro"
                },
                {
                    "authorId": "144368601",
                    "name": "Doina Precup"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology",
                "result"
            ],
            "isInfluential": true,
            "paperId": "0098d7d62239073b04bf74d36d4fc483471d68d2",
            "title": "Using Linear Programming for Bayesian Exploration in Markov Decision Processes",
            "url": "https://www.semanticscholar.org/paper/0098d7d62239073b04bf74d36d4fc483471d68d2",
            "venue": "IJCAI",
            "year": 2007
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2242846524",
                    "name": "Manfred Jaeger"
                },
                {
                    "authorId": "2242851717",
                    "name": "Peter Gj\u00f8l Jensen"
                },
                {
                    "authorId": "2242842320",
                    "name": "Kim G. Larsen"
                },
                {
                    "authorId": "143833035",
                    "name": "Axel Legay"
                },
                {
                    "authorId": "1824281",
                    "name": "Sean Sedwards"
                },
                {
                    "authorId": "2426566",
                    "name": "Jakob Haahr Taankvist"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "077f15439a407c53c794135b2edd55ffb51b452b",
            "title": "Teaching Stratego to Play Ball Optimal Synthesis for Continuous Space MDPs",
            "url": "https://www.semanticscholar.org/paper/077f15439a407c53c794135b2edd55ffb51b452b",
            "venue": "",
            "year": 2020
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                }
            ],
            "doi": "10.1007/978-3-642-27645-3_6",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "01ddeb03485d80ed70186915d7072824771524da",
            "title": "Sample Complexity Bounds of Exploration",
            "url": "https://www.semanticscholar.org/paper/01ddeb03485d80ed70186915d7072824771524da",
            "venue": "Reinforcement Learning",
            "year": 2012
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2227593840",
                    "name": "Doshi-Velez"
                },
                {
                    "authorId": "153676637",
                    "name": "Nicholas A. Roy"
                }
            ],
            "doi": null,
            "intent": [
                "background"
            ],
            "isInfluential": false,
            "paperId": "5f47a5a748723ececc89804532e2e94b74449f40",
            "title": "Reinforcement Learning with Limited Reinforcement : Using Bayes Risk for Active Learning in POMDPs Finale",
            "url": "https://www.semanticscholar.org/paper/5f47a5a748723ececc89804532e2e94b74449f40",
            "venue": "",
            "year": 2012
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144123639",
                    "name": "Jonathan Sorg"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "9d7ff7b1b2369fab19cc743cc9acfa83af4e131c",
            "title": "The Optimal Reward Problem: Designing Effective Reward for Bounded Agents",
            "url": "https://www.semanticscholar.org/paper/9d7ff7b1b2369fab19cc743cc9acfa83af4e131c",
            "venue": "",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "2758123",
                    "name": "A. Nouri"
                }
            ],
            "doi": "10.7282/T35D8RG6",
            "intent": [],
            "isInfluential": false,
            "paperId": "f1d8e49f65b222997c5b62e0afe16067d7733f80",
            "title": "Efficient model-based exploration in continuous state-space environments",
            "url": "https://www.semanticscholar.org/paper/f1d8e49f65b222997c5b62e0afe16067d7733f80",
            "venue": "",
            "year": 2011
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "3161699",
                    "name": "K. Dyagilev"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "282581c4c1c07830b5a973d971da5be37b146504",
            "title": "Reinforcement Learning in Parameterized Models Reinforcement Learning with Polynomial Learning Rate in Parameterized Models",
            "url": "https://www.semanticscholar.org/paper/282581c4c1c07830b5a973d971da5be37b146504",
            "venue": "",
            "year": 2010
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "31651045",
                    "name": "N. Vlassis"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "1712535",
                    "name": "Shie Mannor"
                },
                {
                    "authorId": "1807041",
                    "name": "P. Poupart"
                }
            ],
            "doi": "10.1007/978-0-387-30164-8_67",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "064dec29989aafd3b95aa0eff261556ba70dd363",
            "title": "Bayesian Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/064dec29989aafd3b95aa0eff261556ba70dd363",
            "venue": "Encyclopedia of Machine Learning",
            "year": 2010
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                }
            ],
            "doi": "10.7282/T3S46S46",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "7fcc643342520991951407a356dcc0376a955f92",
            "title": "A unifying framework for computational reinforcement learning theory",
            "url": "https://www.semanticscholar.org/paper/7fcc643342520991951407a356dcc0376a955f92",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "40396948",
                    "name": "A. Bernstein"
                },
                {
                    "authorId": "1742179",
                    "name": "N. Shimkin"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "c90132e25f074a2e5a893b0a3e376d8047b8dae7",
            "title": "Adaptive-Resolution Reinforcement Learning with Efficient Exploration in Deterministic Domains",
            "url": "https://www.semanticscholar.org/paper/c90132e25f074a2e5a893b0a3e376d8047b8dae7",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "ba15d66f5a3a9d2268c4ccf8d195a2a90c64c448",
            "title": "Compact parametric models for efficient sequential decision making in high-dimensional, uncertain domains",
            "url": "https://www.semanticscholar.org/paper/ba15d66f5a3a9d2268c4ccf8d195a2a90c64c448",
            "venue": "",
            "year": 2009
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                },
                {
                    "authorId": "1700606",
                    "name": "Bethany R. Leffler"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "result",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "dbf75129fad5a7c7030000a4fe0e051ba9a3680b",
            "title": "PAC Reinforcement Learning in Noisy Continuous Worlds",
            "url": "https://www.semanticscholar.org/paper/dbf75129fad5a7c7030000a4fe0e051ba9a3680b",
            "venue": "",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                }
            ],
            "doi": "10.7282/T3Z3202G",
            "intent": [
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "401691bfe933f111c96a41ba8fdde94ded096283",
            "title": "Probably Approximately Correct (PAC) Exploration in Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/401691bfe933f111c96a41ba8fdde94ded096283",
            "venue": "ISAIM",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "6e5d009e7d7a2a223c57cc58d989ec264c425679",
            "title": "Reinforcement Learning in Finite MDPs : PAC Analysis Reinforcement Learning in Finite MDPs : PAC Analysis",
            "url": "https://www.semanticscholar.org/paper/6e5d009e7d7a2a223c57cc58d989ec264c425679",
            "venue": "",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2221163",
                    "name": "Vlad M. Cora"
                }
            ],
            "doi": "10.14288/1.0051276",
            "intent": [],
            "isInfluential": false,
            "paperId": "fc0fe8705f3cba3c44cfa022bb8cc65697afdf65",
            "title": "Model-Based Active Learning in Hierarchical Policies",
            "url": "https://www.semanticscholar.org/paper/fc0fe8705f3cba3c44cfa022bb8cc65697afdf65",
            "venue": "",
            "year": 2008
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "78e09c0b4056da0e7dc7ada8ea6c6d895c713f6c",
            "title": "Reinforcement Learning via Online Linear Regression",
            "url": "https://www.semanticscholar.org/paper/78e09c0b4056da0e7dc7ada8ea6c6d895c713f6c",
            "venue": "",
            "year": 2007
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "874d2e69f0fa201e1e91f80c5db31856c8b1a941",
            "title": "PAC Reinforcement Learning Bounds for RTDP and Rand-RTDP Technical Report",
            "url": "https://www.semanticscholar.org/paper/874d2e69f0fa201e1e91f80c5db31856c8b1a941",
            "venue": "",
            "year": 2006
        }
    ],
    "corpusId": 8071208,
    "doi": null,
    "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
    ],
    "influentialCitationCount": 4,
    "isOpenAccess": false,
    "isPublisherLicensed": true,
    "is_open_access": false,
    "is_publisher_licensed": true,
    "numCitedBy": 61,
    "numCiting": 11,
    "paperId": "c962e92295affd68556bdcda5bf1882cb73bd35d",
    "references": [
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1990806",
                    "name": "Alexander L. Strehl"
                },
                {
                    "authorId": "144885169",
                    "name": "M. Littman"
                }
            ],
            "doi": "10.1145/1102351.1102459",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "63e2ca52df9c2ef5728e6326dcc74755ac3161ca",
            "title": "A theoretical analysis of Model-Based Interval Estimation",
            "url": "https://www.semanticscholar.org/paper/63e2ca52df9c2ef5728e6326dcc74755ac3161ca",
            "venue": "ICML",
            "year": 2005
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1388396612",
                    "name": "Eyal Even-Dar"
                },
                {
                    "authorId": "1712535",
                    "name": "Shie Mannor"
                },
                {
                    "authorId": "144830983",
                    "name": "Y. Mansour"
                }
            ],
            "doi": null,
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "96c1211bf54b277c225a3e50329058c2abcfdaca",
            "title": "Action Elimination and Stopping Conditions for Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/96c1211bf54b277c225a3e50329058c2abcfdaca",
            "venue": "ICML",
            "year": 2003
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "81338045",
                    "name": "Michael Kearns"
                },
                {
                    "authorId": "1699868",
                    "name": "Satinder Singh"
                }
            ],
            "doi": "10.1023/A:1017984413808",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "dc649486b881e672eea6546da48c46e1f98daf32",
            "title": "Near-Optimal Reinforcement Learning in Polynomial Time",
            "url": "https://www.semanticscholar.org/paper/dc649486b881e672eea6546da48c46e1f98daf32",
            "venue": "Machine Learning",
            "year": 2002
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2238176724",
                    "name": "R. S. Sutton"
                },
                {
                    "authorId": "1730590",
                    "name": "A. Barto"
                }
            ],
            "doi": null,
            "intent": [],
            "isInfluential": false,
            "paperId": "382f2d3c7e318c3ad2de028c6598a9700899ce80",
            "title": "Introduction to Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/382f2d3c7e318c3ad2de028c6598a9700899ce80",
            "venue": "",
            "year": 1998
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2285442513",
                    "name": "Philip W. L. Fong"
                }
            ],
            "doi": "10.1016/b978-1-55860-377-6.50036-0",
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "97965c78eac7cb56230a61894cb404f3bc67f564",
            "title": "A Quantitative Study of Hypothesis Selection",
            "url": "https://www.semanticscholar.org/paper/97965c78eac7cb56230a61894cb404f3bc67f564",
            "venue": "ICML",
            "year": 1995
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1709512",
                    "name": "L. Kaelbling"
                }
            ],
            "doi": "10.7551/mitpress/4168.001.0001",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "154e8048e5171b79dcff76c8642afd02a5466768",
            "title": "Learning in embedded systems",
            "url": "https://www.semanticscholar.org/paper/154e8048e5171b79dcff76c8642afd02a5466768",
            "venue": "",
            "year": 1993
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "144695232",
                    "name": "S. Kakade"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "a9e5a40b0ff5c40d2db7b73490922e115576adb5",
            "title": "On the sample complexity of reinforcement learning.",
            "url": "https://www.semanticscholar.org/paper/a9e5a40b0ff5c40d2db7b73490922e115576adb5",
            "venue": "",
            "year": 2003
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1680506",
                    "name": "R. Brafman"
                },
                {
                    "authorId": "1708847",
                    "name": "Moshe Tennenholtz"
                }
            ],
            "doi": null,
            "intent": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "paperId": "c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
            "title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning",
            "url": "https://www.semanticscholar.org/paper/c7dbf5ed7e9b63e104adb4e18bfbc98f5d6afdae",
            "venue": "J. Mach. Learn. Res.",
            "year": 2001
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "2238176724",
                    "name": "R. S. Sutton"
                },
                {
                    "authorId": "1730590",
                    "name": "A. Barto"
                }
            ],
            "doi": "10.1109/TNN.1998.712192",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "title": "Reinforcement Learning: An Introduction",
            "url": "https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054",
            "venue": "IEEE Trans. Neural Networks",
            "year": 1998
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1730590",
                    "name": "A. Barto"
                },
                {
                    "authorId": "1819261",
                    "name": "Steven J. Bradtke"
                },
                {
                    "authorId": "1699868",
                    "name": "Satinder Singh"
                }
            ],
            "doi": "10.1016/0004-3702(94)00011-O",
            "intent": [
                "result",
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "eaec01700f5ea63af311cfd7a70a3869460ce080",
            "title": "Learning to Act Using Real-Time Dynamic Programming",
            "url": "https://www.semanticscholar.org/paper/eaec01700f5ea63af311cfd7a70a3869460ce080",
            "venue": "Artif. Intell.",
            "year": 1995
        },
        {
            "arxivId": null,
            "authors": [
                {
                    "authorId": "1741124",
                    "name": "L. Valiant"
                }
            ],
            "doi": "10.1145/1968.1972",
            "intent": [
                "methodology"
            ],
            "isInfluential": false,
            "paperId": "d37b3a66d55d5361aeb5ef0793af66049535c24d",
            "title": "A theory of the learnable",
            "url": "https://www.semanticscholar.org/paper/d37b3a66d55d5361aeb5ef0793af66049535c24d",
            "venue": "CACM",
            "year": 1984
        }
    ],
    "s2FieldsOfStudy": [
        {
            "category": "Computer Science",
            "source": "external"
        },
        {
            "category": "Mathematics",
            "source": "external"
        },
        {
            "category": "Computer Science",
            "source": "s2-fos-model"
        }
    ],
    "title": "Incremental Model-based Learners With Formal Learning-Time Guarantees",
    "topics": [],
    "url": "https://www.semanticscholar.org/paper/c962e92295affd68556bdcda5bf1882cb73bd35d",
    "venue": "Conference on Uncertainty in Artificial Intelligence",
    "year": 2006
}